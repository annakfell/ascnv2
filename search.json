[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Statistics",
    "section": "",
    "text": "Preface\nThis document is a set of course notes for several Applied Statistics courses at California State University, Chico. This is not a textbook replacement, and topics covered will vary depending on the instructor. To make this clear we use the term notebook to refer to this document so as not to be confused with a traditional textbook.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Applied Statistics",
    "section": "Data",
    "text": "Data\nSome data and examples in this notebook are drawn from Practical Multivariate Analysis, 6th ed, Afifi, May, Donatello, Clark and used with permission by the authors. The data with corresponding codebooks can be downloaded from https://norcalbiostat.github.io/pma6_code/\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Applied Statistics",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nAnnabelle Feller (@annakfell)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "dataprep.html",
    "href": "dataprep.html",
    "title": "1  Workflow and Data Cleaning",
    "section": "",
    "text": "1.1 Reproducible workflows\nReproducibility is the ability for any researcher to take the same data set and run the same set of software program instructions as another researcher and achieve the same results.\nThe goal is to create an exact record of what was done to a data set to produce a specific result. To achieve reproducibility, we believe that three things must be present:",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Workflow and Data Cleaning</span>"
    ]
  },
  {
    "objectID": "dataprep.html#reproducible-workflows",
    "href": "dataprep.html#reproducible-workflows",
    "title": "1  Workflow and Data Cleaning",
    "section": "",
    "text": "The unprocessed data are connected directly to software code file(s) that perform data preparation techniques.\nThe processed data are connected directly to other software code file(s) that perform the analyses.\nAll data and code files are self-contained such that they could be given to another researcher to execute the code commands on a separate computer and achieve the same results as the original author.\n\n\n\n\n\n\n\n\nThink about it\n\n\n\nWhy do we need a codebook?\n\nYou are your own collaborator 6 months from now. Make sure you will be able to understand what you were doing.\nInvesting the time to do things clearly and in a reproducible manner will make your future self happy.\nComment your code with explanations and instructions.\n\nHow did you get from point A to B?\nWhy did you recode this variable in this manner?\n\nWe need to record those steps (not just for posterity).\nThis means your code must be saved in a script file.\n\nInclude sufficient notes to yourself describing what you are doing and why.\nFor R, this can be in a .R, .Rmd or .qmd file. I always prefer the latter.\nFor SAS you’ll use a .sas file\nFor STATA this will be a .do file\n\n\n\n\n\n\n\nFigure Credits: Roger Peng\n\n\n\n1.1.1 Literate programming\n\nProgramming paradigm introduced by Knuth (1984)\nExplain the logic of the program or analysis process in a natural language\nSmall code snippets included at each step act as a full set of instructions that can be executed to reproduce the result/analysis being discussed.\n\nLiterate programming tools are integrated into most common statistical packages, including:\n\nMarkdown (R, Stata), Quarto (R, Python, Julia, and JavaScript)\n\\(\\LaTeX\\) (R, SAS, Stata)\n\n\n\n\n\n\n\nLearn more\n\n\n\nThe current gold standard for writing reproducible literate documents in R is to use Quarto. Quarto documents can integrate code snippets from several languages and other code editing platforms, like Jupyter Notebook.\n\n\nPracticing reproducible research techniques using literate programming tools allows such major updates to be a simple matter of recompiling all coded instructions using the updated data set.\nThe effort then is reduced to a careful review and update of any written results.\nUsing literate programming tools, you can create formatted documents with\n\nsection headers\nbold and italicized words\ntables and graphics with built-in captions\n\nin a streamlined manner that is fully synchronized with the code itself.\nThe author writes the text explanations, interpretations, and code in the statistical software program itself, and the program will execute all commands and combine the text, code and output all together into a final dynamic document.\n\n\n\n\n\n\nThink about it\n\n\n\nWhat stages of the pipeline shown above can we conduct using literate programming tools?",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Workflow and Data Cleaning</span>"
    ]
  },
  {
    "objectID": "dataprep.html#import-data",
    "href": "dataprep.html#import-data",
    "title": "1  Workflow and Data Cleaning",
    "section": "1.2 Import data",
    "text": "1.2 Import data\nThis section uses the raw depression data set from the Afifi et. al. textbook. This is a tab-delimited data set, so we opt to use read.table here. We include arguments sep=\"\\t\" to indicate columns are separated with tabs and header=TRUE to indicate the first row of the data is the variable names.\n\ndepress_raw &lt;- read.table(here::here(\"data/Depress.txt\"), \n                      sep=\"\\t\", header=TRUE)  \n\n\n\n\n\n\n\nLearn more\n\n\n\nSee R for Data Science (2e) for more instruction on importing different types of data and ways you can streamline your data import.\n\n\nThe absolute first thing you should do is to look at your raw data table. Are the column headers variable names? Did all the rows get read in? Are there any extra columns or rows included?",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Workflow and Data Cleaning</span>"
    ]
  },
  {
    "objectID": "dataprep.html#data-management",
    "href": "dataprep.html#data-management",
    "title": "1  Workflow and Data Cleaning",
    "section": "1.3 Data management",
    "text": "1.3 Data management\nQuestions to ask yourself while reviewing the codebook to choose variables to be used in an analysis.\n\nAre there codes that indicate missing data? E.g. MISSING or -99?\nDo you need to make response codes more logical?\n\nSome systems will record 1=YES and 2=NO. This should be changed to 0=NO.\n\nDo you need to recode numerical variables to categorical?\n\nSometimes categorical data will be recorded as 1, 2, 3 etc. when those numbers represent named categories.\n\nDo you need to create secondary variables such as an average across measures to create a score?\nAre the variable names user-friendly? Mixtures of CAPS and lower case, names with spaces or special characters should all be changed.\n\nSome of these answers will come only after you look at your data. This can be looking at the raw data itself but also looking at tables and charts generated from the data. Often when you try to create a plot or table you will encounter an error or something odd looking that will be the notification that something has to be adjusted.\nThe next sections go over a few of the common data management processes, but is not comprehensive, and may only show one method for cleaning. There are always different ways to accomplish tasks.\n\n1.3.1 Renaming variable names for sanity’s sake\n\nhead(names(depress_raw))\n\n[1] \"ID\"      \"SEX\"     \"AGE\"     \"MARITAL\" \"EDUCAT\"  \"EMPLOY\" \n\n\nPeeking at the names of the variables we note that they are all in upper case. If that is fine with you, awesome. I prefer to have everything lower case so that I don’t ever have to remember which are the capital letters. Here are two ways to accomplish this:\n\nbasejanitor\n\n\nA base R solution is to use tolower() to turn all variable names to lower case. This code is not run here because it would overwrite the variable names in the same data set (depress_raw). Keep the imported (aka. “raw”) data untouched, and then make a copy of the data once you start making changes.\n\nnames(depress_raw) &lt;- tolower(names(depress_raw))\n\n\n\nA highly recommended method is to use the clean_names() function from the janitor package. This will also remove any special characters, spaces and capital letters from your variable names.\n\ndepress &lt;- depress_raw %&gt;% janitor::clean_names()\n\nI am “staging” the data set at this point because i’m making a major change away from the ‘raw’ data. So i’m saving the changes to the variable names in a new data set called depress.\n\n\n\n\n\n\nNote the use of :: between the package name janitor and the function within that package clean_names. This is a shortcut that allows you to use a function from a specific package without loading the entire package. This can also reduce in function name conflicts that we’ll mention below.\n\n\n\n\n\n\n\n\n1.3.2 Identifying variable types\n\nbasetidyverse\n\n\nThe str function is short for structure. This shows you the variable names, what data types R thinks each variable are, and some of the raw data.\n\nstr(depress)\n\n'data.frame':   294 obs. of  37 variables:\n $ id      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ sex     : int  2 1 2 2 2 1 2 1 2 1 ...\n $ age     : int  68 58 45 50 33 24 58 22 47 30 ...\n $ marital : int  5 3 2 3 4 2 2 1 2 2 ...\n $ educat  : int  2 4 3 3 3 3 2 3 3 2 ...\n $ employ  : int  4 1 1 3 1 1 5 1 4 1 ...\n $ income  : int  4 15 28 9 35 11 11 9 23 35 ...\n $ relig   : int  1 1 1 1 1 1 1 1 2 4 ...\n $ c1      : int  0 0 0 0 0 0 2 0 0 0 ...\n $ c2      : int  0 0 0 0 0 0 1 1 1 0 ...\n $ c3      : int  0 1 0 0 0 0 1 2 1 0 ...\n $ c4      : int  0 0 0 0 0 0 2 0 0 0 ...\n $ c5      : int  0 0 1 1 0 0 1 2 0 0 ...\n $ c6      : int  0 0 0 1 0 0 0 1 3 0 ...\n $ c7      : int  0 0 0 0 0 0 0 0 0 0 ...\n $ c8      : int  0 0 0 3 3 0 2 0 0 0 ...\n $ c9      : int  0 0 0 0 3 1 2 0 0 0 ...\n $ c10     : int  0 0 0 0 0 0 0 0 0 0 ...\n $ c11     : int  0 0 0 0 0 0 0 0 0 0 ...\n $ c12     : int  0 1 0 0 0 1 0 0 3 0 ...\n $ c13     : int  0 0 0 0 0 2 0 0 0 0 ...\n $ c14     : int  0 0 1 0 0 0 0 0 3 0 ...\n $ c15     : int  0 1 1 0 0 0 3 0 2 0 ...\n $ c16     : int  0 0 1 0 0 2 0 1 3 0 ...\n $ c17     : int  0 1 0 0 0 1 0 1 0 0 ...\n $ c18     : int  0 0 0 0 0 0 0 1 0 0 ...\n $ c19     : int  0 0 0 0 0 0 0 1 0 0 ...\n $ c20     : int  0 0 0 0 0 0 1 0 0 0 ...\n $ cesd    : int  0 4 4 5 6 7 15 10 16 0 ...\n $ cases   : int  0 0 0 0 0 0 0 0 1 0 ...\n $ drink   : int  2 1 1 2 1 1 2 2 1 1 ...\n $ health  : int  2 1 2 1 1 1 3 1 4 1 ...\n $ regdoc  : int  1 1 1 1 1 1 1 2 1 1 ...\n $ treat   : int  1 1 1 2 1 1 1 2 1 2 ...\n $ beddays : int  0 0 0 0 1 0 0 0 1 0 ...\n $ acuteill: int  0 0 0 0 1 1 1 1 0 0 ...\n $ chronill: int  1 1 0 1 0 1 1 0 1 0 ...\n\n\nTo check the data type of just one variable, you have two options:\n\nThe typeof function\n\n\ntypeof(depress$marital)\n\n[1] \"integer\"\n\n\n\nThe class function\n\n\nclass(depress$age)\n\n[1] \"integer\"\n\n\n\n\nA tidyverse alternative is glimpse()\n\nglimpse(depress)\n\nRows: 294\nColumns: 37\n$ id       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ sex      &lt;int&gt; 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2…\n$ age      &lt;int&gt; 68, 58, 45, 50, 33, 24, 58, 22, 47, 30, 20, 57, 39, 61, 23, 2…\n$ marital  &lt;int&gt; 5, 3, 2, 3, 4, 2, 2, 1, 2, 2, 1, 2, 2, 5, 2, 1, 1, 4, 1, 5, 1…\n$ educat   &lt;int&gt; 2, 4, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 4, 2, 6, 2, 3…\n$ employ   &lt;int&gt; 4, 1, 1, 3, 1, 1, 5, 1, 4, 1, 3, 2, 1, 4, 1, 1, 1, 3, 1, 4, 1…\n$ income   &lt;int&gt; 4, 15, 28, 9, 35, 11, 11, 9, 23, 35, 25, 24, 28, 13, 15, 6, 8…\n$ relig    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 4, 4, 1, 1, 1, 2, 1, 1, 1, 1, 4, 2…\n$ c1       &lt;int&gt; 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 3, 1, 0, 0, 0…\n$ c2       &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 3, 0, 0, 0, 0…\n$ c3       &lt;int&gt; 0, 1, 0, 0, 0, 0, 1, 2, 1, 0, 1, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0…\n$ c4       &lt;int&gt; 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0…\n$ c5       &lt;int&gt; 0, 0, 1, 1, 0, 0, 1, 2, 0, 0, 1, 0, 0, 1, 0, 1, 3, 1, 0, 0, 0…\n$ c6       &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 1, 3, 0, 2, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0…\n$ c7       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0…\n$ c8       &lt;int&gt; 0, 0, 0, 3, 3, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 3, 0…\n$ c9       &lt;int&gt; 0, 0, 0, 0, 3, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 3…\n$ c10      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0…\n$ c11      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0…\n$ c12      &lt;int&gt; 0, 1, 0, 0, 0, 1, 0, 0, 3, 0, 1, 0, 1, 1, 0, 1, 2, 0, 0, 0, 0…\n$ c13      &lt;int&gt; 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0…\n$ c14      &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0…\n$ c15      &lt;int&gt; 0, 1, 1, 0, 0, 0, 3, 0, 2, 0, 1, 2, 0, 0, 1, 1, 3, 0, 0, 0, 0…\n$ c16      &lt;int&gt; 0, 0, 1, 0, 0, 2, 0, 1, 3, 0, 1, 2, 1, 0, 3, 1, 2, 0, 0, 0, 0…\n$ c17      &lt;int&gt; 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0…\n$ c18      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0…\n$ c19      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0…\n$ c20      &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 3, 0, 0, 0, 0…\n$ cesd     &lt;int&gt; 0, 4, 4, 5, 6, 7, 15, 10, 16, 0, 18, 4, 8, 4, 8, 21, 42, 6, 0…\n$ cases    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0…\n$ drink    &lt;int&gt; 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1…\n$ health   &lt;int&gt; 2, 1, 2, 1, 1, 1, 3, 1, 4, 1, 2, 2, 3, 1, 1, 3, 1, 3, 2, 2, 1…\n$ regdoc   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1…\n$ treat    &lt;int&gt; 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1…\n$ beddays  &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0…\n$ acuteill &lt;int&gt; 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0…\n$ chronill &lt;int&gt; 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1…\n\n\n\n\n\nRight away this tells me that R thinks all variables are numeric integers, not categorical variables. Many of these will have to be changed. We’ll get to that in a moment.\n\n\n1.3.3 Convert number to factor\nWhen variables have numerical levels it is necessary to ensure that the program knows it is a factor variable.\nThe following code uses the factor() function to take the marital status variable and convert it into a factor variable with specified labels that match the codebook.\n\ndepress$marital &lt;- factor(depress$marital, \n      labels = c(\"Never Married\", \"Married\", \"Divorced\", \"Separated\", \"Widowed\"))\n\nNote that I am not making a new variable here, but overwriting the same marital variable. If If it did not you will have to re-run the lread in the raw data set again since the variable marital was replaced.\nIt is important to confirm the recode worked. I do this by creating a two-way table between the variable as it exists on the raw data, and how it exists after this line of code. What we are looking for is that all values on the left/rows (original version) line up with what we want them to say on the right (new version), and that no missing data was created or destroyed.\n\ntable(depress_raw$MARITAL, depress$marital, useNA = \"always\")\n\n      \n       Never Married Married Divorced Separated Widowed &lt;NA&gt;\n  1               73       0        0         0       0    0\n  2                0     127        0         0       0    0\n  3                0       0       43         0       0    0\n  4                0       0        0        13       0    0\n  5                0       0        0         0      38    0\n  &lt;NA&gt;             0       0        0         0       0    0\n\n\n\n\n\n\n\n\nLearn more\n\n\n\nSee more examples on Math 130 Lesson 06\n\n\n\n\n1.3.4 Identifying outliers\nLet’s look at the age variable in the depression data set.\n\npar(mfrow=c(1,2))\nboxplot(depress$age)\nhist(depress$age)\n\n\n\n\n\n\n\n\nJust looking at the data graphically raises no red flags. The boxplot shows no outlying values and the histogram does not look wildly skewed. This is where knowledge about the data set is essential. The codebook does not provide a valid range for the data, but the description of the data starting on page 3 in the textbook clarifies that this data set is on adults. In the research world, this specifies 18 years or older.\nNow look back at the graphics. See anything odd? It appears as if the data go pretty far below 20, possibly below 18. Let’s check the numerical summary to get more details.\n\nsummary(depress$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   28.00   42.50   44.38   59.00   89.00 \n\n\nThe minimum value is a 9, which is outside the range of valid values for this variable. This is where you, as a statistician, data analyst or researcher goes back to the PI and asks for advice. Should this data be set to missing, or edited in a way that changes this data point into a valid piece of data?\nAnother example\n\nboxplot(depress$income)\n\n\n\n\n\n\n\n\nWhile there is at least one potential outliers (denoted by the dots), there are none so far away from the rest of the group (or at values such as 99 or -99 that may indicate missing codes) that we need to be concerned about.\n\n\n1.3.5 Changing numeric values\nWhat you didn’t know until now, is that for demonstration purposes I went in and changed a 19 to a 9. So the correct thing to do here is to change that 9, back to a 19.\n\nifelse()direct assign\n\n\nThis is a very good use of the ifelse() function.\n\ndepress$age &lt;- ifelse(depress$age==9, 19, depress$age)\n\nThe logical statement is depress$age9. Wherever this is true, replace the value of depress$age with 19, and wherever this is false keep the value of depress$age unchanged (by “replacing” the new value with the same old value).\n\n\nAlternatively, you can change that one value using bracket notation. Here you are specifying that you only want the rows where age==9, and directly assign a value of 19 to those rows.\n\ndepress$age[depress$age==9] &lt;- 19\n\nConfirm the recode.\n\nsummary(depress$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   28.00   42.50   44.41   59.00   89.00 \n\n\nLooks like it worked.\n\n\n\n\n\n1.3.6 Creating secondary variables\n\n\n1.3.7 Create a binary indicator\nFor analysis purposes you may need to have a numeric binary indicator (0/1) of a variable.\n\ntable(addhealth$eversmoke_c)\n\n\nNon Smoker     Smoker \n      1773       3324 \n\naddhealth$smoker &lt;- ifelse(addhealth$eversmoke_c==\"Smoker\", 1, 0)\ntable(addhealth$eversmoke_c, addhealth$smoker, useNA=\"always\")\n\n            \n                0    1 &lt;NA&gt;\n  Non Smoker 1773    0    0\n  Smoker        0 3324    0\n  &lt;NA&gt;          0    0 1407\n\n\n\n\n\n\n\n\nLearn more\n\n\n\nSee Math 130 lesson 05 for now.",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Workflow and Data Cleaning</span>"
    ]
  },
  {
    "objectID": "dataprep.html#wrangling-factors",
    "href": "dataprep.html#wrangling-factors",
    "title": "1  Workflow and Data Cleaning",
    "section": "1.4 Wrangling factors",
    "text": "1.4 Wrangling factors\n\n\n\n\n\n\nLearn more\n\n\n\nFor more help on renaming, releveling, lumping, and removing levels see Math 130 lesson 06 for now. Also the forcats vignette.\n\n\n\n1.4.1 Collapsing categorical variables into fewer categories\nFor unbiased and accurate results of a statistical analysis, sufficient data has to be present. Often times once you start slicing and dicing the data to only look at certain groups, or if you are interested in the behavior of certain variables across levels of another variable, sometimes you start to run into small sample size problems.\nFor example, consider marital status again. There are only 13 people who report being separated. This could potentially be too small of a group size for valid statistical analysis. One way to deal with insufficient data within a certain category is to collapse categories.\n\n\n\n\n\n\nNote\n\n\n\nNote I am choosing to ‘stage’ my data here. So if I mess something up in this section, I don’t have to re-read in the raw data again or go back and rerun ALL the code, but just up until depress1 is created.\n\n\n\ndepress1 &lt;- depress\n\n\ncarforcats\n\n\nThe following example code uses the recode() function from the car package to create a new variable that I am calling marital2 that combines the Divorced and Separated levels.\n\ndepress1$marital &lt;- car::recode(depress1$marital, \"'Divorced' = 'Sep/Div'; 'Separated' = 'Sep/Div'\")\n\n\n\n\n\n\n\nNote the use of the :: again. Here it is even more important to use this shortcut because the specific recode function we want to use comes from the car package. There are other packages (probably dplyr) that also have a function called recode. So here I use :: as a way to be SUPER EXPLICIT on which function I want to use.\n\n\n\nAlways confirm your recodes. Check a table of the old variable (depress$marital) against the new one depress1$marital.\n\ntable(depress$marital, depress1$marital , useNA=\"always\")\n\n               \n                Married Never Married Sep/Div Widowed &lt;NA&gt;\n  Never Married       0            73       0       0    0\n  Married           127             0       0       0    0\n  Divorced            0             0      43       0    0\n  Separated           0             0      13       0    0\n  Widowed             0             0       0      38    0\n  &lt;NA&gt;                0             0       0       0    0\n\n\nThis confirms that records where depress$marital (rows) is Divorced or Separated have the value of Sep/Div for depress1$marital (columns). And that no missing data crept up in the process.\n\n\nThe fct_collapse() function from the forcats package can do the same process, without worrying about a package/function conflict.\n\ndepress1$marital &lt;- fct_collapse(depress$marital, SepDiv = c(\"Divorced\", \"Separated\"))\n\nAgain, you should always confirm your recodes. Check a table of the old variable (depress$marital) against the new one depress1$marital\n\ntable(depress$marital, depress1$marital , useNA=\"always\")\n\n               \n                Never Married Married SepDiv Widowed &lt;NA&gt;\n  Never Married            73       0      0       0    0\n  Married                   0     127      0       0    0\n  Divorced                  0       0     43       0    0\n  Separated                 0       0     13       0    0\n  Widowed                   0       0      0      38    0\n  &lt;NA&gt;                      0       0      0       0    0\n\n\n\n\n\n\n\n1.4.2 Binning a continuous variable into categorical ranges.\nWe can use the cut function to create a new variable that categorizes income into the following ranges: &lt;30, [30, 40), [40,50), [50, 60), 60+.\n\ndepress1$inc_cut &lt;- cut(depress1$income, breaks=c(0, 30,40,50,60, 100))\ntable(depress1$inc_cut)\n\n\n  (0,30]  (30,40]  (40,50]  (50,60] (60,100] \n     231       28       16        9       10 \n\n\n\n\n1.4.3 Dichotomizing a measure into 2 categories\nDichotomous variables tend to be binary indicator variables where a code of 1 is the level you’re interested in.\n\nExample 1Example 2\n\n\n\n\n\n\n\n\n\nSwitch example from binary gender to different characteristic\n\n\n\n\nIn this study gender is coded as 2=Female and 1=Male. (This data was collected in the ’70s, and so only two genders were provided as options). We want to convert this be a binary indicator of female, where 1=Female and 0=Male.\n\ndepress1$female &lt;- depress1$sex -1 \ntable(depress1$female)\n\n\n  0   1 \n111 183 \n\n\n0/1 binary coding is mandatory for many analyses. One simple reason is that now you can calculate the mean and interpret it as a proportion.\n\nmean(depress1$female)\n\n[1] 0.622449\n\n\n62% of individuals in this data set are female.\n\n\nSometimes the data is recorded as 1/2 (Yes/No), so just subtracting from 1 doesn’t create a positive indicator of the variable. For example, drink=1 if they are a regular drinker, and drink=2 if they are not. We want not drinking to be coded as 0, not 2.\n\ntable(depress_raw$DRINK)\n\n\n  1   2 \n234  60 \n\n\nThe ifelse() function says that if depress$drink has a value equal to 2 ==2, then change the value to 0. Otherwise leave it alone.\n\ndepress1$drink &lt;- ifelse(depress1$drink==2, 0, depress1$drink)\ntable(depress1$drink, depress$dri)\n\n   \n      1   2\n  0   0  60\n  1 234   0",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Workflow and Data Cleaning</span>"
    ]
  },
  {
    "objectID": "dataprep.html#combining-values-across-multiple-variables",
    "href": "dataprep.html#combining-values-across-multiple-variables",
    "title": "1  Workflow and Data Cleaning",
    "section": "1.5 Combining values across multiple variables",
    "text": "1.5 Combining values across multiple variables\nLet’s stage the data again for this section.\n\ndepress2 &lt;- depress1\n\n\n1.5.1 Row-wise sum or average\nThe Center for Epidemiological Studies Depression Scale (CESD) is series of questions asked to a person to measure their level of depression. CESD is calculated as the sum of all 20 component variables, and is already on this data set. Let’s create a new variable named sleep as subscale for sleep quality by adding up question numbers 5, 11, and 19.\nReference: CESD-R\n\nBasemutate + manual\n\n\n\ndepress2$sleep &lt;- depress2$c5 + depress2$c11 + depress2$c19\n\nI’ll confirm it works by looking at a few rows and making sure they all add up.\n\nhead(depress2[c('c5', 'c11', 'c19', 'sleep')])\n\n  c5 c11 c19 sleep\n1  0   0   0     0\n2  0   0   0     0\n3  1   0   0     1\n4  1   0   0     1\n5  0   0   0     0\n6  0   0   0     0\n\n\n\n\n\ndepress2 &lt;- depress %&gt;% mutate(sleep = c5+c11+c19)\nhead(depress2[c('c5', 'c11', 'c19', 'sleep')])\n\n  c5 c11 c19 sleep\n1  0   0   0     0\n2  0   0   0     0\n3  1   0   0     1\n4  1   0   0     1\n5  0   0   0     0\n6  0   0   0     0",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Workflow and Data Cleaning</span>"
    ]
  },
  {
    "objectID": "dataprep.html#assessing-normality",
    "href": "dataprep.html#assessing-normality",
    "title": "1  Workflow and Data Cleaning",
    "section": "1.6 Assessing Normality",
    "text": "1.6 Assessing Normality\n\n1.6.1 Histogram and density plots\n\nhist(depress2$income, prob=TRUE, xlab=\"Annual income (in thousands)\", \n     main=\"Histogram and Density curve of Income\", ylab=\"\")\nlines(density(depress2$income), col=\"blue\")\n\n\n\n\n\n\n\nsummary(depress2$income)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.00    9.00   15.00   20.57   28.00   65.00 \n\n\nThe distribution of annual income is slightly skewed right with a mean of $20.5k per year and a median of $15k per year income. The range of values goes from $2k to $65k. Reported income above $40k appear to have been rounded to the nearest $10k, because there are noticeable peaks at $40k, $50k, and $60k.\n\n\n1.6.2 Q-Q plot\nAnother common method of assessing normality is to create a normal probability (or normal quantile) plot.\n\nqqnorm(depress2$income);qqline(depress2$income, col=\"red\")\n\n\n\n\n\n\n\n\nThe points on the normal probability plot do not follow the red reference line very well. The dots show a more curved, or U shaped form rather than following a linear line. This is another indication that the data is skewed and a transformation for normality should be created.\n\n1.6.2.1 Transformations\nAs a demonstration of transformations that can be used to shift a distribution more towards a normal shape, here we create three new variables: log10inc as the log base 10 of Income, loginc as the natural log of Income, and xincome which is equal to the negative of one divided by the cubic root of income.\n\nlog10inc &lt;- log10(depress2$income)\nloginc   &lt;- log(depress2$income)\nxincome  &lt;- -1/(depress2$income)^(-1/3)\n\nCreate a single plot that display normal probability plots for the original, and each of the three transformations of income. Use the base graphics grid organizer par(mfrow=c(r,c)) where r is the number of rows and c is the number of columns. Which transformation does a better job of normalizing the distribution of Income?\n\npar(mfrow=c(2,2)) # Try (4,1) and (1,4) to see how this works. \nqqnorm(depress2$income, main=\"Income\"); qqline(depress2$income,col=\"blue\")\nqqnorm(log10inc, main=\"Log 10\"); qqline(log10inc, col=\"blue\")\nqqnorm(loginc, main = \"Natural Log\"); qqline(loginc, col=\"blue\")\nqqnorm(xincome, main=\"-1/cuberoot(income)\"); qqline(xincome, col=\"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo transform or not to transform\n\n\n\nIn general, transformations are more effective when the the standard deviation is large relative to the mean. One rule of thumb is if the sd/mean ratio is less than 1/4, a transformation may not be necessary.\n\nsd(depress2$income) / mean(depress2$income)\n\n[1] 0.743147\n\n\nAlternatively Hoaglin, Mosteller and Tukey (1985) showed that if the largest observation divided by the smallest observation is over 2, then the data may not be sufficiently variable for the transformation to be decisive.\n\nmax(depress2$income) / (min(depress2$income)+.1)\n\n[1] 30.95238\n\n\nNote that these rules are not meaningful for data without a natural zero.",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Workflow and Data Cleaning</span>"
    ]
  },
  {
    "objectID": "dataprep.html#missing-data",
    "href": "dataprep.html#missing-data",
    "title": "1  Workflow and Data Cleaning",
    "section": "1.7 Missing data",
    "text": "1.7 Missing data\n\n1.7.1 Identifying missing data\nIn Excel, missing data can show up as a blank cell. R displays missing data as NA values.\nWhy would data be missing? Other than the obvious data entry errors, tech glitches or just non-cooperative plants or people, sometimes values are out of range and you would rather delete them than change their value (data edit).\n\nCategoricalContinuous\n\n\nLets look at the religion variable in the depression data set.\n\ntable(depress2$relig, useNA=\"always\")\n\n\n   1    2    3    4    6 &lt;NA&gt; \n 155   51   30   56    2    0 \n\n\nLooking at the codebook, there is no category 6 for religion. Let’s change all values to NA.\n\ndepress2$relig[depress2$relig==6] &lt;- NA\n\nThis code says take all rows where relig is equal to 6, and change them to NA.\nConfirm recode.\n\ntable(depress2$relig, useNA=\"always\")\n\n\n   1    2    3    4 &lt;NA&gt; \n 155   51   30   56    2 \n\n\nNotice the use of the useNA=\"always\" argument. If we just looked at the base table without this argument, we would have never known there was missing data!\n\ntable(depress2$relig)\n\n\n  1   2   3   4 \n155  51  30  56 \n\n\n\n\nWhat about continuous variables? Well there happens to be no other missing data in this data set, so let’s make up a set of 7 data points stored in a variable named y.\n\ny &lt;- c(1, 2, 3, NA, 4, NA, 6)\ny\n\n[1]  1  2  3 NA  4 NA  6\n\n\nThe #1 way to identify missing data in a continuous variable is by looking at the summary() values.\n\nmean(y)\n\n[1] NA\n\nsummary(y)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n    1.0     2.0     3.0     3.2     4.0     6.0       2 \n\nmean(y, na.rm=TRUE)\n\n[1] 3.2\n\n\nIn R, any arithmetic function (like addition, multiplication) on missing data results in a missing value. The na.rm=TRUE toggle tells R to calculate the complete case mean. This is a biased measure of the mean, but missing data is a topic worthy of it’s own course and is introduced in ?sec-classification.",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Workflow and Data Cleaning</span>"
    ]
  },
  {
    "objectID": "dataprep.html#export-and-save",
    "href": "dataprep.html#export-and-save",
    "title": "1  Workflow and Data Cleaning",
    "section": "1.8 Export and save",
    "text": "1.8 Export and save\nYou’ve just made a ton of changes!\n\nSave or export the new data set to your computer.\nEdit the codebook to reflect the changes that you made.\nKeep the data, codebook and data management file in the same folder.\n\n\ndepress_clean &lt;- depress2\n\n# Save as a .Rdata file for later use in R\nsave(depress_clean, file = \"data/depress_clean.Rdata\") \n\nNow every time you run your data cleaning script file, it will make all the changes and save/overwrite the depress_clean.Rdata data file. This ensures that any analysis script that uses this data has the most up to date variables.\nWe can use dplyr::select to select and save individual variables without storing the entire data frame.\n\nout &lt;- depress %&gt;% select(list of variables)\nsave(out, \"data/var1.Rdata\")\n\n\n\n\n\n\n\nLearn more\n\n\n\nNeed to export to a different software program? Look into the haven package.",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Workflow and Data Cleaning</span>"
    ]
  },
  {
    "objectID": "dataprep.html#wide-long",
    "href": "dataprep.html#wide-long",
    "title": "1  Workflow and Data Cleaning",
    "section": "1.9 Wide vs. long data",
    "text": "1.9 Wide vs. long data\nThe data on Lung function originally was recorded in wide format, with separate variables for mother’s and father’s FEV1 score (MFEV1 and FFEV). In this format, the data is one record per family.\n\nhead(fev[,1:15])\n\n  ID AREA FSEX FAGE FHEIGHT FWEIGHT FFVC FFEV1 MSEX MAGE MHEIGHT MWEIGHT MFVC\n1  1    1    1   53      61     161  391  3.23    2   43      62     136  370\n2  2    1    1   40      72     198  441  3.95    2   38      66     160  411\n3  3    1    1   26      69     210  445  3.47    2   27      59     114  309\n4  4    1    1   34      68     187  433  3.74    2   36      58     123  265\n5  5    1    1   46      61     121  354  2.90    2   39      62     128  245\n6  6    1    1   44      72     153  610  4.91    2   36      66     125  349\n  MFEV1 OCSEX\n1  3.31     2\n2  3.47     1\n3  2.65     1\n4  2.06     2\n5  2.33     1\n6  3.06     1\n\n\nTo analyze the effect of gender on FEV, the data need to be in long format, with a single variable for fev and a separate variable for gender. The following code chunk demonstrates one method of combining data on height, gender, age and FEV1 for both males and females.\n\nfev2 &lt;- data.frame(gender = c(fev$FSEX, fev$MSEX), \n                   rev = c(fev$FFEV1, fev$MFEV1), \n                   ht = c(fev$FHEIGHT, fev$MHEIGHT), \n                   age = c(fev$FAGE, fev$MAGE))\nfev2$gender &lt;- factor(fev2$gender, labels=c(\"M\", \"F\"))\nhead(fev2)  \n\n  gender  rev ht age\n1      M 3.23 61  53\n2      M 3.95 72  40\n3      M 3.47 69  26\n4      M 3.74 68  34\n5      M 2.90 61  46\n6      M 4.91 72  44\n\n\nNearly all analysis procedures and most graphing procedures require the data to be in long format. There are several R packages that can help with this including reshape2 and tidyr.\n\n\n\n\n\n\nLearn more\n\n\n\nRead more on tidy data in R for Data Science 2e, or look into the mice package vignettes.\n\n\n\n1.9.1 Model predictions\nSituation: You want to add model predictions to the data set, but you have missing data that was automatically dropped prior to analysis.\n\n\n\n\n\n\nWarning\n\n\n\n\nAdd methods for dealing with this\n\n\n\nR objects created by methods such as lm and glm will store the data used in the model in the model object itself in model$data. See ?sec-classification for an example.\n\n\n1.9.2 Factor analysis and principal components\nIf your original data had missing values, here are two methods to get the PC’s / factor scores for available data back onto the data set.\n\nMethod 1Method 2\n\n\nCreate an ID column and merge new variables onto original data. (add columns)\n\nIf no ID column exists, create one on the original dataset id = 1:NROW(data)\nUse select() to extract the ID and all variables used in the factor analysis, then do a na.omit() to drop rows with any missing data. Save this as a new complete case data set.\nConduct PCA / Factor analysis on this new complete case data set (MINUS THE ID). Extract the PCs or factor scores.\nUse bind_cols() to add the ID variable to the data containing factor scores.\nThen left_join(original_data, factor_score_data) the factor scores back to the original data, using the ID variable as the joining key.\n\n\n\nSplit the data, analyze one part then concatenate back together. (add rows)\n\nUse the complete.cases() function to create a boolean vector for if each row is complete\nSplit the data into complete and incomplete.\nDo the analysis on the complete rows, extracting the PC’s/Factors\nAdd the PC/Factor data onto the complete rows using bind_cols\nThen bind_rows the two parts back together.\n\n\ncc.idx &lt;- hiv %&gt;% select(starts_with(\"pb\")) %&gt;% complete.cases() # 1\n\ncomplete.rows &lt;- hiv[cc.idx,] #2\nincomplete.rows &lt;- hiv[!cc.idx,]\n\npc.scores &lt;- princomp(pb)$scores #3 \n\ncomplete.add.pc &lt;- bind_cols(complete.rows, pc.scores) #4\n\nhiv.with.pcs &lt;- bind_rows(complete.add.pc, incomplete.rows) #5",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Workflow and Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data_viz.html",
    "href": "data_viz.html",
    "title": "2  Visualizing Data",
    "section": "",
    "text": "2.1 Choosing your audience\nThe level of detail you put into your graphs/tables will depend on who the graph/table is for. In general, you will be visualizing data for three audience types: yourself and/or your data management team, an internal audience (other specialties within your organization, research coordinators), and external, professional audiences. The tabs below describe the three audience types and show an example graph for each.\nWho is your audience?",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "data_viz.html#choosing-your-audience",
    "href": "data_viz.html#choosing-your-audience",
    "title": "2  Visualizing Data",
    "section": "",
    "text": "YourselfInternal AudienceProfessional\n\n\nGraphs/tables for yourself and/or your working team can be “quick and dirty.” These graphs are for getting a quick look at the data and are meant for people who are already familiar with the data.\n\n\nShow the code\nplot_frq(pen$island)\n\n\n\n\n\n\n\n\n\n\n\nWhen presenting for an audience with some familiarity with the project and the data analysis process, your graphs don’t have to be completely perfect, but they should be fairly polished and be understandable. A good rule of thumb for these graphs is if they were to end up being published as-is without your knowledge, you wouldn’t be completely embarrassed to see them in print.\n\n\nShow the code\nplot_frq(pen$island, title = \"Count of Penguins by Island\") +\n        xlab(\"Island\") +\n        theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nProfessional quality graphs and tables have the highest amount of detail and take the most amount of time to make. They should be able to be interpreted by people not familiar with the project or data analysis, even without reading the rest of the report.\n\n\nShow the code\nplot_frq(pen$island, title = \"Count of Penguins by Island\") +\n          labs(subtitle = \"Includes penguins of Adelie, Gentoo, and Chinstrap species\") +\n          xlab(\"Island\") +\n          ylab(\"Penguin Count\") + \n          theme_minimal()",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "data_viz.html#the-data",
    "href": "data_viz.html#the-data",
    "title": "2  Visualizing Data",
    "section": "2.2 The Data",
    "text": "2.2 The Data\nWe will use the penguins dataset that comes with the palmerpenguins package. This dataset contains size measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica. Review ?penguins to learn about the variables we will be using.\n\npen &lt;- palmerpenguins::penguins\n\n\n\n\n\n\n\n\n[RAD] write a paragraph here about how this section is organized.",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "data_viz.html#one-categorical-variable",
    "href": "data_viz.html#one-categorical-variable",
    "title": "2  Visualizing Data",
    "section": "2.3 One Categorical Variable",
    "text": "2.3 One Categorical Variable\nBoth Nominal and Ordinal data types can be visualized using the same methods: tables, barcharts and pie charts.\n\n2.3.1 Tables\nFrequency and proportion (relative frequency) tables are the most common way to get summary statistics of a categorical variable.\n\nbasegtsummary\n\n\nThe table() function produces a frequency table, where each entry represents the number of records in the data set holding the corresponding labeled value.\n\ntable(pen$species)\n\n\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\n\nThere are 152 Adelie penguins, 68 Chinstrap penguins, and 124 Gentoo penguins in this dataset.\n\n\n\n\n\n\n\n\n\nRobin add text here.\n\nbrief explanation of how it works. Defaults to creating a summary table of the entire data set, so you have to pre-select out only the variables you want to get a summary of. Direct link to function tbl_summary\n\n\n\n\npen |&gt; select(species) |&gt; tbl_summary()\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 3441\n\n\n\n\nspecies\n\n\n\n\n    Adelie\n152 (44%)\n\n\n    Chinstrap\n68 (20%)\n\n\n    Gentoo\n124 (36%)\n\n\n\n1 n (%)\n\n\n\n\n\n\n\npen |&gt; select(species, island) |&gt; tbl_summary()\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 3441\n\n\n\n\nspecies\n\n\n\n\n    Adelie\n152 (44%)\n\n\n    Chinstrap\n68 (20%)\n\n\n    Gentoo\n124 (36%)\n\n\nisland\n\n\n\n\n    Biscoe\n168 (49%)\n\n\n    Dream\n124 (36%)\n\n\n    Torgersen\n52 (15%)\n\n\n\n1 n (%)\n\n\n\n\n\n\n\n\nThis can be advantageous if you want to have a single table showing the frequency distribution of multiple variables.\n\n\n\n\n\n2.3.2 Frequency Barcharts / Barplots\n\n\n\n\n\n\n\nadjust explanatory text here\n\n\n\n\nA Barchart or barplot takes these frequencies, and draws bars along the X-axis where the height of the bars is determined by the frequencies seen in the table.\n\nbaseggplotsjPlot\n\n\nTo create a barplot/barchart in base graphics requires the data to be in summarized in a table form first. Then the result of the table is plotted. The first argument is the table to be plotted, the main argument controls the title.\n\nps &lt;- table(pen$species)\nbarplot(ps, main=\"Barchart using base graphics\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnote yaxis direction and no labels at top of bar\n\n\n\n\n\n\nThe geometry needed to draw a barchart in ggplot is geom_bar().\n\nggplot(pen, aes(x=species)) + geom_bar()\n\n\n\n\n\n\n\n\nAdding Annotation\nThe biggest addition to increase the readability of a barchart is to add the frequencies on top of the bars.\n\nggplot(pen, aes(x=species)) + theme_bw() + \n    geom_bar(aes(y = ..count..)) + ggtitle(\"Frequency of penguins by species\") + \n    geom_text(aes(y=..count.. + 10, label=..count..), stat='count', size = 5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexplain why this is the current best option\n\n\n\n\n\nplot_frq(pen, species,\n         title = \"Count of penguins by species\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.3 Proportion Barcharts\n\nggplotbasesjplot\n\n\nOften you don’t want to compare counts but percents. To accomplish this, we have to aggregate the data to calculate the proportions first, then plot the aggregated data using geom_col to create the columns.\n\nspec.props &lt;- data.frame(prop.table(table(pen$species)))\nspec.props # what does this data look like? \n\n       Var1      Freq\n1    Adelie 0.4418605\n2 Chinstrap 0.1976744\n3    Gentoo 0.3604651\n\nggplot(spec.props, aes(x=Var1, y=Freq)) + geom_col() + \n  ylab(\"Proportion\") + xlab(\"Species\") + \n  ggtitle(\"Proportion of penguins by species\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncreate a ‘base’ tab, but with only space for robin to write\n\n\n\n\n\n\nThis is the same barchart generated by the frequency example, with proportion shown as a percentage below each species’ count.\n\nplot_frq(pen, species,\n         title = \"Count of penguins by species\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.4 Other\n\n\n\n\n\n\n\nRAD add some intro text here.\n\n\n\n\n\nCleveland Dot PlotsPie ChartWaffle Chart\n\n\nAnother way to visualize categorical data that takes up less ink than bars is a Cleveland dot plot. Here again we are plotting summary data instead of the raw data. This uses the geom_segment that draws the lines from x=0 to the value of the proportion (named Freq because of the way data.frame works).\n\nggplot(spec.props, aes(x=Freq, y=Var1)) +  \n  geom_point(size = 3) + xlab(\"Proportion of penguins\") + \n  theme_bw() + ylab(\"Species\") +\n  geom_segment(aes(x=0, xend=Freq, y=Var1, yend=Var1), color='grey50')\n\n\n\n\n\n\n\n\n\n\nJust like barplot(), pie() takes a table object as it’s argument.\nbase\n\nps &lt;- table(pen$species)\npie(ps)\n\n\n\n\n\n\n\n\nPie charts are my least favorite plotting type. Human eyeballs can’t distinguish between angles as well as we can with heights. A mandatory piece needed to make the wedges readable is to add the percentages of each wedge.\n\npie(ps, labels = paste0(names(ps), ' (', prop.table(ps)*100, \"%)\"))\n\n\n\n\n\n\n\n\nggplot\nAnd here I thought pie charts couldn’t get worse… I’m not a fan at all of the ggplot version. So i’m not even going to show it. STHDA has a great tutorial that does show you how to make one.\nHowever – Never say never. Storytelling with data has a blog post with an example of a good use of pie charts.\n\n\nThis type of chart is not natively found in the ggplot2 package, but its own waffle package. These are great for infographics.\n\nwaffle(ps/10, rows=5, size=0.5, \n       title=\"Species of penguins\", \n       xlab=\"1 square == 10 penguins\")",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "data_viz.html#one-continuous-variable",
    "href": "data_viz.html#one-continuous-variable",
    "title": "2  Visualizing Data",
    "section": "2.4 One Continuous Variable",
    "text": "2.4 One Continuous Variable\nHere we can look at the bill length and depth, flipper length, and body mass of the penguins.\n\n2.4.1 Dotplot\n\n\n\n\n\n\n\nRAD change this to an introduction/motivating example instead of describing a plot type.\n\n\n\n\n\nplot(pen$bill_length_mm)\n\n\n\n\n\n\n\n\nThe base function plot() creates a dotplot for a continuous variable. The value of the variable is plotted on the y axis, and the index, or row number, is plotted on the x axis. This gives you a nice, quick way to see the values of the data.\nOften you are not interested in the individual values of each data point, but the distribution of the data. In other words, where is the majority of the data? Does it look symmetric around some central point? Around what values do the bulk of the data lie?\n\n\n2.4.2 Histograms\nRather than showing the value of each observation, we prefer to think of the value as belonging to a . The height of the bars in a histogram display the frequency of values that fall into those of those bins. For example if we cut the bill length distribution into 7 bins of equal width, the frequency table would look like this:\n\ntable(cut(pen$bill_length_mm, 7))\n\n\n  (32.1,36]     (36,40]   (40,43.9] (43.9,47.8] (47.8,51.7] (51.7,55.7] \n         26          74          67          81          75          15 \n(55.7,59.6] \n          4 \n\n\nIn a histogram, the binned counts are plotted as bars into a histogram. Note that the x-axis is continuous, so the bars touch. This is unlike the barchart that has a categorical x-axis, and vertical bars that are separated.\n\nbaseggplotggpubr\n\n\nYou can make a histogram in base graphics super easy.\n\nhist(pen$bill_length_mm)\n\n\n\n\n\n\n\n\nAnd it doesn’t take too much to clean it up. Here you can specify the number of bins by specifying how many breaks should be made in the data (the number of breaks controls the number of bins, and bin width) and use col for the fill color.\n\nhist(pen$bill_length_mm, xlab=\"Bill length in mm\", main=\"Histogram of penguin bill lengths\", col=\"cyan\", breaks=20)\n\n\n\n\n\n\n\n\n\n\n\nggplot(pen, aes(x=bill_length_mm)) + geom_histogram(binwidth = 2.2)\n\n\n\n\n\n\n\n\nThe binwidth here is set by looking at the cut points above that were used to create 7 bins. Notice that darkgrey is the default fill color, but makes it hard to differentiate between the bars. So we’ll make the outline black using colour, and fill the bars with white.\n\nggplot(pen, aes(x=bill_length_mm)) + geom_histogram(colour=\"black\", fill=\"white\") + \n  ggtitle(\"Distribution of penguin bill lengths\")\n\n\n\n\n\n\n\n\nNote I did not specify the binwidth argument here. The size of the bins can hide features from your graph, the default value for ggplot2 is range/30 and usually is a good choice.\n\n\nThe ggpubr package is based on ggplot2, with simpler syntax for quickly generating polished graphs.\n\ngghistogram(pen, x = \"bill_length_mm\",\n            title = \"Distribution of penguin bill lengths\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.3 Density plots\nTo get a better idea of the true shape of the distribution we can “smooth” out the bins and create what’s called a density plot or curve. Notice that the shape of this distribution curve is much more… “wigglier” than the histogram may have implied.\n\nbaseggplot2ggpubr\n\n\n\n# plot(density(na.omit(pen$bill_length_mm))) \npen$bill_length_mm |&gt; na.omit() |&gt; density() |&gt; plot()\n\n\n\n\n\n\n\n\nAwesome title huh? (NOT)\n\n\n\n\n\n\n\ndiscuss why na.omit and nesting vs pipe example\n\n\n\n\n\n\n\nggplot(pen, aes(x=bill_length_mm)) + geom_density()\n\n\n\n\n\n\n\n\n\n\n\nggdensity(pen, x = \"bill_length_mm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.4 Histograms + density\nOften is is more helpful to have the density (or kernel density) plot on top of a histogram plot.\n\nbaseggplotggpubr\n\n\nSince the height of the bars in a histogram default to showing the frequency of records in the data set within that bin, we need to 1) scale the height so that it’s a relative frequency, and then use the lines() function to add a density() line on top.\n\nhist(pen$bill_length_mm, prob=TRUE)\nlines(density(na.omit(pen$bill_length_mm)), col=\"blue\")\n\n\n\n\n\n\n\n\n\n\nThe syntax starts the same, we’ll add a new geom, geom_density and color the line blue. Then we add the histogram geom using geom_histogram but must specify that the y axis should be on the density, not frequency, scale. Note that this has to go inside the aesthetic statement aes(). I’m also going to get rid of the fill by using NA so it doesn’t plot over the density line.\n\nggplot(pen, aes(x=bill_length_mm)) + geom_density(col=\"blue\") + \n  geom_histogram(aes(y=..density..), colour=\"black\", fill=NA)\n\n\n\n\n\n\n\n\n\n\n\ngghistogram(pen,\n            x = \"bill_length_mm\", \n            add_density = TRUE,\n            add.params = list(color=\"blue\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.5 Boxplots\nAnother very common way to visualize the distribution of a continuous variable is using a boxplot. Boxplots are useful for quickly identifying where the bulk of your data lie. R specifically draws a “modified” boxplot where values that are considered outliers are plotted as dots.\n\nbaseggplotggpubr\n\n\n\nboxplot(pen$bill_length_mm)\n\n\n\n\n\n\n\n\nNotice that the only axis labeled is the y=axis. Like a dotplot the x axis, or “width”, of the boxplot is meaningless here. We can make the axis more readable by flipping the plot on it’s side.\n\nboxplot(pen$bill_length_mm, horizontal = TRUE, main=\"Distribution of penguin bill lengths\", xlab=\"Bill length in mm\")\n\n\n\n\n\n\n\n\nHorizontal is a bit easier to read in my opinion.\n\n\nWhat about ggplot? ggplot doesn’t really like to do univariate boxplots. We can get around that by specifying that we want the box placed at a specific x value.\n\nggplot(pen, aes(x=1, y=bill_length_mm)) + geom_boxplot()\n\n\n\n\n\n\n\n\nTo flip it horizontal you may think to simply swap x and y? Good thinking. Of course it wouldn’t be that easy. So let’s just flip the whole darned plot on it’s coordinate axis.\n\nggplot(pen, aes(x=1, y=bill_length_mm)) + geom_boxplot() + coord_flip()\n\n\n\n\n\n\n\n\n\n\n\nggboxplot(pen, x=1, y=\"bill_length_mm\", \n          orientation = \"horizontal\",\n          main = \"Distribution of penguin bill lengths\",\n          ylab = \"Bill length in mm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.6 Violin plots\n\n\n\n\n\n\n\nbrief intro - usage, how they’re different from boxplots\n\n\n\n\n\nggplotggpubr\n\n\n\nggplot(pen, aes(x=1, y=bill_length_mm)) + geom_violin()\n\n\n\n\n\n\n\n\n\n\n\nggviolin(pen, x=1, y=\"bill_length_mm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.7 Boxplot + violin plots\nOverlaying a boxplot and a violin plot serves a similar purpose to Histograms + Density plots.\n\nggplotggpubr\n\n\n\nggplot(pen, aes(x=1, y=bill_length_mm)) + geom_violin() + geom_boxplot()\n\n\n\n\n\n\n\n\nBetter appearance - different levels of transparency of the box and violin.\n\nggplot(pen, aes(x=1, y=bill_length_mm)) + xlab(\"\") + theme_bw() + \n              geom_violin(fill=\"blue\", alpha=.1) + \n              geom_boxplot(fill=\"blue\", alpha=.5, width=.2) + \n              theme(axis.title.x=element_blank(),\n              axis.text.x=element_blank(),\n              axis.ticks.x=element_blank())\n\n\n\n\n\n\n\n\n\n\n\nggviolin(pen, x=1, y=\"bill_length_mm\", \n         alpha=.2, fill=\"blue\", \n         add = \"boxplot\", add.params = list(alpha=.4),\n         xlab=\"\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.8 Normal QQ plots\nThe last useful plot that we will do on a single continuous variable is to assess the normality of the distribution. Basically how close the data follows a normal distribution.\n\nbaseggplotggpubr\n\n\n\nqqnorm(pen$body_mass_g)\nqqline(pen$body_mass_g, col=\"red\")\n\n\n\n\n\n\n\n\nThe line I make red because it is a reference line. The closer the points are to following this line, the more “normal” the shape of the distribution is. Price has some pretty strong deviation away from that line. Below I have plotted what a normal distribution looks like as an example of a “perfect” fit.\n\nz &lt;- rnorm(1000)\nqqnorm(z)\nqqline(z, col=\"blue\")\n\n\n\n\n\n\n\n\n\n\nqq (or qnorm) plots specifically plot the data against a theoretical distribution. That means in the aes() aesthetic argument we don’t specify either x or y, but instead the sample= is the variable we want to plot.\n\nggplot(pen, aes(sample=body_mass_g)) + stat_qq()\n\n\n\n\n\n\n\n\n\n\n\nggqqplot(pen, x=\"body_mass_g\")",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "data_viz.html#categorical-v.-categorical",
    "href": "data_viz.html#categorical-v.-categorical",
    "title": "2  Visualizing Data",
    "section": "2.5 Categorical v. Categorical",
    "text": "2.5 Categorical v. Categorical\n\n2.5.1 Two-way tables\n\n\n\n\n\n\n\nintro to two way frequency and proportion tables\n\n\n\n\n\nFrequencyProportion\n\n\nCross-tabs, cross-tabulations and two-way tables (all the same thing, different names) can be created by using the table() function.\nThe frequency table is constructed using the table() function.\n\ntable(pen$island, pen$species)\n\n           \n            Adelie Chinstrap Gentoo\n  Biscoe        44         0    124\n  Dream         56        68      0\n  Torgersen     52         0      0\n\n\nThere are 44 Adelie penguins and 124 Gentoo penguins on Biscoe Island, 56 Adelie and 68 Chinstrap penguins on Dream Island, and 52 Adelie penguins on Torgersen Island.\n\n\nChoose your percentages depending on your research question. What are you wanting to compare?\nBest practices:\n\nExplanatory variable on the rows\nResponse variable on the columns\nCalculate row %’s as the % of the response for each explanatory group.\n\nHere are demonstrations of how the interpretation of the percents change depending on what the denominator is.\nCell proportions\nWrapping prop.table() around a table gives you the cell proportions.\n\nprop.table(table(pen$island, pen$species))\n\n           \n               Adelie Chinstrap    Gentoo\n  Biscoe    0.1279070 0.0000000 0.3604651\n  Dream     0.1627907 0.1976744 0.0000000\n  Torgersen 0.1511628 0.0000000 0.0000000\n\n\n12.8% of all penguins are Adelie penguins living on Biscoe Island, and 36% are Gentoo penguins living on Biscoe Island, so about 39% of all penguins in our data are living on Biscoe Island.\nRow proportions\nTo get the row proportions, you specify margin=1. The percentages now add up to 1 across the rows.\n\nround(prop.table(table(pen$island, pen$species), margin=1),3)\n\n           \n            Adelie Chinstrap Gentoo\n  Biscoe     0.262     0.000  0.738\n  Dream      0.452     0.548  0.000\n  Torgersen  1.000     0.000  0.000\n\n\n26.2% of penguins on Biscoe Island are from the Adelie species. The remaining 73.8% on the island are Gentoo penguins.\nColumn proportions\nTo get the column proportions, you specify margin=2. The percentages now add up to 1 down the columns.\n\nround(prop.table(table(pen$island, pen$species), margin=2),3)\n\n           \n            Adelie Chinstrap Gentoo\n  Biscoe     0.289     0.000  1.000\n  Dream      0.368     1.000  0.000\n  Torgersen  0.342     0.000  0.000\n\n\n0.289% of all Adelie penguins are on Biscoe Island. Chinstrap and Gentoo species are endemic to a single island, so 100% of their populations are on Dream and Torgersen Island, respectively.\n\n\n\n\n\n2.5.2 Grouped bar charts: stacked\nTo compare proportions of one categorical variable within the same level of another, we can use grouped barcharts.\n\nbaseggplot\n\n\nAs before, the object to be plotted needs to be the result of a table.\n\nis &lt;- table(pen$island, pen$species)\nbarplot(is)\n\n\n\n\n\n\n\n\n\n\nAgain plot the cut on the x axis, but then fill using the second categorical variable. This has the effect of visualizing the row percents from the table above, or the percent of species on each island.\n\nggplot(pen, aes(x=island, fill=species)) + geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.3 Grouped bar charts: side by side\nStacked bars can be difficult to interpret, and very difficult to compare values between groups. A side by side barchart is preferable. The beside=TRUE is what controls the placement of the bars.\n\nbaseggplot\n\n\n\nbarplot(is, main=\"quick side by side barchart using base graphics\", \n        beside=TRUE)\n\n\n\n\n\n\n\n\n\n\nAgain the default is a stacked barchart. So we just specify position=dodge to put the bars side by side.\n\nggplot(pen, aes(x=island, fill=species)) + geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\nAnd look, an automatic legend. What if I wanted to better compare island population proportions within species? This is the column percentages. Just switch which variable is the x axis and which one is used to fill the colors!\n\nggplot(pen, aes(x=species, fill=island)) + geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\nFor more than 2 colors I do not recommend choosing the colors yourself. I know little about color theory so I use the built-in color palettes. Here is a great cheatsheet about using color palettes.\nAnd this easy change is why we love ggplot2.\n\n\n\n\n\n2.5.4 Grouped bar charts with percentages\nNot as easy as one would hope, but the solution is to calculate the desired percentages first and then plot the summary data using either geom_bar(stat='identity') or geom_col().\n\ncalc.props &lt;- pen %&gt;% group_by(island, species) %&gt;%\n              summarise(count=n()) %&gt;%\n              mutate(pct=round(count/sum(count),3))\ncalc.props\n\n# A tibble: 5 × 4\n# Groups:   island [3]\n  island    species   count   pct\n  &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Biscoe    Adelie       44 0.262\n2 Biscoe    Gentoo      124 0.738\n3 Dream     Adelie       56 0.452\n4 Dream     Chinstrap    68 0.548\n5 Torgersen Adelie       52 1    \n\n\n\nggplotsjPlot\n\n\nSince we’re plotting summary data, the height of the bars is specified using y=pct.\n\nggplot(calc.props, aes(x=island, fill=species, y=pct)) +\n                  geom_col(position=\"dodge\") + theme_bw() \n\n\n\n\n\n\n\n\nNow set some options to the y axis using scale_y_continuous() to make the graph more accurate and readable. The labels=percent comes from the scales package.\n\nggplot(calc.props, aes(x=island, fill=species, y=pct)) +\n                  geom_col(position=\"dodge\") + theme_bw() +\n                  scale_y_continuous(limits=c(0,1), labels=percent)\n\n\n\n\n\n\n\n\n\n\nsjPlot does a very nice job of being able to cleanly show not only n’s but percents.\n\nplot_xtab(pen$island, pen$species, margin=\"row\", coord.flip = TRUE) \n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.5 Mosaic plots\nBut what if you want to know how two categorical variables are related and you don’t want to look at two different barplots? Mosaic plots are a way to visualize the proportions in a table. So here’s the two-way table we’ll be plotting.\n\ntable(pen$island, pen$species)\n\n           \n            Adelie Chinstrap Gentoo\n  Biscoe        44         0    124\n  Dream         56        68      0\n  Torgersen     52         0      0\n\n\nThe syntax for a mosaic plot uses model notation, which is basically y ~ x where the ~ is read as “twiddle” or “tilde”. It’s to the left of your 1 key.\n\nmosaicplot(island~species, data=pen)\n\n\n\n\n\n\n\n\nHelpful, ish. Here are two very useful options. In reverse obviousness, color applies shades of gray to one of the factor levels, and shade applies a color gradient scale to the cells in order of what is less than expected (red) to what is more than expected (blue) if these two factors were completely independent.\n\npar(mfrow=c(1,2)) # display the plots in 1 row and 2 columns\nmosaicplot(sex~species, data=pen, color=TRUE)\nmosaicplot(island~species, data=pen, shade=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nswitch out old interpretation for penguins\n\nFor example, there are fewer ‘Very Good’ cut diamonds that are color ‘G’, and fewer ‘Premium’ cut diamonds that are color ‘H’. As you can see, knowing what your data means when trying to interpret what the plots are telling you is essential.\n\n\n\nThat’s about all the ways you can plot categorical variables.\nIf you are wondering why there was no 3D barcharts demonstrated see here, here, and here for other ways you can really screw up your visualization.",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "data_viz.html#continuous-v.-continuous",
    "href": "data_viz.html#continuous-v.-continuous",
    "title": "2  Visualizing Data",
    "section": "2.6 Continuous v. Continuous",
    "text": "2.6 Continuous v. Continuous\n\n2.6.1 Scatterplot\nThe most common method of visualizing the relationship between two continuous variables is by using a scatterplot.\n\nbaseggplot\n\n\nBack to the plot() command. Here we use model notation again, so it’s \\(y~x\\).\n\nplot(flipper_length_mm~body_mass_g, data=pen)\n\n\n\n\n\n\n\n\nLooks like for the most part as penguin body mass increases so does flipper length. That makes sense.\n\n\nWith ggplot we specify both the x and y variables, and add a point.\n\nggplot(pen, aes(x=body_mass_g, y=flipper_length_mm)) + geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.6.2 Adding lines to the scatterplots\nTwo most common trend lines added to a scatterplots are the “best fit” straight line and the “lowess” smoother line.\n\nbaseggplot\n\n\nThe best fit line (in blue) gets added by using the abline() function wrapped around the linear model function lm(). Note it uses the same model notation syntax and the data= statement as the plot() function does. The lowess line is added using the lines() function, but the lowess() function itself doesn’t allow for the data= statement so we have to use $ sign notation.\n\nplot(flipper_length_mm~body_mass_g, data=pen)\nabline(lm(flipper_length_mm~body_mass_g, data=pen), col=\"blue\")\nlines(lowess(na.omit(pen$flipper_length_mm)~na.omit(pen$body_mass_g)),\n      col=\"red\")\n\n\n\n\n\n\n\n\n\n\nWith ggplot, we just add a geom_smooth() layer.\n\nggplot(pen, aes(x=body_mass_g, y=flipper_length_mm)) + geom_point() + geom_smooth() \n\n\n\n\n\n\n\n\nHere the point-wise confidence interval for this lowess line is shown in grey. If you want to turn the confidence interval off, use se=FALSE. Also notice that the smoothing geom uses a different function or window than the lowess function used in base graphics.\nHere it is again using the ggplot plotting function and adding another geom_smooth() layer for the lm (linear model) line in blue, and the lowess line (by not specifying a method) in red.\n\nggplot(pen, aes(x=body_mass_g, y=flipper_length_mm)) + geom_point() + \n  geom_smooth(se=FALSE, method=\"lm\", color=\"blue\") + \n  geom_smooth(se=FALSE, color=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.6.3 Line plots\nLine plots connect each dot with a straight line. This is most often done when measuring trends of the response as the value of x increases (such as a time series)\nWe saw earlier that body_mass_g and flipper_length_mm seemed possibly linear. Let see how the average flipper length changes with body mass.\n\nmm.per.g &lt;- pen %&gt;% group_by(body_mass_g) %&gt;% summarise(mean = mean(flipper_length_mm))\n\n\nbaseggplot\n\n\nFor base graphics, type=‘b’ means both points and lines, ‘l’ gives you just lines and ‘p’ gives you only points. You can find more plotting character options under ?pch.\n\nplot(mean~body_mass_g, data=mm.per.g, type='l')\n\n\n\n\n\n\n\n\n\n\nWith ggplot we specify that we want a line geometry only.\n\nggplot(mm.per.g, aes(x=body_mass_g, y=mean)) + geom_line()\n\n\n\n\n\n\n\n\n\n\n\nHow does this relationship change with penguin species? First lets get the average flipper length per combination of body mass and species\n\nmmpgs &lt;- pen %&gt;% group_by(body_mass_g, species) %&gt;% summarise(mean = mean(flipper_length_mm))\n\n\nggplotbase\n\n\nThis is where ggplot starts to excel in it’s ease of creating more complex plots. All we have to do is specify that we want the lines colored by the cut variable.\n\nggplot(mmpgs, aes(x=body_mass_g, y=mean, col=species)) + geom_line()\n\n\n\n\n\n\n\n\nAnd we get one line per species\n\n\nThis plot can be created in base graphics, but it takes an advanced knowledge of the graphics system to do so. So I do not show it here.",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "data_viz.html#continuous-v.-categorical",
    "href": "data_viz.html#continuous-v.-categorical",
    "title": "2  Visualizing Data",
    "section": "2.7 Continuous v. Categorical",
    "text": "2.7 Continuous v. Categorical\nCreate an appropriate plot for a continuous variable, and plot it for each level of the categorical variable.\n\n2.7.1 Dotplot/strip chart\nDotplots can be very useful when plotting dots against several categories. They can also be called stripcharts.\n\nbaseggplot\n\n\n\nstripchart(body_mass_g ~ species, data=pen)\n\n\n\n\n\n\n\n\n\n\nWe can reproduce the same thing by plotting one continuous variable against one categorical variable, and adding a layer of points. I’d argue that horizontal looks better due to the axis-labels.\n\na &lt;- ggplot(pen, aes(y=body_mass_g, x=species)) + geom_point()\nb &lt;- ggplot(pen, aes(y=species, x=body_mass_g)) + geom_point()\ngrid.arrange(a, b, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.7.2 Grouped boxplots\n\nbaseggplotAdding violins\n\n\nBase graphics plots grouped boxplots with also just the addition of a twiddle (tilde) ~. Another example of where model notation works.\n\nboxplot(bill_length_mm~species, data=pen)\n\n\n\n\n\n\n\n\n\n\nA simple addition, just define your x and y accordingly.\n\nggplot(pen, aes(x=species, y=bill_depth_mm, fill=species)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nViolin plots can be overlaid here as well.\n\nggplot(pen, aes(x=species, y=bill_depth_mm, fill=species)) +\n        geom_violin(alpha=.1) + \n        geom_boxplot(alpha=.5, width=.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.7.3 Grouped histograms\n\nggplotbase\n\n\nBy default ggplot wants to overlay all plots on the same grid. This doesn’t look to good with histograms. Instead you can overlay density plots\n\na &lt;- ggplot(pen, aes(x=bill_depth_mm, fill=species)) + geom_histogram()\nb &lt;- ggplot(pen, aes(x=bill_depth_mm, fill=species)) + geom_density() \ngrid.arrange(a,b, ncol=2)\n\n\n\n\n\n\n\n\nThe solid fills are still difficult to read, so we can either turn down the alpha (turn up the transparency) or only color the lines and not the fill.\n\nc &lt;- ggplot(pen, aes(x=bill_depth_mm, fill=species)) + geom_density(alpha=.2)\nd &lt;- ggplot(pen, aes(x=bill_depth_mm, fill=species)) + geom_density() \ngrid.arrange(c,d, ncol=2)\n\n\n\n\n\n\n\n\n\n\nThere is no easy way to create grouped histograms in base graphics, so we will skip it.\n\n\n\n\n\n2.7.4 Ridgeline plots\n\n\n\n\n\n\ncheck this section for updates - has this changed?\n\n\n\nSomewhat new (2017), ridgeline plots have not been added to the base distribution of ggplot2 yet. For now it’s available in the ggridges package. Really good way to visualize density plots without the overlapping issue.\n\nggplot(pen, aes(x=bill_depth_mm, y=species)) + geom_density_ridges()",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "data_viz.html#faceting-paneling",
    "href": "data_viz.html#faceting-paneling",
    "title": "2  Visualizing Data",
    "section": "2.8 Faceting / paneling",
    "text": "2.8 Faceting / paneling\nThis is a good place to introduce a term called faceting. The definition is a particular aspect or feature of something, or one side of something many-sided, especially of a cut gem. Basically instead of plotting the grouped graphics on the same plotting area, we let each group have it’s own plot, or facet.\nWe add a facet_wrap() and specify that we want to panel on the species group. Note the twiddle in front of species\n\nggplot(pen, aes(x=bill_depth_mm, fill=species)) + geom_density() + facet_wrap(~species)\n\n\n\n\n\n\n\n\nThe grid placement can be semi-controlled by using the ncol argument in the facet_wrap() statement.\n\nggplot(pen, aes(x=bill_depth_mm, fill=species)) + \n  geom_density() + facet_wrap(~species, ncol=4)\n\n\n\n\n\n\n\n\nIt is important to compare distributions across groups on the same scale, and our eyes can compare items vertically better than horizontally. So let’s force ncol=1.\n\nggplot(pen, aes(x=bill_depth_mm, fill=species)) + \n  geom_density() + facet_wrap(~species, ncol=1)\n\n\n\n\n\n\n\n\n\n2.8.1 Paneling on two variables\nWho says we’re stuck with only faceting on one variable? A variant on facet_wrap is facet_grid. Here we can specify multiple variables to panel on.\n\nggplot(pen, aes(x=body_mass_g, fill=sex)) + geom_density() + facet_grid(sex~island)\n\n\n\n\n\n\n\n\nHow about plotting bill length against flipper length, for all combinations of species and island, with the points further separated by sex?\n\nggplot(pen, aes(x=flipper_length_mm, y=bill_length_mm, color=sex)) + geom_point() + facet_grid(island~species)",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "data_viz.html#multiple-plots-per-window",
    "href": "data_viz.html#multiple-plots-per-window",
    "title": "2  Visualizing Data",
    "section": "2.9 Multiple plots per window",
    "text": "2.9 Multiple plots per window\n\nbaseggplotQuarto\n\n\nI use par(mfrow=c(r,c)) for base graphics, where r is the number of rows and c the number of columns.\n\npar(mfrow=c(1,3))\nplot(pen$bill_depth_mm)\nplot(pen$species)\nplot(pen$bill_length_mm ~ pen$bill_depth_mm)\n\n\n\n\n\n\n\n\nOther resources including learning about layouts. Multipanel plotting with base graphics\n\n\nUse the grid.arrange function in the gridExtra package. I’ve done it several times above. You assign the output of a ggplot object to an object (here it’s plot1 and plot2). Then you use grid.arrange() to arrange them either side by side or top and bottom.\n\na &lt;- ggplot(pen, aes(x=bill_depth_mm, fill=species)) + geom_density(alpha=.2)\nb &lt;- ggplot(pen, aes(x=bill_depth_mm, fill=species)) + geom_density() \ngrid.arrange(a,b, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nadd example or reference to https://quarto.org/docs/get-started/computations/rstudio.html#multiple-figures",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "data_viz.html#multivariate-3-variables",
    "href": "data_viz.html#multivariate-3-variables",
    "title": "2  Visualizing Data",
    "section": "2.10 Multivariate (3+ variables)",
    "text": "2.10 Multivariate (3+ variables)\nThis is not much more complicated than taking an appropriate bivariate plot and adding a third variable through paneling, coloring, or changing a shape.\nThis is trivial to do in ggplot, not trivial in base graphics. So I won’t show those examples.\n\n2.10.1 Three continuous\nContinuous variables can also be mapped to the size of the point. Here I set the alpha on the points so we could see the overplotting (many points on a single spot). So the darker the spot the more data points on that spot.\n\nggplot(pen, aes(x=bill_length_mm, y=bill_depth_mm, size=body_mass_g)) + geom_point(alpha=.2)\n\n\n\n\n\n\n\n\n\n\n2.10.2 Scatterplot matrix\nA scatterplot matrix allows you to look at the bivariate comparison of multiple pairs of variables simultaneously. First we need to trim down the data set to only include the variables we want to plot, then we use the pairs() function.\n\nc.vars &lt;- pen[,c('flipper_length_mm', 'bill_length_mm', 'bill_depth_mm', \n                 'body_mass_g')]\npairs(c.vars)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReplace original interpretation- We can see price has a non-linear relationship with X, Y and Z and x & y have a near perfect linear relationship.\n\n\n\n\n\n\n2.10.3 Two categorical and one continuous\nThis is very similar to side by side boxplots, one violin plot per sex, within each island\n\n\n\n\n\n\n\nReplace original interpretation- This is difficult to really see due to the large number of categories each factor has. The categorical variables in the penguins dataset have less categories w less combinations\n\n\n\n\n\nggplot(pen, aes(x=island, y=bill_length_mm, fill=species)) + geom_violin()\n\n\n\n\n\n\n\n\nBest bet here would be to panel on species and change the x axis to location.\n\nggplot(pen, aes(x=species, y=bill_length_mm, fill=species)) + geom_violin() + facet_wrap(~island)\n\n\n\n\n\n\n\n\n\n\n2.10.4 Two continuous and one categorical\nAnd lastly let’s look back at how we can play with scatterplots of using a third categorical variable (using ggplot2 only). We can color the points by species,\n\nggplot(pen, aes(x=flipper_length_mm, y=bill_length_mm, color=species)) + geom_point()\n\n\n\n\n\n\n\n\nWe could add a smoothing lowess line for each species separately,\n\nggplot(pen, aes(x=flipper_length_mm, y=bill_length_mm, color=species)) + geom_point() + geom_smooth(se=FALSE)\n\n\n\n\n\n\n\n\nAnd using grid.arrange we can include both visualizations.\n\na &lt;- ggplot(pen, aes(x=bill_length_mm, y=bill_depth_mm, color=species)) + geom_point() + ggtitle(\"Colored by species\")\nd &lt;- ggplot(pen, aes(x=bill_length_mm, y=bill_depth_mm, color=species)) + geom_point() + \n      geom_smooth(se=FALSE) +ggtitle(\"Lowess line per species\")\ngrid.arrange(a, d, ncol=2, nrow=1)\n\n\n\n\n\n\n\n\nOr we just panel by species.\n\nggplot(pen, aes(x=bill_length_mm, y=bill_depth_mm)) + geom_point() + facet_wrap(~species)\n\n\n\n\n\n\n\n\nWe could even change the color by sex, and shape by species.\n\nggplot(pen, aes(x=flipper_length_mm, y=bill_length_mm, color=sex, shape=species)) + geom_point() \n\n\n\n\n\n\n\n\nThat’s harder to read. So note that just because you can change an aesthetic, doesn’t mean you should. And just because you can plot things on the same axis, doesn’t mean you have to.\nBefore you share your plot with any other eyes, always take a step back and try to explain what it is telling you. If you have to take more than a minute to get to the point then it may be too complex and simpler graphics are likely warranted.",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "data_viz.html#troubleshooting",
    "href": "data_viz.html#troubleshooting",
    "title": "2  Visualizing Data",
    "section": "2.11 Troubleshooting",
    "text": "2.11 Troubleshooting\n\n2.11.1 Problem: Missing data showing up as a category in ggplot?\nGet rid of that far right bar!\n\nggplot(NCbirths, aes(x=marital)) + geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nUse dplyr to select only the variables you are going to plot, then pipe in the na.omit() at the end. It will create a temporary data frame (e.g) plot.data that you then provide to ggplot().\n\nplot.data &lt;- NCbirths %&gt;% select(marital) %&gt;% na.omit()\nggplot(plot.data, aes(x=marital)) + geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.11.2 Problem: Got numerical binary 0/1 data but want to plot it as categorical?\n\n\n\n\n\n\n\nInclude other related error messages: Continuous x aesthetic – did you forget aes(group=…)?\n\n\n\n\nConsider a continuous variable for the number of characters in an email num_char, and a 0/1 binary variable spam.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nCreate a second variable var_factor for plotting and keep the binary var as 0/1 for analysis.\n\nemail$spam_cat &lt;- factor(email$spam, labels=c(\"Ham\", \"Spam\"))\nggplot(email, aes(y=num_char, x=spam_cat)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.11.3 Problem: You want to change the legend title for a fill or color scale.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nAdd the name= argument to whatever layer you added that created the legend. Here I speciefied a fill, and it was a discrete variable. So I use the scale_fill_discrete() layer.\n\nggplot(email, aes(y=num_char, x=spam_cat, fill=spam_cat)) + geom_boxplot() + \n  scale_fill_discrete(name=\"Ya like Spam?\")\n\n\n\n\n\n\n\n\nHere, I colored the points by a discrete variable, so the layer is scale_color_discrete().\n\nggplot(email, aes(x=num_char, y=line_breaks, col=spam_cat)) + geom_point() +\n  scale_color_discrete(name=\"Ya like Spam?\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.11.4 Problem: You want to add means to boxplots.\nBoxplots are great. Even better with violin overlays. Know what makes them even better than butter? Adding a point for the mean. stat_summary is the layer you want to add. Check out this stack overflow post for more context.\n\nggplot(email, aes(x=spam_cat, y=num_char, fill=spam_cat)) +\n  geom_boxplot() +\n  stat_summary(fun.y=\"mean\", geom=\"point\", size=3, pch=17,color=\"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nI suggest playing around with size and plotting character pch to get a feel for how these work. You can also look at ?pch (and scroll down in the help file) to see the 25 default plotting characters.",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "data_viz.html#but-what-about",
    "href": "data_viz.html#but-what-about",
    "title": "2  Visualizing Data",
    "section": "2.12 But what about…",
    "text": "2.12 But what about…\n\nLegend adjustment: remove it, move it to another side, rename it\nCustom specified colors and shapes\n\nGo to the R Graphics Cookbook, 2e for these.\nOther visualizations:\n\nHeat maps\nWord clouds, or simpler\nInteractive plots - Look into plotly() and ggplotly(), start with their guide directed towards R users here or browse the Plotly R open source graphing library",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "data_viz.html#additional-resources",
    "href": "data_viz.html#additional-resources",
    "title": "2  Visualizing Data",
    "section": "2.13 Additional Resources",
    "text": "2.13 Additional Resources\nFor any Google Search - be sure to limit searches to within the past year or so. R packages get updated very frequently, and many functions change or become obsolete.\n\nComprehensive guides\n\nR Graphics 2e: The best book about using base graphics\nR Graphics Cookbook 2e: The best book for using ggplot2\nggplot2: Elegant Graphics for Data Analysis 3e\n\nSubject reference and tutorial sites\n\nMath 130 Lesson 07 - Creating graphics - more on ggplot2 syntax\nSTHDA: Statistical tools for high-throughput data analysis.\n\nggplot2 essentials\nggplot2 qq plot (quantile - quantile graph) : Quick start guide\nOther graphics options\n\nQuick-R / Datacamp:\n\nBasic Graphs\nggplot2\nScatterplots\n\nR-bloggers\n\nWaffle charts\nScatterplot matrices\n\n\nLiterate programming tools\n\nQuarto\n\nMultiple figure alignment\n\n\nHelp lists\n\nggplot2 Google Groups\nStackoverflow questions tagged ggplot2\nChico R users group",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "select_analysis.html",
    "href": "select_analysis.html",
    "title": "3  Selecting Appropriate Analyses",
    "section": "",
    "text": "Section in progress\n\n\n\nThis section covers how to choose appropriate analyses for any number and type of measurements. Think like, flow chart.\nConsiderations:\n\nPurpose of analysis.\n\nTypes of variables in data set.\n\nData used in analysis.\n\nAssumptions needed; satisfied?\n\nChoice of analyses is often arbitrary: consider several\n\nExample:\n5 independent variables: 3 interval, 1 ordinal, 1 nominal\n1 dependent variable: interval\nAnalysis options\n\nMultiple regression: pretend independent ordinal variable is an interval variable use dummy (0 /1) variables for nominal variables\nAnalysis of variance: categorize all independent variables\nAnalysis of covariance: leave variables as is, check assumptions\nLogistic regression: Categorize dependent variable: high, low\nSurvival analysis: IF dependent variable is time to an event\n\nUnsure? Do several and compare results.",
    "crumbs": [
      "Preparing Data for Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Selecting Appropriate Analyses</span>"
    ]
  },
  {
    "objectID": "foundations.html",
    "href": "foundations.html",
    "title": "4  Foundations for Inference",
    "section": "",
    "text": "Nothing here yet",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Foundations for Inference</span>"
    ]
  },
  {
    "objectID": "bivariate_analysis.html",
    "href": "bivariate_analysis.html",
    "title": "5  Bivariate Analysis",
    "section": "",
    "text": "5.1 Assumption of Independent Observations\nThe primary assumption of most standard statistical procedures is that observations are independent of each other. That is, the value of one observation does not change or affect another observation. However, there are many examples where measurements are made on subjects before and after a certain exposure or treatment (pre-post), or an experiment to compare two cell phone packages might use pairs of subjects that are the same age, sex and income level. One subject would be randomly assigned to the first phone package, the other in the pair would get the second phone package. This chapter only deals with non-correlated analyses, leaving that topic for a later chapter.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bivariate Analysis</span>"
    ]
  },
  {
    "objectID": "bivariate_analysis.html#choosing-appropriate-bivariate-analysis",
    "href": "bivariate_analysis.html#choosing-appropriate-bivariate-analysis",
    "title": "5  Bivariate Analysis",
    "section": "5.2 Choosing appropriate bivariate analysis",
    "text": "5.2 Choosing appropriate bivariate analysis\nChoosing which statistical analyses procedure is appropriate completely depending on the data types of the explanatory and response variable. This is a simplified table, only covering the common/standard types of bivariate analysis.\n\nfigure out how to get table here.\n\nFor this set of notes, the variable types are referred to using the first letter, e.g. Q for quantitative, B for binary, and C for categorical. Thus a T-test is a (Q \\(\\sim\\) B) analysis, and a correlation analysis is (Q \\(\\sim\\) Q) analysis.\nLinks to the example data used in this chapter.\n\nsource(\"_common.R\")\nlibrary(knitr)\nlibrary(kableExtra)\nload(here::here('data', 'addhealth_clean.Rdata'))\ncounty &lt;- read.csv(\"https://www.norcalbiostat.com/data/countyComplete.csv\", header=TRUE, stringsAsFactors = FALSE)\n\n\n\n\n\n\n\n\ncounty dataset will need to be replaced or example switched - no longer included in openintro & the csv on https://www.norcalbiostat.com/data/#countyComplete is missing federal spending column",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bivariate Analysis</span>"
    ]
  },
  {
    "objectID": "bivariate_analysis.html#bv-ttest",
    "href": "bivariate_analysis.html#bv-ttest",
    "title": "5  Bivariate Analysis",
    "section": "5.3 (Q~B) Two means: T-Test",
    "text": "5.3 (Q~B) Two means: T-Test\nIt is common to compare means from different samples. For instance, we might investigate the effectiveness of a certain educational intervention by looking for evidence of greater reading ability in the treatment group against a control group. That is, our research hypothesis is that reading ability of a child is associated with an educational intervention.\nThe null hypothesis states that there is no relationship, or no effect, of the educational intervention (binary explanatory variable) on the reading ability of the child (quantitative response variable). This can be written in symbols as follows:\n\\[H_{0}: \\mu_{1} = \\mu_{2}\\mbox{ or }\\qquad  H_{0}: \\mu_{1} -\\mu_{2}=0\\]\nwhere \\(\\mu_{1}\\) is the average reading score for students in the control group (no intervention) and \\(\\mu_{2}\\) be the average reading score for students in the intervention group. Notice it can be written as one mean equals the other, but also as the difference between two means equaling zero. The alternative hypothesis \\(H_{A}\\) states that there is a relationship:\n\\[H_{A}: \\mu_{1} \\neq \\mu_{2} \\qquad \\mbox{ or } \\qquad H_{A}: \\mu_{1}-\\mu_{2} \\neq 0\\]\n\n5.3.1 Assumptions\n\nThe data distribution for each group is approximately normal.\nThe scores are independent within each group.\nThe scores from the two groups are independent of each other (i.e. the two samples are independent).\n\n\n\n5.3.2 Sampling Distribution for the difference\nWe use \\(\\bar{x}_1 - \\bar{x}_2\\) as a point estimate for \\(\\mu_1 - \\mu_2\\), which has a standard error of\n\\[\nSE_{\\bar{x}_1 - \\bar{x}_2}\n   = \\sqrt{SE_{\\bar{x}_1}^2 + SE_{\\bar{x}_2}^2}\n     = \\sqrt{\\frac{\\sigma^{2}_{1}}{n_1} + \\frac{\\sigma^{2}_{2}}{n_2}}\n\\]\nSo the equations for a Confidence Interval is,\n\\[\n  \\left( \\bar{x}_{1} - \\bar{x}_{2} \\right) \\pm t_{\\frac{\\alpha}{2}, df}\n    \\sqrt{ \\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}} }\n\\]\nand Test Statistic is,\n\\[\n  t^{*} =  \\frac{\\left( \\bar{x}_{1} - \\bar{x}_{2} \\right) - d_{0}}\n       {\\left( \\sqrt{ \\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}} }\n       \\right )}\n\\]\nTypically it is unlikely that the population variances \\(\\sigma^{2}_{1}\\) and \\(\\sigma^{2}_{2}\\) are known so we will use sample variances \\(s^{2}_{1}\\) and \\(s^{2}_{2}\\) as estimates.\nWhile you may never hand calculate these equations, it is important to see the format, or structure, of the equations. Equation \\(\\ref{2sampCImean}\\) has the same format of\n\\[ \\mbox{point estimate} \\pm 2*\\mbox{standard error}\\]\nregardless what it is we’re actually trying to estimate. Thus in a pinch, you can calculate approximate confidence intervals for whatever estimate you are trying to understand, given only the estimate and standard error, even if the computer program does not give it to you easily or directly.\n\n\n5.3.3 Example: Smoking and BMI\nWe would like to know, is there convincing evidence that the average BMI differs between those who have ever smoked a cigarette in their life compared to those who have never smoked? This example uses the Addhealth dataset.\n1. Identify response and explanatory variables.\n\nThe quantitative response variable is BMI (variable )\nThe binary explanatory variable is whether the person has ever smoked a cigarette (variable )\n\n2. Visualize and summarize bivariate relationship.\n\n\n\n\n\n\nUsing na.omit() is dangerous! This will remove ALL rows with ANY missing data in it. Regardless if the missing values are contained in the variables you are interested in. The example below employs a trick/work around to not have NA values show in the output. We take the data set addhealth and then select the variables we want to plot, and then we use na.omit() to delete all rows with missing data. Then that is saved as a new, temporary data frame specifically named for this case (plot.bmi.smoke).\n\n\n\n\nnote for later. Move this explanation into data viz section.\n\n\nplot.bmi.smoke &lt;- addhealth %&gt;% select(eversmoke_c, BMI) %&gt;% na.omit()\n\nggplot(plot.bmi.smoke, aes(x=eversmoke_c, y=BMI, fill=eversmoke_c)) +\n      geom_boxplot(width=.3) + geom_violin(alpha=.4) +\n      labs(x=\"Smoking status\") +\n      scale_fill_viridis_d(guide=FALSE) +\n      stat_summary(fun.y=\"mean\", geom=\"point\", size=3, pch=17,\n      position=position_dodge(width=0.75))\n\n\n\n\n\n\n\n\n\nplot.bmi.smoke %&gt;% group_by(eversmoke_c) %&gt;%\n summarise(mean=mean(BMI, na.rm=TRUE),\n             sd = sd(BMI, na.rm=TRUE),\n             IQR = IQR(BMI, na.rm=TRUE))\n\n# A tibble: 2 × 4\n  eversmoke_c  mean    sd   IQR\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Non Smoker   29.7  7.76  9.98\n2 Smoker       28.8  7.32  9.02\n\n\nSmokers have an average BMI of 28.8, smaller than the average BMI of non-smokers at 29.7. Nonsmokers have more variation in their BMIs (sd 7.8 v. 7.3 and IQR 9.98 v. 9.02), but the distributions both look normal, if slightly skewed right.\n3. Write the relationship you want to examine in the form of a research question.\n\nNull Hypothesis: There is no relationship between BMI and smoking status.\nAlternate Hypothesis: There is a relationship between BMI and smoking status.\n\n4. Perform an appropriate statistical analysis.\nI. Let \\(\\mu_1\\) denote the average BMI for nonsmokers, and \\(\\mu_2\\) the average BMI for smokers.\n\n\\(\\mu_1 - \\mu_2 = 0\\) There is no difference in the average BMI between smokers and nonsmokers. \\(\\mu_1 - \\mu_2 \\neq 0\\) There is a difference in the average BMI between smokers and nonsmokers.\nWe are comparing the means between two independent samples. A Two-Sample T-Test for a difference in means will be conducted. The assumptions that the groups are independent is upheld because each individual can only be either a smoker or nonsmoker. The difference in sample means \\(\\bar{x_1} - \\bar{x_2}\\) is normally distributed – this is a valid assumption due to the large sample size and that differences typically are normally distributed. The observations are independent, and the variability is roughly equal (IQR 9.9 v. 9.0).\nWe use the t.test function, but use model notation of the format outcome \\(\\sim\\) category. Here, BMI is our continuous outcome that we’re testing across the (binary) categorical predictor eversmoke_c.\n\n\nt.test(BMI ~ eversmoke_c, data=addhealth)\n\n\n    Welch Two Sample t-test\n\ndata:  BMI by eversmoke_c\nt = 3.6937, df = 3395.3, p-value = 0.0002245\nalternative hypothesis: true difference in means between group Non Smoker and group Smoker is not equal to 0\n95 percent confidence interval:\n 0.3906204 1.2744780\nsample estimates:\nmean in group Non Smoker     mean in group Smoker \n                29.67977                 28.84722 \n\n\nWe have very strong evidence against the null hypothesis, \\(p = 0.0002\\).\n5. Write a conclusion in the context of the problem.\nOn average, nonsmokers have a significantly higher BMI by 0.83 (0.39, 1.27) compared to nonsmokers (\\(p = 0.0002\\)).\n\n\n\n\n\n\nAlways check the output against the direction you are testing. R always will calculate a difference as group 1 - group 2, and it defines the groups alphabetically. For example, for a factor variable that has groups A and B, R will automatically calculate the difference as A-B. In this example it is Nonsmoker - Smoker.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bivariate Analysis</span>"
    ]
  },
  {
    "objectID": "bivariate_analysis.html#bv-anova",
    "href": "bivariate_analysis.html#bv-anova",
    "title": "5  Bivariate Analysis",
    "section": "5.4 (Q~C) Multiple means: ANOVA",
    "text": "5.4 (Q~C) Multiple means: ANOVA\nFrequently, a researcher wants to compare the means of an outcome across three or more treatments in a single experiment. We might initially think to do pairwise comparisons (1v2, 1v3, 2v3) for a total of three comparisons. However, this strategy can be treacherous. If we have many groups and do many comparisons, it is likely that we will eventually find a difference just by chance, even if there is no difference in the populations.\nWhen we analyze a conventional two-treatment experiment, we are prepared to run a 1 in 20 risk of an apparently significant result arising purely by accident (the 5% chance of a Type I error). We regard such a risk as being fairly unlikely and feel justified in accepting with confidence any significant results we obtain.\nAnalyzing a single experiment as a series of 10 treatment pairs is a very different proposition. The chance of an apparently significant result arising purely by chance somewhere in the 10 analyses increases dramatically. Using a 5% error rate, the chance of NOT making at Type I error is .95. To not make a Type I error 10 times is \\(.95^{10} = .6\\). That means there is a 40% of making a Type I error! \n\n5.4.0.1 Example: Visual Comparison\nExamine the figure below. Compare groups I, II, and III. Can you visually determine if the differences in the group centers is due to chance or not? What about groups IV, V, and VI?\n\n\n\nSide-by-side dot plot for the outcomes for six groups.\n\n\nSo we need some method of comparing treatments for more than two groups at a time. This is done using an Analysis of Variance (ANOVA) model. ### Terminology\n\nResponse Variable: The response variable in the ANOVA setting is the quantitative (continuous) variable that we want to compare among the different treatments.\nFactor/Treatment: A property or characteristic (categorical variable) that allows us to distinguish the different populations from one another. An independent variable to be studied in an investigation such as temperature, type of plant, color of flower, location.\nFactor/Treatment level: Factors have different levels, such as 3 temperatures, 5 locations, 3 colors, etc.\nWithin-sample Variation: Variation within a sample from one population. Individuals who receive the same treatment will experience identical experimental conditions. The variation within each of the treatment groups must therefore be a consequence of solely random variation.\nBetween-sample Variation: Variation between samples. This is the difference between the group means. If some treatments are genuinely more effective than others, then we would expect to see relatively large differences between the treatment means and a relatively large between-treatments variation.\n\n\n\n5.4.1 Formulation of the One-way ANOVA model\nANOVA is a mathematical technique which uses a model based approach to partition the variance in an experiment into different sources of variance. This technique enables us to test if most the variation in the treatment means is due to differences between the groups.\nStarting with our generic conceptual understanding of statistical models:\n\nDATA = MODEL + RESIDUAL\n\nour MODEL for this situation is the group membership. Does knowing what group an observation is in tell you about the location of the data? The one-way (or one-factor) ANOVA model is\n\\[\ny_{ij} = \\mu_{i} + \\epsilon_{ij} \\qquad \\qquad\n\\epsilon_{ij} \\overset{iid}{\\sim} \\mathcal{N}(0,\\sigma^{2})\n\\]\nfor \\(i=1, \\ldots, I\\) factor levels and \\(j = 1, \\ldots, n_{i}\\) subjects within each factor level. The random error terms are independently and identically distributed (iid) normally with common variance.\nThe null and alternative hypotheses are always phrased as follows:\n\n\\(H_0\\): The mean outcome is the same across all groups. \\(\\mu_1 = \\mu_2 = \\cdots = \\mu_k\\)\n\\(H_A\\): At least one mean is different.\n\nHow do we compare means using an ANalysis Of VAriance? By comparing the portion of the variance in the outcome that is explained by the groups, to the portion that’s leftover due to unexplained randomness. Essentially we’re comparing the ratio of MODEL to RESIDUAL.\nThe total variation of the observed data is broken down into 2 parts:\n\nTotal Variation = Between Group Variation + Within Group Variation\n\nVariation is measured using the Sum of Squares (SS): The sum of the squares within a group (SSE), the sum of squares between groups (SSG), and the total sum of squares (SST).\nSSG (Between groups): Measures the variation of the \\(I\\) group means around the overall mean. \\[\n  SSG = \\sum_{i=1}^{I}n_{i}(\\bar{y}_{i.}-\\bar{y}..)^{2} = n_{1}(\\bar{y}_{1.}-\\bar{y}..)^{2} + n_{2}(\\bar{y}_{2.}-\\bar{y}..)^{2} + n_{3}(\\bar{y}_{3.}-\\bar{y}..)^{2}\n\\]\nSSE (Within group): Measures the variation of each observation around its group mean. \\[\nSSE = \\sum_{i=1}^{I}\\sum_{j=1}^{n_{i}}(y_{ij}-\\bar{y}_{i.})^{2} = \\sum_{i=1}^{I}(n_{i}-1)Var(Y_{i})\n\\]\nSST (Total): Measures the variation of the \\(N\\) data points around the overall mean. \\[\nSST =  \\sum_{i=1}^{I}\\sum_{j=1}^{n_{i}}(y_{ij}-\\bar{y}..)^{2} = (N-1)Var(Y)\n\\]\n\n\n5.4.2 Analysis of Variance Table*:\nThe results of an analysis of variance test are always summarized in an ANOVA table. The format of an ANOVA table is as follows:\n\n\n\n\n\n\n\n\n\n\nSource :========= Groups\nSS :======: SSG\ndf :=====: \\(I-1\\)\nMS :=========================: MSG = \\(\\frac{SSG}{I-1}\\)\nF | :=====:+ \\(\\frac{MSG}{MSE}\\)\n\n\n\n\nError\nSSE\n\\(N-I\\)\nMSE = \\(\\frac{MSG}{N-I}\\)\n\n\n\nTotal\nSST\n\\(N-1\\)\n  |\n\n\n\n\n\n5.4.3 The F-distribution\nThe \\(p\\)-value of the test is the area to the right of the F statistic density curve. This is always to the right because the F-distribution is not symmetric, truncated at 0 and skewed right. This is true regardless of the \\(df\\).\n\n\n\n\n\n\n\n\n\n\n\n5.4.4 Assumptions\nGenerally we must check three conditions on the data before performing ANOVA:\n\nThe observations are independent within and across groups\nThe data within each group are nearly normal\nThe variability across the groups is about equal.\n\nWhen these three conditions are met, we may perform an ANOVA to determine whether the data provide strong evidence against the null hypothesis that all the \\(\\mu_i\\) are equal.\n\n\n\n\n\n\nExample: A comparison of plant species under low water conditions\n\n\n\nThe PLANTS1 data file gives the percent of nitrogen in four different species of plants grown in a laboratory. The researchers collected these data in parts of the country where there is very little rainfall. To examine the effect of water, they varied the amount per day from 50mm to 650mm in 100mm increments. There were 9 plants per species-by-water combination. Because the plants are to be used primarily for animal food, with some parts that can be consumed by people, a high nitrogen content is very desirable. Let’s formally test to see if the nitrogen content in the plants differ across species.\n\n\n1. Identify response and explanatory variables.\n\nThe quantitative response variable is % nitrogen (pctnit)\nThe categorical explanatory variable is species (species)\n\n2. Visualize and summarize bivariate relationship.\n\nplot.nitrogen.species &lt;- plants1 %&gt;% select(species, pctnit) %&gt;% na.omit()\n\nggplot(plot.nitrogen.species, aes(x=species, y = pctnit, fill=species)) +\n      geom_boxplot(width=.3) + geom_violin(alpha=.4) +\n      labs(x=\"Species\") +\n      scale_fill_viridis_d(guide=FALSE) +\n      stat_summary(fun.y=\"mean\", geom=\"point\", size=3, pch=17,\n      position=position_dodge(width=0.75))\n\n\n\n\n\n\n\n\n\nplot.nitrogen.species %&gt;% group_by(species) %&gt;%\n summarise(mean=mean(pctnit, na.rm=TRUE),\n             sd = sd(pctnit, na.rm=TRUE),\n             IQR = IQR(pctnit, na.rm=TRUE)) %&gt;% kable()\n\n\n\n\nspecies\nmean\nsd\nIQR\n\n\n\n\n1\n3.039810\n0.2506118\n0.2690\n\n\n2\n2.092841\n0.2377523\n0.2725\n\n\n3\n3.284365\n0.3218599\n0.5065\n\n\n4\n1.195587\n0.2342217\n0.3125\n\n\n\n\n\nWhile the standard deviations are relatively similar across all species, the means are different (3.04 v. 2.09 v. 3.28 v. 1.20), with species 3 having the largest mean nitrogen content and species 4 the smallest. Species 3 has the highest IQR and species 1 has the lowest 0.506 v. 0.269).\n3. Write the relationship you want to examine in the form of a research question.\n\nNull Hypothesis: There is no difference in the average nitrogen content among plant species 1 through 4.\nAlternative Hypothesis: There is a difference in the average nitrogen content among plant species 1 through 4.\n\n4. Perform an appropriate statistical analysis.\nI. Let \\(\\mu_{1}\\), \\(\\ldots\\), \\(\\mu_{4}\\) be the mean nitrogen content in plant species 1 through 4 respectively.\n\n\\(H_{0}: \\mu_{1} = \\mu_{2} = \\mu_{3} = \\mu_{4}\\)\n\\(H_{A}:\\) At least one mean is different.\nWe are comparing means from multiple groups, so an ANOVA is the appropriate procedure. We need to check for independence, approximate normality and approximately equal variances across groups.\n\nIndependence: We are assuming that each plant was sampled independently of each other, and that the species themselves are independent of each other.\nNormality: With grouped data it’s easier to look at the histograms than qqplots.\n\nggplot(plants1, aes(x=pctnit, fill=species)) + ylab(\"\") + geom_density() + \n  facet_grid(species~.) +\n  theme(legend.position=\"bottom\") +\n  scale_y_continuous(breaks=NULL) + scale_fill_viridis_d()\n\n\n\n\n\n\n\n\nThe distributions per group tend to follow an approximate normal distribution.\nEqual variances: One way to assess if the groups have approximately equal variances is by comparing the IQR across groups.\n\nplants1 %&gt;% group_by(species) %&gt;% summarise(IQR = IQR(pctnit), SD = sd(pctnit)) %&gt;% kable()\n\n\n\n\nspecies\nIQR\nSD\n\n\n\n\n1\n0.2690\n0.2506118\n\n\n2\n0.2725\n0.2377523\n\n\n3\n0.5065\n0.3218599\n\n\n4\n0.3125\n0.2342217\n\n\n\n\n\nThe IQRs are similar so assumption of equal variances is not grossly violated. We can proceed with the ANOVA procedure.\n\nWe use the aov(response $\\sim$ predictor) function on the relationship between the nitrogen levels and tree species. We then pipe in summary() to make the output display nicely.\n\n\naov(pctnit~species, data=plants1) %&gt;% summary()\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nspecies       3 172.39   57.46   827.5 &lt;2e-16 ***\nResiduals   248  17.22    0.07                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n5. Write a conclusion in the context of the problem. The results of the ANOVA test indicate that at least one species has a different average nitrogen content than the other varieties (\\(p&lt;\\).001).\n\n\n5.4.5 Coefficient of determination \\(R^{2}\\)\nThe coefficient of determination is defined as \\(R^{2} = \\frac{SSG}{SST}\\) and can be interpreted as the % of the variation seen in the outcome that is due to subject level variation within each of the treatment groups. The strength of this measure can be thought of in a similar manner as the correlation coefficient \\(r\\): \\(&lt; .3\\) indicates a poor fit, \\(&lt; .5\\) indicates a medium fit, and \\(&gt; .7\\) indicates a good fit.\n\n172.39/(172.39+17.22)*100\n\n[1] 90.9182\n\n\nA large amount (91%) of the variation seen in nitrogen content in the plant can be explained by the species of plant.\n\n\n5.4.6 Multiple Comparisons\nSuppose that an ANOVA test reveals that there is a difference in at least one of the means. How can we determine which groups are significantly different without increasing our chance of a Type I error?\nSimple! We perform all the pairwise comparisons but using a test statistic that retains a family-wise error rate of 0.05 (or our chosen \\(\\alpha\\)). There are different methods to adjust for multiple comparisons, we will be using the Tukey HSD (honest significant difference) test. Continuing on with the analysis of nitrogen across plant species.\n\nTukeyHSD(aov(pctnit~species, data=plants1))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = pctnit ~ species, data = plants1)\n\n$species\n          diff        lwr        upr   p adj\n2-1 -0.9469683 -1.0684156 -0.8255209 0.0e+00\n3-1  0.2445556  0.1231082  0.3660029 2.4e-06\n4-1 -1.8442222 -1.9656696 -1.7227748 0.0e+00\n3-2  1.1915238  1.0700764  1.3129712 0.0e+00\n4-2 -0.8972540 -1.0187014 -0.7758066 0.0e+00\n4-3 -2.0887778 -2.2102252 -1.9673304 0.0e+00\n\n\nThe results from Tukey’s HSD for all pairwise comparisons indicate that the average nitrogen content in one species is significantly different from each of the three other species. The nice benefit of this procedure is that the difference between the means of the two groups are compared, and a 95confidence interval for each difference is included. So specifically, species 2 has on average 0.94 (0.82, 1.09) lower percent nitrogen compared to species 1 (\\(p&lt;.0001\\)). Also, species 3 has on average 1.19 (1.07, 1.31) higher percent nitrogen compared to species 2 (\\(p&lt;.0001\\)).",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bivariate Analysis</span>"
    ]
  },
  {
    "objectID": "bivariate_analysis.html#bv-chisq",
    "href": "bivariate_analysis.html#bv-chisq",
    "title": "5  Bivariate Analysis",
    "section": "5.5 (C~C) Multiple Proportions: \\(\\chi^{2}\\)",
    "text": "5.5 (C~C) Multiple Proportions: \\(\\chi^{2}\\)\nRecall that the point estimates for the proportion of an event occurring is \\(\\frac{x}{n}\\), where \\(x\\) is the number of times the event occurs out of \\(n\\) records. In this section we we would like to make conclusions about the difference in two population proportions: \\(p_1 - p_2\\). In other words we’re testing the hypothesis that \\(p_{1}-p_{2}=0\\).\nOur estimate for the difference in proportions based on the sample is \\(\\hat{p}_1 - \\hat{p}_2\\). No surprise there. What is slightly different is that we use a pooled proportion to check the condition of normality, and to calculate the standard error of the estimate. This pooled proportion is calculated by pooling the number of events in both groups, divided by the effective sample size for those groups.\n\\[ \\hat{p} = \\frac{x_{1} + x_{2}}{n_{1}+n_{2}} \\]\nThen the standard error of the point estimate is calculated as\n\\[ \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n_1} + \\frac{\\hat{p}(1-\\hat{p})}{n_2}} \\]\nSo the equations for the Confidence Interval for the difference in proportions is,\n\\[\n\\left( \\hat{p}_{1} - \\hat{p}_{2} \\right) \\pm t_{\\frac{\\alpha}{2}, df}\n\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n_1} + \\frac{\\hat{p}(1-\\hat{p})}{n_2}}\n\\]\nwith test statistic, \\[\nt^{*} =  \\frac{\\left( \\hat{p}_{1} - \\hat{p}_{2} \\right) - d_{0}}\n        {\\left( \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n_1} + \\frac{\\hat{p}(1-\\hat{p})}{n_2}}\n        \\right )}\n\\]\n\n5.5.1 Conditions for the sampling distribution to be normal.\nThe difference \\(\\hat{p}_1 - \\hat{p}_2\\) tends to follow a normal model when 1) each proportion separately follows a normal model, and 2) the two samples are independent of each other. #1 can be verified by checking the success-failure condition for each group.\nThat means:\n\n\\(\\hat{p}n_{1} \\geq 10\\), AND\n\\(\\hat{p}n_{2} \\geq 10\\), AND\n\\(\\hat{q}n_{1} \\geq 10\\), AND\n\\(\\hat{q}n_{1} \\geq 10\\).\n\nWhere, if I’ve forgotten to mention it yet, \\(q = 1-p\\).\nWhen sample sizes are below 10, but still not super small, say like 5, we turn to the non-parameteric version of this test called a Fisher’s Exact Test.\n\n\n5.5.2 Example: Are Mammograms effective?\nThis example comes from the OpenIntro Statistics textbook (3rd ed).\nA 30-year study was conducted with nearly 90,000 female participants. [(Miller AB. 2014)][mammo] During a 5-year screening period, each woman was randomized to one of two groups: in the first group, women received regular mammograms to screen for breast cancer, and in the second group, women received regular non-mammogram breast cancer exams. No intervention was made during the following 25 years of the study, and we’ll consider death resulting from breast cancer over the full 30-year period. Results from the study are summarized in the following table.\n\n\n\n\n\n\nImportant\n\n\n\nfootnote not working\nmammo Twenty five year follow-up for breast cancer incidence and mortality of the Canadian National Breast Screening Study: randomized screening trial. BMJ 2014;348:g366.\n\n\n\n\n\n\n\n\nAlive\nDead\nSum\n\n\n\n\nControl\n44405\n505\n44910\n\n\nMammogram\n44425\n500\n44925\n\n\nSum\n88830\n1005\n89835\n\n\n\n\n\n\n\n\nThe independent/explanatory variable is treatment (additional mammograms), and the dependent/response variable is death from breast cancer. Are these measures associated?\nIf mammograms are much more effective than non-mammogram breast cancer exams, then we would expect to see additional deaths from breast cancer in the control group (there is a relationship). On the other hand, if mammograms are not as effective as regular breast cancer exams, we would expect to see no difference in breast cancer deaths in the two groups (there is no relationship).\nWhat we need to do is to figure out how many deaths would be expected, if there was no relationship between treatment death by breast cancer, and then examine the residuals - the difference between the observed (\\(O_{ij}\\)) and expected (\\(E_{ij}\\)).\nIn our DATA = MODEL + RESIDUAL framework, the DATA is the observed counts \\(O_{ij}\\), and the MODEL is the expected counts \\(E_{ij}\\).\nTo see how the expected counts are calculated, we need to define a few more symbols, so we can find our way around the cells of a table. Just like rows and columns in a matrix, rows are indexed first (as \\(i\\) and columns indexed as \\(j\\)). So the cell in the top left is \\(i=1\\) and \\(j=1\\).\n\n\n\n\n\n\n\n\n\n\n\\(O_{ij}\\)\nAlive\nDead\nTotal\n\n\n\n\nMammo\n\\(n_{11}\\)\n\\(n_{12}\\) | \\(n_{1.}\\)\n\n\nControl\n\\(n_{21}\\)\n\\(n_{22}\\) | \\(n_{2.}\\)\n\n\nTotal\n\\(n_{.1}\\)\n\\(n_{.2}\\)\n\\(N\\)\n\n\n\nThe expected count for each cell is calculated as the row total times the column total for that cell, divided by the overall total. Yes this will end up as a fraction.\n\\[E_{ij} = \\frac{n_{i.}n_{.j}}{N}\\]\n\n\n\n\n\n\nAlive\nDead\n\n\n\n\nControl\n44407.58\n502.4161\n\n\nMammogram\n44422.42\n502.5839\n\n\n\n\n\n\n\nThe residuals are calculated as \\[ RESIDUALS = (O_{ij} - E_{ij})\\]\n\n\n\n\n\n\nAlive\nDead\n\n\n\n\nControl\n-0.0122616\n0.1152775\n\n\nMammogram\n0.0122596\n-0.1152583\n\n\n\n\n\n\n\nExamining the residuals can tell us which combinations had counts more or less observations than expected. If mammograms were not associated with survival, there were 0.01 fewer people still alive than expected, and 0.11 more people dead. This is trivially small (2 x 2) example with very large sample sizes. There will be another example provided later.\nThe \\(\\chi^2\\) test statistic is defined as the sum of the squared residuals, divided by the expected counts, and follows a \\(\\chi^2\\) distribution with degrees of freedom (#rows -1)(#cols -1).\n\\[ \\sum_{ij}\\frac{(O_{ij}-E_{ij})^{2}}{E_{ij}} \\]\nLike every other statistical test, large values of test statistics correspond to low p-values.\nBelow is a picture of the distribution for the current example. The p-value is reported on the left (vertically), the purple shaded area denotes the rejection region if we were using a hard cutoff of 0.05. (The rejection region is the area where the test statistic had to be at for a p-value to be smaller than .05.). For this example the test statistic was 0.017, which corresponds to a p-value of 0.895. Thus, this study does not provide enough evidence to support the claim that mammograms decrease the rate of deaths by breast cancer.\n\n\n\n\n\n\n\n\n\n\n\n5.5.3 Example: Smoking and General Health\nMore often than not, we will have the full data available. That is, data at each individual record not just a summary table like in the previous example. Let’s work through an example.\nUsing the Addhealth data set, what can we say about the relationship between smoking and a person’s perceived general level of general health?\n1. Identify response and explanatory variables.\n\nThe binary explanatory variable is whether the person has ever smoked an entire cigarette (eversmoke_c)\nThe categorical explanatory variable is the person’s general health (genhealth) and has levels “Excellent”, “Very Good”, “Good”, “Fair”, and “Poor”.\n\n2. Visualize and summarise bivariate relationship.\n\nsjPlot::plot_xtab(grp=addhealth$eversmoke_c, x=addhealth$genhealth, \n                  show.total = FALSE, margin=\"row\", legend.title=\"\") \n\n\n\n\n\n\n\n\nThe percentage of smokers seems to increase as the general health status decreases. Almost three-quarters (73%, n=40) of those reporting poor health have smoked an entire cigarette at least once in their life compared to 59% (n=573) of those reporting excellent health.\n3. Write the relationship you want to examine in the form of a research question.\nIs the proportion of those who have ever smoked equal across all levels of general health?\n\nNull Hypothesis: The proportion of smokers in each general health category is the same.\nAlternate Hypothesis: At least one proportion is different.\n\n4. Perform an appropriate statistical analysis.\nI. Define the parameters under consideration.\n\nLet \\(p_{1}\\) be the true proportion of smokers within the ``Excellent” health category.\nLet \\(p_{2}\\) be the true proportion of smokers within the ``Very good” health category.\nLet \\(p_{3}\\) be the true proportion of smokers within ``Good” health category.\nLet \\(p_{4}\\) be the true proportion of smokers within ``Fair” health category.\nLet \\(p_{5}\\) be the true proportion of smokers within ``Poor” health category.\n\n\n\\(H_{0}: p_{1} = p_{2} = p_{3} = p_{4} = p_{5}\\)\n\\(H_{A}:\\) At least one proportion is different.\nI will conduct a \\(\\chi\\)-squared test of association. There is at least 10 observations in each combination of smoking status and general health.\nConduct the test.\n\n\nhealth.smoke.model &lt;- chisq.test(addhealth$genhealth, addhealth$eversmoke_c)\nhealth.smoke.model\n\n\n    Pearson's Chi-squared test\n\ndata:  addhealth$genhealth and addhealth$eversmoke_c\nX-squared = 30.795, df = 4, p-value = 3.371e-06\n\n\nWe have strong evidence against the null; the \\(p\\)-value is less than .0001.\n5. Write a conclusion in context of the problem. We can conclude that there is an association between ever smoking a cigarette in their life and perceived general health (\\(\\chi^2\\) = 30.8, df=4, \\(p&lt;.0001\\)).\n\n\n5.5.4 Multiple Comparisons\nJust like with ANOVA, if we find that the chi-squared test indicates that at least one proportion is different from the others, it’s our job to figure out which ones might be different! We will analyze the residuals to accomplish this. Not by hand! Never again! You’re not learning how to code for nothing.\nThe residuals are automatically stored in the model output. You can either print them out and look at the values directly:\n\nhealth.smoke.model$residuals\n\n                   addhealth$eversmoke_c\naddhealth$genhealth Non Smoker     Smoker\n          Excellent  3.4461139 -2.5168277\n          Very good  0.4810893 -0.3513578\n          Good      -2.4431255  1.7843072\n          Fair      -1.0556365  0.7709714\n          Poor      -0.9446378  0.6899048\n\n\nOr you can extract them and save them as a data frame. Then use ggplot with geom_raster to fill in your squares.\n\nplot.residuals &lt;- health.smoke.model$residuals %&gt;% data.frame()\nggplot(plot.residuals, aes(x=addhealth.genhealth, y=addhealth.eversmoke_c)) +\n       geom_raster(aes(fill=Freq)) +  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nThe proportion of those who have never smoked and report being in Excellent health is higher than expected if these two measures were independent (high positive residual means observed is greater than expected). A lower percent of people reporting Good health never smoked, which is lower than expected if smoking and health status were independent. So these two categories are likely to be the groups that have a different proportion of lifetime smoker \\(p_{i}\\) compared to the other groups.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bivariate Analysis</span>"
    ]
  },
  {
    "objectID": "bivariate_analysis.html#bv-corr",
    "href": "bivariate_analysis.html#bv-corr",
    "title": "5  Bivariate Analysis",
    "section": "5.6 (Q~Q) Correlation",
    "text": "5.6 (Q~Q) Correlation\nThe correlation coefficient is designated by \\(r\\) for the sample correlation, and \\(\\rho\\) for the population correlation. The correlation is a measure of the strength and direction of a linear relationship between two variables.\nThe correlation ranges from +1 to -1. A correlation of +1 means that there is a perfect, positive linear relationship between the two variables. A correlation of -1 means there is a perfect, negative linear relationship between the two variables. In both cases, knowing the value of one variable, you can perfectly predict the value of the second.\n\n5.6.1 Strength of the correlation\nHere are rough estimates for interpreting the strengths of correlations based on the magnitude of \\(r\\).\n\n\\(|r| \\geq 0.7\\): Very strong relationship\n\\(0.4 \\leq |r| &lt; 0.7\\): Strong relationship\n\\(0.3 \\leq |r| &lt; 0.4\\): Moderate relationship\n\\(0.2 \\leq |r| &lt; 0.3:\\) Weak relationship\n\\(|r| &lt; 0.2:\\) Negligible or no relationship\n\n\n\n5.6.2 Example: Federal spending per capita and poverty rate\n\n#ggplot(county, aes(x=poverty, y=fed_spend00)) +\n#  geom_point() + ylab(\"federal spending per capita\") +\n#  xlab(\"poverty rate\")\n#cor(county$poverty, county$fed_spend00, use=\"complete.obs\")\n\n\nThere is a negligible, positive, linear relationship between poverty rate and per capita federal spending (\\(r = 0.03\\)).\nLet \\(\\rho\\) denote the true correlation between poverty rate and federal spending per capita.\nOur null hypothesis is that there is no correlation between poverty rate and federal spending (\\(\\rho = 0\\)), and the alternative hypothesis is that they are correlated (\\(\\rho \\neq 0\\)).\nWe can use the cor.test() function to analyze the evidence in favor of this alternative hypothesis.\n\n\n#cor.test(county$poverty, county$fed_spend00)\n\nWe conclude from this that there was a non-statistically significant, negligible correlation between poverty and federal spending (\\(r = 0.03 (-0.0003, .069), p = 0.05\\)).",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bivariate Analysis</span>"
    ]
  }
]