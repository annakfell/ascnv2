{
  "hash": "b173ff102e6fed7bea2a3722b0b53607",
  "result": {
    "engine": "knitr",
    "markdown": "# Factor Analysis {#sec-fa}\n\n::: callout-note\n#### Packages Used\n\nThis chapter uses the following packages:\n[corrplot](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html),\n[psych](https://personality-project.org/r/psych/),\n[ggfortify](https://cran.r-project.org/web/packages/ggfortify/vignettes/basics.html)\n:::\n\nObserved events tend to co-occur for a reason. Consider physical symptoms such as a fever, cough, sore through etc. These are often observed characteristics of an underlying event such as a viral infection. Now, not all individuals express the same symptoms, or event the same severity of symptoms, but we would say that there is a strong correlation between symptoms and underlying disease.  \n\nFactor analysis aims to understand the patterns of correlations between the underlying disease, and observed symptoms. \n\n## Introduction\n\n::: {.callout-tip appearance=simple}\n\nThis intro comes from [A gentle non-technical introduction to factor analysis](https://assessingpsyche.wordpress.com/2014/01/13/a-gentle-non-technical-introduction-to-factor-analysis/)\n:::\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\nNo attempt will be made to present a comprehensive treatment of this subject. For more detail see the references mentioned in PMA6 Chapter 15.2 and the links in the [Additional Resources](#help) section for more information. \n\n\n### Latent Constructs\n\nLatent variables are ones that cannot be measured directly; e.g. Depression, Anxiety, Mathematical ability. They drive how we would respond to various tasks and questions that _can_ be measured; vocabulary, arithmetic, statistical reasoning. \n\n\nFactor Analysis aims to\n\n* Generalize of principal components analysis\n* Explain interrelationships among a set of variables\n* Where we select a small number of factors to convey essential information\n* Can perform additional analyses to improve interpretation\n\n\n### Comparison with PCA\n* Similar in that no dependent variable\n* PCA: \n    - Select a number of components that explain as much of the total variance as possible. \n* FA: Factors selected mainly to explain the interrelationships among the original variables.\n    - Ideally, the number of factors expected is known in advance. \n    - Major emphasis is placed on obtaining easily understandable factors that convey the \n      essential information contained in the original set of variables.\n    \n\n![[Reference](https://www.researchgate.net/figure/Conceptual-distinction-between-factor-analysis-and-principal-component-analysis-ote-An_fig1_47386956)](images/fa_vs_pca_concept.png)\n\n\n* Mirror image of PCA\n    - Each PC is expressed as a linear combination of X's\n    - Each $X$ is expressed as a linear combination of Factors\n\n\n### EFA vs CFA\n\n**Exploratory Factor Analysis**\n\n* Explore the possible underlying factor structure of a set of observed variables\n* Does not impose a preconceived structure on the outcome. \n\n\n**Confirmatory Factor Analysis**\n\n* Verifies the theoretical factor structure of a set of observed variables\n* Test the relationship between observed variables and theoretical underlying latent constructs\n* Variable groupings are determined ahead of time. \n\n\n## Factor Model\n\n* Start with P **standardized** variables. That is $\\frac{(x_{i}-\\bar{x})}{s_{i}}$. \n    - So for the rest of these FA notes, understand that each $X$ written has already been standardized. \n* Express each variable as (its own) linear combination of $m$ common factors plus a unique factor $e$. \n\n$$\n\\begin{equation}\n\\begin{aligned}\nX_{1} = l_{11}F_{1} + l_{12}F_{2} + \\ldots + l_{1m}F_{m} + e_{1} \\\\\nX_{2} = l_{21}F_{1} + l_{22}F_{2} + \\ldots + l_{2m}F_{m} + e_{1} \\\\\n\\vdots \\\\\nX_{P} = l_{P1}F_{1} + l_{P2}F_{2} + \\ldots + l_{Pm}F_{m} + e_{P} \\\\\n\\end{aligned}\n\\end{equation}\n$$\n\n* $m$ is the number of common factors, typicall $m << P$. Somemtimes, $m$ is known in advance. \n* $X_{i}    = \\sum l_{ij} F_{j}+ \\epsilon_{i}$\n* $F_{j}$ \t= common or latent factors. \n    - They are uncorrelated and each having mean 0 and variance 1\n* $l_{ij}$ \t= coefficients of common factors \t= factor loadings\n* $e_{i}$ \t= unique factors relating to one of the original variables. \n    - $e_{i}$’s and $F_{j}$’s are uncorrelated\n\n\n\n\n### Components of Variance\n\nRecall that $x_{i}$ is standardized, so $Var(X)=1$. \n\nSince each response variable $x_{i}$ is broken into two parts, so is the variance. \n\n* **communality**: part due to common factors. Denoted as $h^{2}_{i}$.\n* **specificity**: part due to a unique factor. Denoted as $u^{2}_{i}$.\n\n    \n   \n$V(X_{i}) = h^{2}_{i} + u^{2}_{i}$\n\n\n::: {.callout-caution}\nIf the number $m$ of common factors is not known (EFA), it is recommended that you start with the default option available in the softare program. Often this is the number of factors with eigenvalues greater than 1. \n\nSince the results are highly dependent on $m$, you should always try several factors to gain further insight into the data. \n:::\n\n\n### Two big steps\n\n1. The first step is to numerical find estimates of the loadings $l_{ij}$, and the communalities $h^{2}_{i}$. \nThis process is called _initial factor extraction_. There are a number of methods to solve, we will explore three: principal components, iterated components, and maximum likelihood. The mathematical details of each are left in the textbook for interested readers. \n\n2. The second step is to obtain a new set of factors, called _rotated factors_ which is done to improve interpretation. \n\nWe will first explore these steps using simulated data. \n\n## Example data setup {#fa-example}\n\nGenerate 100 data points from the following multivariate normal distribution: \n\n$$\\mathbf{\\mu} = \n  \\left(\\begin{array}\n  {r}\n  0.163 \\\\\n  0.142 \\\\\n  0.098 \\\\\n  -0.039 \\\\\n  -0.013\n  \\end{array}\\right), \n  \\mathbf{\\Sigma} = \n  \\left(\\begin{array}\n  {cc}\n  1     &       &   &   &     &  \\\\\n  0.757 & 1     &   &   &     &  \\\\\n  0.047 & 0.054 & 1 &   &     &  \\\\\n  0.155 & 0.176 & 0.531 & 1   &  \\\\\n  0.279 & 0.322 & 0.521 & 0.942 & 1\n  \\end{array}\\right)\n$$. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(456)\nm <- c(0.163, 0.142, 0.098, -0.039, -0.013)\ns <- matrix(c(1.000, 0.757, 0.047, 0.155, 0.279, \n              0.757, 1.000, 0.054, 0.176, 0.322, \n              0.047, 0.054, 1.000, 0.531, 0.521, \n              0.155, 0.176, 0.531, 1.000, 0.942, \n              0.279, 0.322, 0.521, 0.942, 1.000), \n            nrow=5)\ndata <- data.frame(MASS::mvrnorm(n=100, mu=m, Sigma=s))\ncolnames(data) <- paste0(\"X\", 1:5)\n```\n:::\n\n\n\n\n\nStandardize the $X$'s. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstan.dta <- as.data.frame(scale(data))\n```\n:::\n\n\n\n\n\nThe hypothetical data model is that these 5 variables are generated from 2 underlying factors. \n\n$$\n\\begin{equation}\n\\begin{aligned}\nX_{1} &=  (1)*F_{1} +    (0)*F_{2} + e_{1} \\\\\nX_{2} &=  (1)*F_{1} +    (0)*F_{2} + e_{2} \\\\\nX_{3} &=  (0)*F_{1} +   (.5)*F_{2} + e_{3} \\\\\nX_{4} &=  (0)*F_{1} + (1.5)*F_{2} + e_{4} \\\\\nX_{5} &=  (0)*F_{1} +    (2)*F_{2} + e_{5} \\\\\n\\end{aligned}\n\\end{equation}\n$$\n\n**Implications**\n\n* $F_{1}, F_{2}$ and all $e_{i}$'s are independent normal variables\n* The first two $X$'s are inter-correlated, and the last 3 $X$'s are inter-correlated\n* The first 2 $X$'s are NOT correlated with the last 3 $X$'s\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(corrplot)\ncorrplot(cor(stan.dta), tl.col=\"black\")\n```\n\n::: {.cell-output-display}\n![](FA_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Factor Extraction Methods {#fa-extract}\n\nMethods\n\n1. Principal Components\n2. Iterated Components\n3. Maximum Likelihood\n\n\n### Principal components (PC Factor model)\n\nRecall that $\\mathbf{C} = \\mathbf{A}\\mathbf{X}$,  C's are a function of X\n\n$$ C_{1} = a_{11}X_{1} + a_{12}X_{2} + \\ldots + a_{1P}X_{p} $$\n\nWe want the reverse: X's are a function of F's. \n\n* Use the inverse! --> If $c = 5x$ then $x = 5^{-1}C$\n\nThe inverse PC model is $\\mathbf{X} = \\mathbf{A}^{-1}\\mathbf{C}$. \n\nSince $\\mathbf{A}$ is orthogonal, $\\mathbf{A}^{-1} = \\mathbf{A}^{T} = \\mathbf{A}^{'}$, so\n\n$$ X_{1} = a_{11}C_{1} + a_{21}C_{2} + \\ldots + a_{P1}C_{p} $$\n\nBut there are more PC's than Factors...\n\n$$\n\\begin{equation}\n\\begin{aligned}\nX_{i} &=  \\sum_{j=1}^{P}a_{ji}C_{j} \\\\\n&= \\sum_{j=1}^{m}a_{ji}C_{j} + \\sum_{j=m+1}^{m}a_{ji}C_{j} \\\\\n&= \\sum_{j=1}^{m}l_{ji}F_{j} + e_{i} \\\\\n\\end{aligned}\n\\end{equation}\n$$\n\n**Adjustment**\n\n* $V(C_{j}) = \\lambda_{j}$ not 1\n* We transform: $F_{j} = C_{j}\\lambda_{j}^{-1/2}$\n* Now $V(F_{j}) = 1$\n* Loadings: $l_{ij} = \\lambda_{j}^{1/2}a_{ji}$\n\n::: {.cell type='rmdnote'}\n\\BeginKnitrBlock{rmdnote}<div class=\"rmdnote\">$l_{ij}$ is the correlation coefficient between variable $i$ and factor $j$</div>\\EndKnitrBlock{rmdnote}\n:::\n\nThis is similar to $a_{ij}$ in PCA. \n\n\n**R code**\n\n\nFactor extraction via principal components can be done using the `principal` function in the `psych` package. We choose `nfactors=2` here because we know there are 2 underlying factors in the data generation model. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(psych)\npc.extract.norotate <- principal(stan.dta, nfactors=2, rotate=\"none\")\nprint(pc.extract.norotate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPrincipal Components Analysis\nCall: principal(r = stan.dta, nfactors = 2, rotate = \"none\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n    PC1   PC2   h2    u2 com\nX1 0.57  0.75 0.89 0.112 1.9\nX2 0.61  0.72 0.89 0.113 1.9\nX3 0.58 -0.51 0.59 0.406 2.0\nX4 0.87 -0.38 0.89 0.109 1.4\nX5 0.92 -0.27 0.91 0.086 1.2\n\n                       PC1  PC2\nSS loadings           2.63 1.55\nProportion Var        0.53 0.31\nCumulative Var        0.53 0.83\nProportion Explained  0.63 0.37\nCumulative Proportion 0.63 1.00\n\nMean item complexity =  1.7\nTest of the hypothesis that 2 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.09 \n with the empirical chi square  16.62  with prob <  4.6e-05 \n\nFit based upon off diagonal values = 0.96\n```\n\n\n:::\n:::\n\n\n\n\n\n$$\n\\begin{equation}\n\\begin{aligned}\nX_{1} &=  0.53F_{1} + 0.78F_{2} + e_{1} \\\\\nX_{2} &=  0.59F_{1} + 0.74F_{2} + e_{2} \\\\\nX_{3} &=  0.70F_{1} - 0.39F_{2} + e_{3} \\\\\nX_{4} &=  0.87F_{1} - 0.38F_{2} + e_{4} \\\\\nX_{5} &=  0.92F_{1} - 0.27F_{2} + e_{5} \\\\\n\\end{aligned}\n\\end{equation}\n$$\n\nThese equations come from the top of the output, under _Standardized loadings_. \n\n### Iterated components\n\nSelect common factors to maximize the total communality\n\n1. Get initial communality estimates\n2. Use these (instead of original variances) to get the PC's and factor loadings\n3. Get new communality estimates\n4. Rinse and repeat\n5. Stop when no appreciable changes occur. \n\nR code not shown, but can be obtained using the [`factanal`](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/factanal) package in R. \n\n\n### Maximum Likelihood\n\n* Assume that all the variables are normally distributed\n* Use Maximum Likelihood to estimate the parameters\n\n\n**R code**\n\nThe `cutoff` argument hides loadings under that value for ease of interpretation. Here I am setting that cutoff at 0 so that all loadings are being displayed. I encourage you to adjust this cutoff value in practice to see how it can be useful in reducing cognitave load of looking through a grid of numbers. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nml.extract.norotate <- factanal(stan.dta, factors=2, rotation=\"none\")\nprint(ml.extract.norotate, digits=2, cutoff=0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nfactanal(x = stan.dta, factors = 2, rotation = \"none\")\n\nUniquenesses:\n  X1   X2   X3   X4   X5 \n0.33 0.06 0.72 0.08 0.01 \n\nLoadings:\n   Factor1 Factor2\nX1  0.35    0.74  \nX2  0.41    0.88  \nX3  0.50   -0.18  \nX4  0.94   -0.19  \nX5  0.99   -0.07  \n\n               Factor1 Factor2\nSS loadings       2.41    1.39\nProportion Var    0.48    0.28\nCumulative Var    0.48    0.76\n\nTest of the hypothesis that 2 factors are sufficient.\nThe chi square statistic is 0.4 on 1 degree of freedom.\nThe p-value is 0.526 \n```\n\n\n:::\n:::\n\n\n\n\n\nThe factor equations now are: \n\n$$\n\\begin{equation}\n\\begin{aligned}\nX_{1} &=  -0.06F_{1} + 0.79F_{2} + e_{1} \\\\\nX_{2} &=  -0.07F_{1} + 1F_{2} + e_{2} \\\\\nX_{3} &=  0.58F_{1} + 0.19F_{2} + e_{3} \\\\\n\\vdots\n\\end{aligned}\n\\end{equation}\n$$\n\n\n### Uniqueness\n\nRecall Factor analysis splits the variance of the observed X's into a part due to the communality $h_{i}^{2}$ and specificity $u_{i}^{2}$. This last term is the portion of the variance that is due to the _unique_ factor. Let's look at how those differ depending on the extraction method: \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npc.extract.norotate$uniquenesses\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        X1         X2         X3         X4         X5 \n0.11151283 0.11336123 0.40564130 0.10890203 0.08591098 \n```\n\n\n:::\n\n```{.r .cell-code}\nml.extract.norotate$uniquenesses\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        X1         X2         X3         X4         X5 \n0.33432315 0.05506386 0.71685548 0.07508356 0.01414656 \n```\n\n\n:::\n:::\n\n\n\n\n\nHere we see that the uniqueness for X2, X4 and X5 under ML is pretty low compared to the PC extraction method, but that's almost offset by a much higher uniqueness for x1 and X3. \n\nIdeally we want the variance in the X's to be captured by the factors. So we want to see a low unique variance. \n\n\n### Resulting factors\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,2)) # grid of 2 columns and 1 row\n\npc.load <- pc.extract.norotate$loadings[,1:2]\nplot(pc.load, type=\"n\", main=\"PCA Extraction\") # set up the plot but don't put points down\ntext(pc.load, labels=rownames(pc.load)) # put names instead of points\n\nml.load <- ml.extract.norotate$loadings[,1:2]\nplot(ml.load, type=\"n\", main=\"ML Extraction\") \ntext(ml.load, labels=rownames(ml.load))\n```\n\n::: {.cell-output-display}\n![](FA_files/figure-html/unnamed-chunk-9-1.png){width=1152}\n:::\n:::\n\n\n\n\n\nPCA Extraction\n\n* X1 and X2 load high on PC1, and low on PC1. \n* X3, 4 and 5 are negative on PC2, and moderate to high on PC1. \n* PC1 is not highly correlated with X3\n\n\nML Extraction\n\n* Same overall split, X3 still not loading high on Factor 1. \n* X1 loading lower on Factor 2 compared to PCA extraction method. \n\n::: {.callout-caution appearance=simple}\nNeither extraction method reproduced our true hypothetical factor model. Rotating the factors will achieve our desired results. \n:::\n\n## Rotating Factors\n\n* Find new factors that are easier to interpret\n* For each $X$, we want some high/large (near 1) loadings and some low/small (near zero)\n* Two common rotation methods: Varimax rotation, and oblique rotation. \n\nSame(ish) goal as PCA, find a new set of axes to represent the factors. \n\n\n### Varimax Rotation\n\n* Restricts the new axes to be orthogonal to each other. (Factors are independent)\n* Maximizes the sum of the variances of the squared factor loadings within each factor $\\sum Var(l_{ij}^{2}|F_{j})$\n* Interpretations slightly less clear\n\nVarimax rotation with principal components extraction.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npc.extract.varimax <- principal(stan.dta, nfactors=2, rotate=\"varimax\")\nprint(pc.extract.varimax)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPrincipal Components Analysis\nCall: principal(r = stan.dta, nfactors = 2, rotate = \"varimax\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n    RC1   RC2   h2    u2 com\nX1 0.06  0.94 0.89 0.112 1.0\nX2 0.11  0.93 0.89 0.113 1.0\nX3 0.76 -0.10 0.59 0.406 1.0\nX4 0.93  0.16 0.89 0.109 1.1\nX5 0.91  0.28 0.91 0.086 1.2\n\n                       RC1  RC2\nSS loadings           2.30 1.88\nProportion Var        0.46 0.38\nCumulative Var        0.46 0.83\nProportion Explained  0.55 0.45\nCumulative Proportion 0.55 1.00\n\nMean item complexity =  1.1\nTest of the hypothesis that 2 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.09 \n with the empirical chi square  16.62  with prob <  4.6e-05 \n\nFit based upon off diagonal values = 0.96\n```\n\n\n:::\n:::\n\n\n\n\n\nVarimax rotation with maximum likelihood extraction. Here I'm using the cutoff argument to only show the values of loadings over 0.3. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nml.extract.varimax <- factanal(stan.dta, factors=2, rotation=\"varimax\")\nprint(ml.extract.varimax, digits=2, cutoff=.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nfactanal(x = stan.dta, factors = 2, rotation = \"varimax\")\n\nUniquenesses:\n  X1   X2   X3   X4   X5 \n0.33 0.06 0.72 0.08 0.01 \n\nLoadings:\n   Factor1 Factor2\nX1          0.81  \nX2          0.97  \nX3  0.53          \nX4  0.95          \nX5  0.96          \n\n               Factor1 Factor2\nSS loadings       2.13    1.67\nProportion Var    0.43    0.33\nCumulative Var    0.43    0.76\n\nTest of the hypothesis that 2 factors are sufficient.\nThe chi square statistic is 0.4 on 1 degree of freedom.\nThe p-value is 0.526 \n```\n\n\n:::\n:::\n\n\n\n\n\n\nCommunalities are unchanged after varimax (part of variance due to common factors). This will always be the case for orthogonal (perpendicular) rotations. \n\n\n### Oblique rotation\n\n* Same idea as varimax, but drop the orthogonality requirement\n* Less restrictions allow for greater flexibility\n* Factors are still correlated\n* Better interpretation\n* Methods: \n    - _quartimax_ or _quartimin_ minimizes the number of factors needed to explain each variable\n    - _direct oblimin_ standard method, but results in diminished interpretability of factors\n    - _promax_ is computationally faster than _direct oblimin_, so good for very large datasets\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npc.extract.quartimin <- principal(stan.dta, nfactors=2, rotate=\"quartimin\")\nml.extract.promax<- factanal(stan.dta, factors=2, rotation=\"promax\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,3))\nplot(pc.extract.norotate, title=\"PC + norotate\")\nplot(pc.extract.varimax, title=\"PC + Varimax\")\nplot(pc.extract.quartimin, title=\"PC + quartimin\")\n\n\nload <- ml.extract.norotate$loadings[,1:2]\nplot(load, type=\"n\", main=\"ML + norotate\")\ntext(load, labels=rownames(load))\n\nload <- ml.extract.varimax$loadings[,1:2]\nplot(load, type=\"n\", main=\"ML + Varimax\") \ntext(load, labels=rownames(load)) \n\nload <- ml.extract.promax$loadings[,1:2]\nplot(load, type=\"n\", main= \"ML + Promax\") \ntext(load, labels=rownames(load)) \n```\n\n::: {.cell-output-display}\n![](FA_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n\n\nVarimax vs oblique here doesn't make much of a difference, and typically this is the case. You almost always use some sort of rotation. Recall, this is a hypothetical example and we set up the variables in a distinct two-factor model. So this example will look nice. \n\n\n## Factor Scores\n\nWe could obtain Factor scores for an individual based on only the X's that highly load on that factor. Essentially here, FA would identify subgroups of correlated variables. \n\nIn our hypothetical example where after rotation x1 and x2 loaded highly on factor 2, and x3-5 loaded highly on factor 1, we could calculate factor scores as\n\n* factor score 2 for person $i$ = $x_{i1} + x_{i2}$\n* factor score 3 for person $i$ = $x_{i3} + x_{i4} + x_{i5}$\n\nIn some simple applications, this approach may be sufficient. \n\nMore commonly, we will use a **regression procedure** to compute factor scores. This method accounts for the correlation between the $x_{i}$'s and uses the factor loadings $l_{ij}$ to calculate the factor scores. \n\n* Can be used as dependent or independent variables in other analyses\n* Can be generated by adding the `scores=\"regression\"` option to `factanal()`, or `scores=TRUE` in `principal()`\n* Each record in the data set _with no missing data_ will have a corresponding factor score.\n    - `principal()` also has a `missing` argument that if set to `TRUE` it will \n    impute missing values. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfa.ml.varimax <- factanal(stan.dta, factors=2, rotation=\"varimax\", scores=\"regression\")\nsummary(fa.ml.varimax$scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Factor1            Factor2         \n Min.   :-2.47094   Min.   :-2.335593  \n 1st Qu.:-0.70659   1st Qu.:-0.737829  \n Median : 0.08397   Median :-0.002978  \n Mean   : 0.00000   Mean   : 0.000000  \n 3rd Qu.: 0.67114   3rd Qu.: 0.792273  \n Max.   : 2.13449   Max.   : 1.670956  \n```\n\n\n:::\n\n```{.r .cell-code}\nhead(fa.ml.varimax$scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Factor1     Factor2\n[1,]  0.9713019  1.29838695\n[2,] -1.1676730  0.57888326\n[3,] -0.6068270 -0.09329792\n[4,]  1.2569753  0.31231783\n[5,]  1.3817494 -0.77707241\n[6,]  0.2311359  1.11142513\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(ggforitfy)\nautoplot(fa.ml.varimax) # see vignette for more info. Link at bottom\n```\n\n::: {.cell-output-display}\n![](FA_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n\nTo merge these scores back onto the original data set **providing there is no missing data** you can use the `bind_cols()` function in `dplyr`. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.withscores <- bind_cols(data, data.frame(fa.ml.varimax$scores))\nkable(head(data.withscores))\n```\n\n::: {.cell-output-display}\n\n\n|         X1|         X2|         X3|         X4|         X5|    Factor1|    Factor2|\n|----------:|----------:|----------:|----------:|----------:|----------:|----------:|\n|  0.5989843|  1.3729499|  1.0871992|  0.8597854|  1.2485892|  0.9713019|  1.2983869|\n|  0.7429206|  0.3819301| -0.0113714| -1.1316383| -1.1316216| -1.1676730|  0.5788833|\n| -0.5699028| -0.1331253| -0.3594030| -0.7153705| -0.7274903| -0.6068270| -0.0932979|\n|  1.6526585|  0.2216533|  0.6564431|  1.4564378|  1.1959989|  1.2569753|  0.3123178|\n| -0.8582815| -0.6310620|  1.2474978|  1.2291103|  1.0684566|  1.3817494| -0.7770724|\n|  1.1682966|  0.9849325| -0.8860830| -0.0713417|  0.5105760|  0.2311359|  1.1114251|\n\n\n:::\n:::\n\n\n\n\n\n\n## What to watch out for\n\n* Number of factors should be chosen with care. Check default options.\n* There should be at least two variables with non-zero weights per factor\n* If the factors are to be correlated, try oblique factor analysis\n* Results usually are evaluated by reasonableness to investigator rather than by formal tests\n* Motivate theory, not replace it. \n* Missing data - factors will only be created using available data. \n\n\n\n## Additional Resources{#help}\n\n* [A gentle non-technical introduction to factor analysis](https://assessingpsyche.wordpress.com/2014/01/13/a-gentle-non-technical-introduction-to-factor-analysis/)\n* [Tutorial](https://web.stanford.edu/class/psych253/tutorials/FactorAnalysis.html ) by a Psych 253 student at Stanford \n* `ggfortify` [vignette](https://cran.r-project.org/web/packages/ggfortify/vignettes/plot_pca.html ) for the `autoplot()` function \n\nThe [`FactomineR`](http://factominer.free.fr/ ) package looks promising, it has some helpful graphics for determining/confirming variable groupings and aiding interpretations. \n\n* STHDA [tutorial](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/116-mfa-multiple-factor-analysis-in-r-essentials/ ) using FactomineR \n\n",
    "supporting": [
      "FA_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}