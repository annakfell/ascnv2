{
  "hash": "df8a0192c1dd292f312fb4acfe24ecd5",
  "result": {
    "markdown": "# Bivariate Analysis {#sec-bivariate-analysis}\n\nSo far we have been concerned with making inference about a single population parameter. \nMany problems deal with comparing a parameter across two or more groups. Research \nquestions include questions like: \n\n* Does the average life span differ across subspecies of a particular turtle?\n* Who has a higher percentage of the female vote - Democrats or Republicans?\n\n\n\nA good way to think about all statistical models is that the observed data comes from some true model with some random error.\n\n> DATA = MODEL + RESIDUAL\n\nThe `MODEL` is a mathematical formula (like $y = f(x)$). \nThe formulation of the `MODEL` will change depending on the number of, and data types of explanatory variables. One goal of inferential analysis is to explain the variation in our data, using information contained in other measures. \n\n\n## Assumption of Independent Observations\n\nThe primary assumption of most standard statistical procedures is that observations are\nindependent of each other. That is, the value of one observation does not change or affect another observation. \nHowever, there are many examples where measurements are made \non subjects before and after a certain exposure or treatment (pre-post), or an \nexperiment to compare two cell phone packages might use pairs of subjects that \nare the same age, sex and income level. One subject would be randomly assigned \nto the first phone package, the other in the pair would get the second phone package. \nThis chapter only deals with non-correlated analyses, leaving that topic for a later chapter. \n\n## Choosing appropriate bivariate analysis\n\nChoosing which statistical analyses procedure is appropriate completely depending on the data types of the explanatory and response variable. This is a simplified table, only covering the common/standard types of bivariate analysis. \n\n\n> figure out how to get table here. \n\nFor this set of notes, the variable types are referred to using the first letter, \ne.g. *Q* for quantitative, *B* for binary, and *C* for categorical. \nThus a T-test is a (Q $\\sim$ B) analysis, and a correlation analysis is (Q $\\sim$ Q) analysis. \n\n\nLinks to the example data used in this chapter. \n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"_common.R\")\nlibrary(knitr)\nlibrary(kableExtra)\nload(here::here('data', 'addhealth_clean.Rdata'))\ncounty <- read.csv(\"https://www.norcalbiostat.com/data/countyComplete.csv\", header=TRUE, stringsAsFactors = FALSE)\n```\n:::\n\n\n::: {.callout-warning appearance=simple}\n> county dataset will need to be replaced or example switched - no longer included in openintro & the csv on https://www.norcalbiostat.com/data/#countyComplete is missing \nfederal spending column\n\n:::\n\n----\n\n## (Q~B) Two means: T-Test {#bv-ttest}\n\n\nIt is common to compare means from different samples. For instance, we\nmight investigate the effectiveness of a certain educational intervention\nby looking for evidence of greater reading ability in the treatment group\nagainst a control group. That is, our research hypothesis is that reading\nability of a child is associated with an educational intervention.\n\nThe null hypothesis states that there is no relationship, or no effect, of\nthe educational intervention (binary explanatory variable) on the reading ability\nof the child (quantitative response variable). This can be written in symbols as follows:\n\n\n$$H_{0}: \\mu_{1} = \\mu_{2}\\mbox{ or }\\qquad  H_{0}: \\mu_{1} -\\mu_{2}=0$$\n\nwhere $\\mu_{1}$ is the average reading score for students in the control group (no intervention)\nand $\\mu_{2}$ be the average reading score for students in the intervention group. Notice it can be written as one mean equals the other, but also as the difference between two means equaling zero. \nThe alternative hypothesis $H_{A}$ states that there is a relationship:\n\n$$H_{A}: \\mu_{1} \\neq \\mu_{2} \\qquad \\mbox{ or } \\qquad H_{A}: \\mu_{1}-\\mu_{2} \\neq 0$$\n\n\n### Assumptions\n* The data distribution for each group is approximately normal.\n* The scores are independent within each group.\n* The scores from the two groups are independent of each other (i.e. the two samples are independent).\n\n\n\n### Sampling Distribution for the difference\n\nWe use $\\bar{x}_1 - \\bar{x}_2$ as a point estimate for $\\mu_1 - \\mu_2$, which has a standard error of\n\n$$\n SE_{\\bar{x}_1 - \\bar{x}_2}\n   = \\sqrt{SE_{\\bar{x}_1}^2 + SE_{\\bar{x}_2}^2}\n \t = \\sqrt{\\frac{\\sigma^{2}_{1}}{n_1} + \\frac{\\sigma^{2}_{2}}{n_2}}\n$$\n\nSo the equations for a Confidence Interval is, \n\n$$\n  \\left( \\bar{x}_{1} - \\bar{x}_{2} \\right) \\pm t_{\\frac{\\alpha}{2}, df}\n    \\sqrt{ \\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}} }\n$$ \n\nand Test Statistic is, \n\n$$\n  t^{*} =  \\frac{\\left( \\bar{x}_{1} - \\bar{x}_{2} \\right) - d_{0}}\n       {\\left( \\sqrt{ \\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}} }\n       \\right )} \n$$\n\n\nTypically it is unlikely that the population variances $\\sigma^{2}_{1}$ and $\\sigma^{2}_{2}$ are known so we will use sample variances $s^{2}_{1}$ and $s^{2}_{2}$ as estimates. \n\nWhile you may never hand calculate these equations, it is important to see the format, or structure, of the equations. Equation \\ref{2sampCImean} has the same format of \n\n$$ \\mbox{point estimate} \\pm 2*\\mbox{standard error}$$\n\nregardless what it is we're actually trying to estimate. Thus in a pinch, you can calculate approximate confidence intervals for whatever estimate you are trying to understand, given only the estimate and standard error, even if the computer program does not give it to you easily or directly. \n\n\n\n### Example: Smoking and BMI\n\nWe would like to know, is there convincing evidence that the average BMI differs between those who have ever smoked a cigarette in their life compared to those who have never smoked? This example uses the Addhealth dataset.\n\n\n**1. Identify response and explanatory variables.**\n\n* The quantitative response variable is BMI (variable \\R{BMI})\n* The binary explanatory variable is whether the person has ever smoked a cigarette (variable \\R{eversmoke\\_c})\n\n**2. Visualize and summarize bivariate relationship.**\n\n::: {.callout-warning appearance=simple}\n\nUsing `na.omit()` is dangerous! This will remove ALL rows with ANY missing data in it. Regardless if the missing values are contained in the variables you are interested in. \nThe example below employs a trick/work around to not have NA values show in the output.\nWe take the data set `addhealth` _and then_ `select` the variables we want to plot, _and then_ we use `na.omit()` to delete all rows with missing data. Then that is saved as a new, temporary data frame specifically named for this case (`plot.bmi.smoke`). \n:::\n\n> note for later. Move this explanation into data viz section. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot.bmi.smoke <- addhealth %>% select(eversmoke_c, BMI) %>% na.omit()\n\nggplot(plot.bmi.smoke, aes(x=eversmoke_c, y=BMI, fill=eversmoke_c)) +\n      geom_boxplot(width=.3) + geom_violin(alpha=.4) +\n      labs(x=\"Smoking status\") +\n      scale_fill_viridis_d(guide=FALSE) +\n      stat_summary(fun.y=\"mean\", geom=\"point\", size=3, pch=17,\n      position=position_dodge(width=0.75))\n```\n\n::: {.cell-output-display}\n![](bivariate_analysis_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot.bmi.smoke %>% group_by(eversmoke_c) %>%\n summarise(mean=mean(BMI, na.rm=TRUE),\n             sd = sd(BMI, na.rm=TRUE),\n             IQR = IQR(BMI, na.rm=TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 4\n  eversmoke_c  mean    sd   IQR\n  <fct>       <dbl> <dbl> <dbl>\n1 Non Smoker   29.7  7.76  9.98\n2 Smoker       28.8  7.32  9.02\n```\n:::\n:::\n\n\nSmokers have an average BMI of 28.8, smaller than the average BMI of non-smokers at 29.7. Nonsmokers have more variation in their BMIs (sd 7.8 v. 7.3 and IQR 9.98 v. 9.02), but the distributions both look normal, if slightly skewed right.\n\n\n**3. Write the relationship you want to examine in the form of a research question.**\n\n* Null Hypothesis: There is no relationship between BMI and smoking status.\n* Alternate Hypothesis: There is a relationship between BMI and smoking status.\n\n\n**4. Perform an appropriate statistical analysis.**\n\nI. Let $\\mu_1$ denote the average BMI for nonsmokers, and $\\mu_2$ the average BMI for smokers.\n\nII.\n  $\\mu_1 - \\mu_2 = 0$ There is no difference in the average BMI between smokers and nonsmokers.\n  $\\mu_1 - \\mu_2 \\neq 0$ There is a difference in the average BMI between smokers and nonsmokers. \n\nIII. We are comparing the means between two independent samples. A Two-Sample T-Test for a difference in means will be conducted. The assumptions that the groups are independent is upheld because each individual can only be either a smoker or nonsmoker. The difference in sample means $\\bar{x_1} - \\bar{x_2}$ is normally distributed -- this is a valid assumption due to the large sample size and that differences typically are normally distributed. The observations are independent, and the variability is roughly equal (IQR 9.9 v. 9.0).\n\nIV. We use the `t.test` function, but use model notation of the format `outcome` $\\sim$ `category`. Here, `BMI` is our continuous outcome that we're testing across the (binary) categorical predictor `eversmoke_c`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(BMI ~ eversmoke_c, data=addhealth)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  BMI by eversmoke_c\nt = 3.6937, df = 3395.3, p-value = 0.0002245\nalternative hypothesis: true difference in means between group Non Smoker and group Smoker is not equal to 0\n95 percent confidence interval:\n 0.3906204 1.2744780\nsample estimates:\nmean in group Non Smoker     mean in group Smoker \n                29.67977                 28.84722 \n```\n:::\n:::\n\n\nWe have very strong evidence against the null hypothesis, $p = 0.0002$.\n\n**5. Write a conclusion in the context of the problem.**\n\nOn average, nonsmokers have a significantly higher BMI by 0.83 (0.39, 1.27) compared to nonsmokers ($p = 0.0002$).\n\n::: {.callout-warning appearance=simple}\n\nAlways check the output against the direction you are testing. R always will calculate a difference as group 1 - group 2, and it defines the groups alphabetically. For example, for a factor variable that has groups A and B, R will automatically calculate the difference as A-B. In this example it is Nonsmoker - Smoker.\n\n:::\n\n\n----\n\n## (Q~C) Multiple means: ANOVA {#bv-anova}\n\nFrequently, a researcher wants to compare the means of an outcome across three or more treatments in a single experiment. We might initially think to do pairwise comparisons (1v2, 1v3, 2v3) for a total of three comparisons. However, this strategy can be treacherous. If we have many groups and do many comparisons, it is likely that we will eventually find a difference just by chance, even if there is no difference in the populations.\n\nWhen we analyze a conventional two-treatment experiment, we are prepared to run a 1 in 20 risk of an apparently significant result arising purely by accident (the 5% chance of a Type I error). We regard such a risk as being fairly unlikely and feel justified in accepting with confidence any significant results we obtain.\n\nAnalyzing a single experiment as a series of 10 treatment pairs is a very different proposition. The chance of an apparently significant result arising purely by chance somewhere in the 10 analyses increases dramatically. Using a 5% error rate, the chance of NOT making at Type I error is .95. To not make a Type I error 10 times is $.95^{10} = .6$. That means there is a 40% of making a Type I error! \\emph{See: \\url{https://xkcd.com/882/}.}\n\n#### Example: Visual Comparison\nExamine the figure below. Compare groups I, II, and III. Can you visually determine if the differences in the group centers is due to chance or not? What about groups IV, V, and VI?\n\n![Side-by-side dot plot for the outcomes for six groups.](images/toyANOVA.png)\n\nSo we need some method of comparing treatments for more than two groups at a time. \nThis is done using an Analysis of Variance (ANOVA) model. \n### Terminology\n\n* **Response Variable**: The response variable in the ANOVA setting is the quantitative (continuous) variable that we want to compare among the different treatments.\n* **Factor/Treatment**: A property or characteristic (categorical variable) that allows us to distinguish the different populations from one another. An independent variable to be studied in an investigation such as temperature, type of plant, color of flower, location.\n* **Factor/Treatment level**: Factors have different levels, such as 3 temperatures, 5 locations, 3 colors, etc.\n* **Within-sample Variation**: Variation within a sample from one population. Individuals who receive the same treatment will experience identical experimental conditions. The variation within each of the treatment groups must therefore be a consequence of solely random variation. \n* **Between-sample Variation**: Variation between samples. This is the difference between the group means. If some treatments are genuinely more effective than others, then we would expect to see relatively large differences between the treatment means and a relatively large between-treatments variation. \n\n\n### Formulation of the One-way ANOVA model\n\nANOVA is a mathematical technique which uses a model based approach to partition the variance in an experiment into different sources of variance. This technique enables us to test if most the variation in the treatment means is due to differences between the groups.\n\nStarting with our generic conceptual understanding of statistical models: \n\n> DATA = MODEL + RESIDUAL\n\nour MODEL for this situation is the group membership. Does knowing what group an observation is in tell you about the location of the data? The one-way (or one-factor) ANOVA model is\n\n$$\ny_{ij} = \\mu_{i} + \\epsilon_{ij} \\qquad \\qquad\n\\epsilon_{ij} \\overset{iid}{\\sim} \\mathcal{N}(0,\\sigma^{2})\n$$\n\nfor $i=1, \\ldots, I$ factor levels and $j = 1, \\ldots, n_{i}$ subjects within each factor level. The random error terms are independently and identically distributed (iid) normally with common variance. \n\nThe null and alternative hypotheses are always phrased as follows: \n\n* $H_0$: The mean outcome is the same across all groups. $\\mu_1 = \\mu_2 = \\cdots = \\mu_k$\n* $H_A$: At least one mean is different.\n\nHow do we compare means using an **AN**alysis **O**f **VA**riance? By comparing the portion of the variance in the outcome that is explained by the groups, to the portion that's leftover due to  unexplained randomness. Essentially we're comparing the ratio of `MODEL` to `RESIDUAL`. \n\nThe total variation of the observed data is broken down into 2 parts:\n\n> Total Variation = Between Group Variation + Within Group Variation\n\nVariation is measured using the Sum of Squares (SS): The sum of the squares within a group (SSE), the sum of squares between groups (SSG), and the total sum of squares (SST).\n\n\n**SSG (Between groups)**: Measures the variation of the $I$ group means around the overall mean.\n$$\n  SSG = \\sum_{i=1}^{I}n_{i}(\\bar{y}_{i.}-\\bar{y}..)^{2} = n_{1}(\\bar{y}_{1.}-\\bar{y}..)^{2} + n_{2}(\\bar{y}_{2.}-\\bar{y}..)^{2} + n_{3}(\\bar{y}_{3.}-\\bar{y}..)^{2}\n$$\n\n**SSE (Within group)**: Measures the variation of each observation around its group mean.\n$$\nSSE = \\sum_{i=1}^{I}\\sum_{j=1}^{n_{i}}(y_{ij}-\\bar{y}_{i.})^{2} = \\sum_{i=1}^{I}(n_{i}-1)Var(Y_{i})\n$$\n\n**SST (Total)**: Measures the variation of the $N$ data points around the overall mean.\n$$\nSST =  \\sum_{i=1}^{I}\\sum_{j=1}^{n_{i}}(y_{ij}-\\bar{y}..)^{2} = (N-1)Var(Y)\n$$\n\n### Analysis of Variance Table*: \n\nThe results of an analysis of variance test are always summarized in an ANOVA table. The format of an ANOVA table is as follows:\n\n+----------+--------+-------+---------------------------+-------+\n| Source   | SS     | df    | MS                        | F     |\n+:=========+:======:+:=====:+:=========================:+:=====:+\n| Groups   | SSG    | $I-1$ | MSG = $\\frac{SSG}{I-1}$   | $\\frac{MSG}{MSE}$ |\n+----------+--------+-------+---------------------------+-------+\n| Error    | SSE    | $N-I$ | MSE = $\\frac{MSG}{N-I}$   |\n+----------+--------+-------+---------------------------+\n|**Total** | **SST**| $N-1$ |       |\n+----------+--------+-------+-------+\n\n\n### The F-distribution\n\nThe $p$-value of the test is the **area to the right** of the F statistic density curve. This is always to the right because the F-distribution is not symmetric, truncated at 0 and skewed right. This is true regardless of the $df$.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bivariate_analysis_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' fig-pos='!h' width=480}\n:::\n:::\n\n\n### Assumptions\n\nGenerally we must check three conditions on the data before performing ANOVA:\n\n* The observations are independent within and across groups\n* The data within each group are nearly normal\n* The variability across the groups is about equal.\n\nWhen these three conditions are met, we may perform an ANOVA to determine whether the data provide strong evidence against the null hypothesis that all the $\\mu_i$ are equal.\n\n\n### Example: A comparison of plant species under low water conditions\nThe `PLANTS1` data file gives the percent of nitrogen in four different species of plants grown in a laboratory. The researchers collected these data in parts of the country where there is very little rainfall. To examine the effect of water, they varied the amount per day from 50mm to 650mm in 100mm increments. There were 9 plants per species-by-water combination. Because the plants are to be used primarily for animal food, with some parts that can be consumed by people, a high nitrogen content is very desirable. Let's formally test to see if the nitrogen content in the plants differ across species.\n\n\n::: {.cell}\n\n:::\n\n\n**1. Identify response and explanatory variables.**\n\n* The quantitative response variable is % nitrogen (`pctnit`)\n* The categorical explanatory variable is species (`species`)\n\n**2. Visualize and summarize bivariate relationship.**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot.nitrogen.species <- plants1 %>% select(species, pctnit) %>% na.omit()\n\nggplot(plot.nitrogen.species, aes(x=species, y = pctnit, fill=species)) +\n      geom_boxplot(width=.3) + geom_violin(alpha=.4) +\n      labs(x=\"Species\") +\n      scale_fill_viridis_d(guide=FALSE) +\n      stat_summary(fun.y=\"mean\", geom=\"point\", size=3, pch=17,\n      position=position_dodge(width=0.75))\n```\n\n::: {.cell-output-display}\n![](bivariate_analysis_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' fig-pos='!h' width=480}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot.nitrogen.species %>% group_by(species) %>%\n summarise(mean=mean(pctnit, na.rm=TRUE),\n             sd = sd(pctnit, na.rm=TRUE),\n             IQR = IQR(pctnit, na.rm=TRUE)) %>% kable()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> species </th>\n   <th style=\"text-align:right;\"> mean </th>\n   <th style=\"text-align:right;\"> sd </th>\n   <th style=\"text-align:right;\"> IQR </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3.039810 </td>\n   <td style=\"text-align:right;\"> 0.2506118 </td>\n   <td style=\"text-align:right;\"> 0.2690 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2 </td>\n   <td style=\"text-align:right;\"> 2.092841 </td>\n   <td style=\"text-align:right;\"> 0.2377523 </td>\n   <td style=\"text-align:right;\"> 0.2725 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3 </td>\n   <td style=\"text-align:right;\"> 3.284365 </td>\n   <td style=\"text-align:right;\"> 0.3218599 </td>\n   <td style=\"text-align:right;\"> 0.5065 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4 </td>\n   <td style=\"text-align:right;\"> 1.195587 </td>\n   <td style=\"text-align:right;\"> 0.2342217 </td>\n   <td style=\"text-align:right;\"> 0.3125 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nWhile the standard deviations are relatively similar across all species, the means are different (3.04 v. 2.09 v. 3.28 v. 1.20), with species 3 having the largest mean nitrogen content and species 4 the smallest. Species 3 has the highest IQR and species 1 has the lowest 0.506 v.\\ 0.269).\n\n**3. Write the relationship you want to examine in the form of a research question.**\n\n* Null Hypothesis: There is no difference in the average nitrogen content among plant species 1 through 4.\n* Alternative Hypothesis: There is a difference in the average nitrogen content among plant species 1 through 4.\n\n**4. Perform an appropriate statistical analysis.**\n\nI. Let $\\mu_{1}$, $\\ldots$, $\\mu_{4}$ be the mean nitrogen content in plant species 1 through 4 respectively.\n\nII.  \n$H_{0}: \\mu_{1} = \\mu_{2} = \\mu_{3} = \\mu_{4}$  \n$H_{A}:$ At least one mean is different.  \n\nIII. We are comparing means from multiple groups, so an ANOVA is the appropriate procedure. We need to check for independence, approximate normality and approximately equal variances across groups.\n   \n**Independence:** We are assuming that each plant was sampled independently of each other, and that the species themselves are independent of each other.\n \n**Normality:** With grouped data it's easier to look at the histograms than qqplots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(plants1, aes(x=pctnit, fill=species)) + ylab(\"\") + geom_density() + \n  facet_grid(species~.) +\n  theme(legend.position=\"bottom\") +\n  scale_y_continuous(breaks=NULL) + scale_fill_viridis_d()\n```\n\n::: {.cell-output-display}\n![](bivariate_analysis_files/figure-html/unnamed-chunk-9-1.png){width=288}\n:::\n:::\n\n\nThe distributions per group tend to follow an approximate normal distribution.\n\n**Equal variances:** One way to assess if the groups have approximately equal variances is by comparing the IQR across groups.\n\n::: {.cell}\n\n```{.r .cell-code}\nplants1 %>% group_by(species) %>% summarise(IQR = IQR(pctnit), SD = sd(pctnit)) %>% kable()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> species </th>\n   <th style=\"text-align:right;\"> IQR </th>\n   <th style=\"text-align:right;\"> SD </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.2690 </td>\n   <td style=\"text-align:right;\"> 0.2506118 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.2725 </td>\n   <td style=\"text-align:right;\"> 0.2377523 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.5065 </td>\n   <td style=\"text-align:right;\"> 0.3218599 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.3125 </td>\n   <td style=\"text-align:right;\"> 0.2342217 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe IQRs are similar so assumption of equal variances is not grossly violated. We can proceed with the ANOVA procedure.\n\nIV. We use the `aov(response $\\sim$ predictor)` function on the relationship between the nitrogen levels and tree species. We then pipe in `summary()` to make the output display nicely.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(pctnit~species, data=plants1) %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Df Sum Sq Mean Sq F value Pr(>F)    \nspecies       3 172.39   57.46   827.5 <2e-16 ***\nResiduals   248  17.22    0.07                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n**5. Write a conclusion in the context of the problem.**\nThe results of the ANOVA test indicate that at least one species has a different average nitrogen content than the other varieties ($p<$.001).\n\n\n### Coefficient of determination $R^{2}$\n\nThe coefficient of determination is defined as $R^{2} = \\frac{SSG}{SST}$ and can be interpreted as the \\% of the variation seen in the outcome that is due to subject level variation within each of the treatment groups. The strength of this measure can be thought of in a similar manner as the correlation coefficient $r$: $< .3$ indicates a poor fit, $< .5$ indicates a medium fit, and $> .7$ indicates a good fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n172.39/(172.39+17.22)*100\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 90.9182\n```\n:::\n:::\n\n\nA large amount (91%) of the variation seen in nitrogen content in the plant can be explained by the species of plant.\n\n\n### Multiple Comparisons\nSuppose that an ANOVA test reveals that there is a difference in at least one of the means. How can we determine which groups are significantly different without increasing our chance of a Type I error?\n\nSimple! We perform all the pairwise comparisons but using a test statistic that retains a **family-wise error rate** of 0.05 (or our chosen $\\alpha$). There are different methods to adjust for multiple comparisons, we will be using the **Tukey HSD (honest significant difference) test**. Continuing on with the analysis of nitrogen across plant species.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTukeyHSD(aov(pctnit~species, data=plants1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = pctnit ~ species, data = plants1)\n\n$species\n          diff        lwr        upr   p adj\n2-1 -0.9469683 -1.0684156 -0.8255209 0.0e+00\n3-1  0.2445556  0.1231082  0.3660029 2.4e-06\n4-1 -1.8442222 -1.9656696 -1.7227748 0.0e+00\n3-2  1.1915238  1.0700764  1.3129712 0.0e+00\n4-2 -0.8972540 -1.0187014 -0.7758066 0.0e+00\n4-3 -2.0887778 -2.2102252 -1.9673304 0.0e+00\n```\n:::\n:::\n\n\n\\doublespace\nThe results from Tukey's HSD for all pairwise comparisons indicate that the average\nnitrogen content in one species is significantly different from each of the three other\nspecies. The nice benefit of this procedure is that the difference between the means\nof the two groups are compared, and a 95confidence interval for each difference\nis included. So specifically, species 2 has on average 0.94 (0.82, 1.09) lower percent \nnitrogen compared to species 1 ($p<.0001$). Also, species 3 has on average 1.19 (1.07, 1.31) \nhigher percent nitrogen compared to species 2 ($p<.0001$).\n\n\n----\n\n\n## (C~C) Multiple Proportions: $\\chi^{2}$ {#bv-chisq}\n\n\nRecall that the point estimates for the proportion of an event occurring is $\\frac{x}{n}$, \nwhere $x$ is the number of times the event occurs out of $n$ records. \nIn this section we we would like to make conclusions about the difference in two\npopulation proportions: $p_1 - p_2$. In other words we're testing the hypothesis that $p_{1}-p_{2}=0$.\n\nOur estimate for the difference in proportions based on the sample is $\\hat{p}_1 - \\hat{p}_2$. \nNo surprise there. What is slightly different is that we use a **pooled proportion** to check the condition\nof normality, and to calculate the standard error of the estimate. \nThis pooled proportion is calculated by pooling the number of events in both groups, divided by the effective\nsample size for those groups. \n\n$$ \\hat{p} = \\frac{x_{1} + x_{2}}{n_{1}+n_{2}} $$\n\nThen the standard error of the point estimate is calculated as\n\n$$ \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n_1} + \\frac{\\hat{p}(1-\\hat{p})}{n_2}} $$\n\nSo the equations for the Confidence Interval for the difference in proportions is,\n\n$$\n\\left( \\hat{p}_{1} - \\hat{p}_{2} \\right) \\pm t_{\\frac{\\alpha}{2}, df}\n\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n_1} + \\frac{\\hat{p}(1-\\hat{p})}{n_2}}\n$$\n\nwith test statistic, \n$$ \nt^{*} =  \\frac{\\left( \\hat{p}_{1} - \\hat{p}_{2} \\right) - d_{0}}\n        {\\left( \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n_1} + \\frac{\\hat{p}(1-\\hat{p})}{n_2}}\n        \\right )}\n$$\n\n\n### Conditions for the sampling distribution to be normal.\n\nThe difference $\\hat{p}_1 - \\hat{p}_2$ tends to follow a normal model when 1) each proportion separately follows a normal model, and 2) the two samples are independent of each other. #1 can be verified by checking the **success-failure condition** for each group. \n\nThat means: \n\n* $\\hat{p}n_{1} \\geq 10$, AND \n* $\\hat{p}n_{2} \\geq 10$, AND \n* $\\hat{q}n_{1} \\geq 10$, AND \n* $\\hat{q}n_{1} \\geq 10$. \n\nWhere, if I've forgotten to mention it yet, $q = 1-p$.\n\nWhen sample sizes are below 10, but still not _super_ small, say like 5, we turn to the non-parameteric version of this test called a **Fisher's Exact Test**. \n\n### Example: Are Mammograms effective? \n\nThis example comes from the [OpenIntro Statistics](https://www.openintro.org/stat/textbook.php?stat_book=os) textbook (3rd ed). \n\nA 30-year study was conducted with nearly 90,000 female participants.\n[(Miller AB. 2014)][mammo] During a 5-year\nscreening period, each woman was randomized to one of two groups: in the\nfirst group, women received regular mammograms to screen for breast cancer,\nand in the second group, women received regular non-mammogram breast cancer\nexams. No intervention was made during the following 25 years of the study,\nand we'll consider death resulting from breast cancer over the full 30-year\nperiod. Results from the study are summarized in the following table. \n\n[mammo]:Twenty five year follow-up for breast cancer incidence and mortality of the Canadian National Breast Screening Study: randomized screening trial. BMJ 2014;348:g366.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> Alive </th>\n   <th style=\"text-align:right;\"> Dead </th>\n   <th style=\"text-align:right;\"> Sum </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Control </td>\n   <td style=\"text-align:right;\"> 44405 </td>\n   <td style=\"text-align:right;\"> 505 </td>\n   <td style=\"text-align:right;\"> 44910 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Mammogram </td>\n   <td style=\"text-align:right;\"> 44425 </td>\n   <td style=\"text-align:right;\"> 500 </td>\n   <td style=\"text-align:right;\"> 44925 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Sum </td>\n   <td style=\"text-align:right;\"> 88830 </td>\n   <td style=\"text-align:right;\"> 1005 </td>\n   <td style=\"text-align:right;\"> 89835 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n<br>\n\nThe independent/explanatory variable is treatment (additional mammograms), and \nthe dependent/response variable is death from breast cancer. Are these measures \nassociated? \n\nIf mammograms are much more effective than non-mammogram breast cancer exams,\nthen we would expect to see additional deaths from breast cancer in the control\ngroup (there is a relationship). On the other hand, if mammograms are not as \neffective as regular breast cancer exams, we would expect to see no difference\nin breast cancer deaths in the two groups (there is no relationship).\n\nWhat we need to do is to figure out how many deaths would be **expected**, \nif there was no relationship between treatment death by breast cancer, and then \nexamine the **residuals** - the difference between the observed ($O_{ij}$) and \nexpected ($E_{ij}$). \n\nIn our DATA = MODEL + RESIDUAL framework, the DATA is the observed counts $O_{ij}$,\nand the MODEL is the expected counts $E_{ij}$. \n\nTo see how the expected counts are calculated, we need to define a few more symbols, \nso we can find our way around the cells of a table. \nJust like rows and columns in a matrix, rows are indexed first (as $i$ and columns indexed as $j$). \nSo the cell in the top left is $i=1$ and $j=1$. \n\n\n+----------+---------+----------+---------+\n| $O_{ij}$ | Alive   | Dead     | Total   |\n+:=========+:=======:+:========:+:=======:+\n| Mammo    |$n_{11}$ | $n_{12}$ | $n_{1.}$|\n+----------+---------+-----------+--------+\n| Control  |$n_{21}$ | $n_{22}$ | $n_{2.}$|\n+----------+---------+----------+---------+\n|Total     |$n_{.1}$ | $n_{.2}$ | $N$     |\n+----------+---------+----------+---------+\n\nThe expected count for each cell is calculated as the row total times the column total for that cell, divided by the overall total. Yes this will end up as a fraction. \n\n$$E_{ij} = \\frac{n_{i.}n_{.j}}{N}$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> Alive </th>\n   <th style=\"text-align:right;\"> Dead </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Control </td>\n   <td style=\"text-align:right;\"> 44407.58 </td>\n   <td style=\"text-align:right;\"> 502.4161 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Mammogram </td>\n   <td style=\"text-align:right;\"> 44422.42 </td>\n   <td style=\"text-align:right;\"> 502.5839 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe residuals are calculated as\n$$ RESIDUALS = (O_{ij} - E_{ij})$$ \n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> Alive </th>\n   <th style=\"text-align:right;\"> Dead </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Control </td>\n   <td style=\"text-align:right;\"> -0.0122616 </td>\n   <td style=\"text-align:right;\"> 0.1152775 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Mammogram </td>\n   <td style=\"text-align:right;\"> 0.0122596 </td>\n   <td style=\"text-align:right;\"> -0.1152583 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nExamining the residuals can tell us which combinations had counts more or less observations than expected. If mammograms were not associated with survival, there were 0.01 fewer people still alive than expected, and 0.11 more people dead. This is trivially small (2 x 2) example with very large sample sizes. There will be another example provided later. \n\nThe $\\chi^2$ test statistic is defined as the sum of the squared residuals, divided by the expected counts, and follows a $\\chi^2$ distribution with degrees of freedom (#rows -1)(#cols -1). \n\n$$ \\sum_{ij}\\frac{(O_{ij}-E_{ij})^{2}}{E_{ij}} $$\n\nLike every other statistical test, large values of test statistics correspond to low p-values. \n\nBelow is a picture of the distribution for the current example. The p-value is reported on the left (vertically), the purple shaded area denotes the rejection region if we were using a hard cutoff of 0.05. (The rejection region is the area where the test statistic had to be at for a p-value to be smaller than .05.). For this example the test statistic was 0.017, which corresponds to a p-value of 0.895. Thus, this study does not provide enough evidence to support the claim that mammograms decrease the rate of deaths by breast cancer. \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bivariate_analysis_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' fig-pos='!h' width=672}\n:::\n:::\n\n\n### Example: Smoking and General Health\n\nMore often than not, we will have the full data available. That is, data at each individual record not just a summary table like in the previous example. Let's work through an example. \n\nUsing the Addhealth data set, what can we say about the relationship between smoking and a person's perceived general level of general health? \n\n\n**1. Identify response and explanatory variables.**\n\n* The binary explanatory variable is whether the person has ever smoked an entire cigarette (`eversmoke_c`)\n* The categorical explanatory variable is the person's general health (`genhealth`) and has levels \"Excellent\", \"Very Good\", \"Good\", \"Fair\", and \"Poor\". \n\n\n**2. Visualize and summarise bivariate relationship.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::plot_xtab(grp=addhealth$eversmoke_c, x=addhealth$genhealth, \n                  show.total = FALSE, margin=\"row\", legend.title=\"\") \n```\n\n::: {.cell-output-display}\n![](bivariate_analysis_files/figure-html/unnamed-chunk-18-1.png){width=768}\n:::\n:::\n\n\nThe percentage of smokers seems to increase as the general health status decreases. Almost three-quarters (73%, n=40) of those reporting poor health have smoked an entire cigarette at least once in their life compared to 59% (n=573) of those reporting excellent health. \n\n**3. Write the relationship you want to examine in the form of a research question.**\n\nIs the proportion of those who have ever smoked equal across all levels of general health? \n\n* Null Hypothesis: The proportion of smokers in each general health category is the same. \n* Alternate Hypothesis: At least one proportion is different.\n\n\n**4. Perform an appropriate statistical analysis.**\n\nI. Define the parameters under consideration.\n\n- Let $p_{1}$ be the true proportion of smokers within the ``Excellent\" health category.\n- Let $p_{2}$ be the true proportion of smokers within the ``Very good\" health category. \n- Let $p_{3}$ be the true proportion of smokers within ``Good\" health category.\n- Let $p_{4}$ be the true proportion of smokers within ``Fair\" health category.\n- Let $p_{5}$ be the true proportion of smokers within ``Poor\" health category.\n\nII. $H_{0}: p_{1} = p_{2} = p_{3} = p_{4} = p_{5}$  \n    $H_{A}:$ At least one proportion is different.\n    \nIII. I will conduct a $\\chi$-squared test of association. There is at least 10 observations in each combination of smoking status and general health.\n\nIV. Conduct the test. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhealth.smoke.model <- chisq.test(addhealth$genhealth, addhealth$eversmoke_c)\nhealth.smoke.model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  addhealth$genhealth and addhealth$eversmoke_c\nX-squared = 30.795, df = 4, p-value = 3.371e-06\n```\n:::\n:::\n\n\nWe have strong evidence against the null; the $p$-value is less than .0001. \n\n**5. Write a conclusion in context of the problem.**\nWe can conclude that there is an association between ever smoking a cigarette in their life and perceived general health ($\\chi^2$ = 30.8, df=4, $p<.0001$). \n\n### Multiple Comparisons\nJust like with ANOVA, if we find that the chi-squared test indicates that at least one proportion is different from the others, it's our job to figure out which ones might be different! We will analyze the residuals to accomplish this. Not by hand! Never again! You're not learning how to code for nothing. \n\nThe residuals are automatically stored in the model output. You can either print them out and look at the values directly: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhealth.smoke.model$residuals\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   addhealth$eversmoke_c\naddhealth$genhealth Non Smoker     Smoker\n          Excellent  3.4461139 -2.5168277\n          Very good  0.4810893 -0.3513578\n          Good      -2.4431255  1.7843072\n          Fair      -1.0556365  0.7709714\n          Poor      -0.9446378  0.6899048\n```\n:::\n:::\n\n\n\nOr you can extract them and save them as a data frame. Then use ggplot with `geom_raster` to fill in your squares.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot.residuals <- health.smoke.model$residuals %>% data.frame()\nggplot(plot.residuals, aes(x=addhealth.genhealth, y=addhealth.eversmoke_c)) +\n       geom_raster(aes(fill=Freq)) +  scale_fill_viridis_c()\n```\n\n::: {.cell-output-display}\n![](bivariate_analysis_files/figure-html/unnamed-chunk-21-1.png){fig-align='center' width=576}\n:::\n:::\n\n\nThe proportion of those who have never smoked and report being in Excellent health is higher than expected if these two measures were independent (high positive residual means observed is greater than expected). A lower percent of people reporting Good health never smoked, which is lower than expected if smoking and health status were independent. So these two categories are likely to be the groups that have a different proportion of lifetime smoker $p_{i}$ compared to the other groups. \n\n\n\n## (Q~Q) Correlation {#bv-corr}\n\nThe **correlation coefficient** is designated by $r$ for the sample correlation, and $\\rho$ for the population correlation. The correlation is a measure of the strength and direction of a _linear relationship_ between two variables. \n\nThe correlation ranges from +1 to -1. A correlation of +1 means that there is a perfect, positive linear relationship between the two variables. A correlation of -1 means there is a perfect, negative linear relationship between the two variables.\nIn both cases, knowing the value of one variable, you can perfectly predict the value of the second.\n\n### Strength of the correlation\n\nHere are rough estimates for interpreting the strengths of correlations based on the magnitude of $r$.\n\n* $|r| \\geq 0.7$: Very strong relationship\n* $0.4 \\leq |r| < 0.7$: Strong relationship\n* $0.3 \\leq |r| < 0.4$: Moderate relationship\n* $0.2 \\leq |r| < 0.3:$ Weak relationship\n* $|r| < 0.2:$ Negligible or no relationship\n\n\n### Example: Federal spending per capita and poverty rate\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ggplot(county, aes(x=poverty, y=fed_spend00)) +\n#  geom_point() + ylab(\"federal spending per capita\") +\n#  xlab(\"poverty rate\")\n#cor(county$poverty, county$fed_spend00, use=\"complete.obs\")\n```\n:::\n\n\n* There is a negligible, positive, linear relationship between poverty rate and per capita federal spending ($r = 0.03$). \n* Let $\\rho$ denote the true correlation between poverty rate and federal spending per capita. \n* Our null hypothesis is that there is no correlation between poverty rate and federal spending ($\\rho = 0$), and the alternative hypothesis is that they are correlated ($\\rho \\neq 0$). \n* We can use the `cor.test()` function to analyze the evidence in  favor of this alternative hypothesis. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#cor.test(county$poverty, county$fed_spend00)\n```\n:::\n\n\nWe conclude from this that there was a non-statistically significant, negligible correlation\nbetween poverty and federal spending ($r = 0.03 (-0.0003, .069), p = 0.05$). \n\n\n",
    "supporting": [
      "bivariate_analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}