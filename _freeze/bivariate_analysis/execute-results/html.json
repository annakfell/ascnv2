{
  "hash": "7d3f451fe6e88bd322b781d84bf2b25f",
  "result": {
    "engine": "knitr",
    "markdown": "# Bivariate Analysis {#sec-bivariate-analysis}\n\n::: callout-note\n#### Packages Used\n\nThis chapter uses the following packages:\n[knitr](https://yihui.org/knitr/#overview), [kableExtra](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html), and the [Addhealth](https://www.norcalbiostat.com/data/#AddHealth) dataset.\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nSo far we have been concerned with making inference about a single\npopulation parameter. Many problems deal with comparing a parameter\nacross two or more groups. Research questions include questions like:\n\n-   Does the average life span differ across subspecies of a particular\n    turtle?\n-   Who has a higher percentage of the female vote - Democrats or\n    Republicans?\n\nA good way to think about all statistical models is that the observed\ndata comes from some true model with some random error.\n\n> DATA = MODEL + RESIDUAL\n\nThe `MODEL` is a mathematical formula (like $y = f(x)$). The formulation\nof the `MODEL` will change depending on the number of, and data types of\nexplanatory variables. One goal of inferential analysis is to explain\nthe variation in our data, using information contained in other\nmeasures.\n\n## Assumption of independent observations\n\nThe primary assumption of most standard statistical procedures is that\nobservations are independent of each other. That is, the value of one\nobservation does not change or affect another observation.\n\nHowever, there are many examples where measurements are made on subjects\nbefore and after a certain exposure or treatment (pre-post), or an\nexperiment to compare two cell phone packages might use pairs of\nsubjects that are the same age, sex and income level. One subject would\nbe randomly assigned to the first phone package, the other in the pair\nwould get the second phone package. This chapter only deals with\nnon-correlated analyses, leaving that topic for a later chapter.\n\n## Choosing appropriate bivariate analysis\n\nChoosing which statistical analyses procedure is appropriate completely\ndepending on the data types of the explanatory and response variable.\nThis is a simplified table, only covering the common/standard types of\nbivariate analysis.\n\n::: {.callout-warning appearance=\"simple\"}\n> figure out how to get table here. latex doesnt work. - some options:\n> raw markdown, knitr::kable, mermaid diagram\n:::\n\nFor this set of notes, the variable types are referred to using the\nfirst letter, e.g. *Q* for quantitative, *B* for binary, and *C* for\ncategorical. Thus a T-test is a (Q $\\sim$ B) analysis, and a correlation\nanalysis is (Q $\\sim$ Q) analysis.\n\n::: {.callout-warning appearance=\"simple\"}\n> county dataset will need to be replaced or example switched - no\n> longer included in openintro & the csv on\n> https://www.norcalbiostat.com/data/#countyComplete is missing federal\n> spending column\n:::\n\n## (Q\\~B) Two means: T-Test\n\nIt is common to compare means from different samples. For instance, we\nmight investigate the effectiveness of a certain educational\nintervention by looking for evidence of greater reading ability in the\ntreatment group against a control group. That is, our research\nhypothesis is that reading ability of a child is associated with an\neducational intervention.\n\nThe null hypothesis states that there is no relationship, or no effect,\nof the educational intervention (binary explanatory variable) on the\nreading ability of the child (quantitative response variable). This can\nbe written in symbols as follows:\n\n$$H_{0}: \\mu_{1} = \\mu_{2}\\mbox{ or }\\qquad  H_{0}: \\mu_{1} -\\mu_{2}=0$$\n\nwhere $\\mu_{1}$ is the average reading score for students in the control\ngroup (no intervention) and $\\mu_{2}$ be the average reading score for\nstudents in the intervention group. Notice it can be written as one mean\nequals the other, but also as the difference between two means equaling\nzero. The alternative hypothesis $H_{A}$ states that there is a\nrelationship:\n\n$$H_{A}: \\mu_{1} \\neq \\mu_{2} \\qquad \\mbox{ or } \\qquad H_{A}: \\mu_{1}-\\mu_{2} \\neq 0$$\n\n### Assumptions\n\n-   The data distribution for each group is approximately normal.\n-   The scores are independent within each group.\n-   The scores from the two groups are independent of each other (i.e.\n    the two samples are independent).\n\n### Sampling Distribution for the difference\n\nWe use $\\bar{x}_1 - \\bar{x}_2$ as a point estimate for $\\mu_1 - \\mu_2$,\nwhich has a standard error of\n\n$$\n SE_{\\bar{x}_1 - \\bar{x}_2}\n   = \\sqrt{SE_{\\bar{x}_1}^2 + SE_{\\bar{x}_2}^2}\n     = \\sqrt{\\frac{\\sigma^{2}_{1}}{n_1} + \\frac{\\sigma^{2}_{2}}{n_2}}\n$$\n\nSo the equations for a Confidence Interval is,\n\n$$\n  \\left( \\bar{x}_{1} - \\bar{x}_{2} \\right) \\pm t_{\\frac{\\alpha}{2}, df}\n    \\sqrt{ \\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}} }\n$$\n\nand Test Statistic is,\n\n$$\n  t^{*} =  \\frac{\\left( \\bar{x}_{1} - \\bar{x}_{2} \\right) - d_{0}}\n       {\\left( \\sqrt{ \\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}} }\n       \\right )} \n$$\n\nTypically it is unlikely that the population variances $\\sigma^{2}_{1}$\nand $\\sigma^{2}_{2}$ are known so we will use sample variances\n$s^{2}_{1}$ and $s^{2}_{2}$ as estimates.\n\nWhile you may never hand calculate these equations, it is important to\nsee the format, or structure, of the equations. Equation\n\\ref{2sampCImean} has the same format of\n\n$$ \\mbox{point estimate} \\pm 2*\\mbox{standard error}$$\n\nregardless what it is we're actually trying to estimate. Thus in a\npinch, you can calculate approximate confidence intervals for whatever\nestimate you are trying to understand, given only the estimate and\nstandard error, even if the computer program does not give it to you\neasily or directly.\n\n### Example: Smoking and BMI\n\n::: {.callout-tip appearance=\"minimal\"}\nWe would like to know, **is there convincing evidence that the average\nBMI differs between those who have ever smoked a cigarette in their life\ncompared to those who have never smoked?** This example uses the\nAddhealth dataset.\n:::\n\n**1. Identify response and explanatory variables.**\n\n-   The quantitative response variable is BMI (variable \\R{BMI})\n-   The binary explanatory variable is whether the person has ever\n    smoked a cigarette (variable \\R{eversmoke\\_c})\n\n**2. Visualize and summarize bivariate relationship.**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot.bmi.smoke <- addhealth %>% select(eversmoke_c, BMI) %>% na.omit()\n\nggplot(plot.bmi.smoke, aes(x=eversmoke_c, y=BMI, fill=eversmoke_c)) +\n      geom_boxplot(width=.3) + geom_violin(alpha=.4) +\n      labs(x=\"Smoking status\") +\n      scale_fill_viridis_d(guide=FALSE) +\n      stat_summary(fun.y=\"mean\", geom=\"point\", size=3, pch=17, \n                   position=position_dodge(width=0.75))\n```\n\n::: {.cell-output-display}\n![](bivariate_analysis_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot.bmi.smoke %>% group_by(eversmoke_c) %>%\n summarise(mean=mean(BMI, na.rm=TRUE),\n             sd = sd(BMI, na.rm=TRUE),\n             IQR = IQR(BMI, na.rm=TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 4\n  eversmoke_c  mean    sd   IQR\n  <fct>       <dbl> <dbl> <dbl>\n1 Non Smoker   29.7  7.76  9.98\n2 Smoker       28.8  7.32  9.02\n```\n\n\n:::\n:::\n\n\n\n\nSmokers have an average BMI of 28.8, smaller than the average BMI of\nnon-smokers at 29.7. Nonsmokers have more variation in their BMIs (sd\n7.8 v. 7.3 and IQR 9.98 v. 9.02), but the distributions both look\nnormal, if slightly skewed right.\n\n**3. Write the relationship you want to examine in the form of a\nresearch question.**\n\n-   Null Hypothesis: There is no relationship between BMI and smoking\n    status.\n-   Alternate Hypothesis: There is a relationship between BMI and\n    smoking status.\n\n**4. Perform an appropriate statistical analysis.**\n\nI. Let $\\mu_1$ denote the average BMI for nonsmokers, and $\\mu_2$ the\naverage BMI for smokers.\n\nII. $\\mu_1 - \\mu_2 = 0$ There is no difference in the average BMI\n    between smokers and nonsmokers. $\\mu_1 - \\mu_2 \\neq 0$ There is a\n    difference in the average BMI between smokers and nonsmokers.\n\nIII. We are comparing the means between two independent samples. A\n     Two-Sample T-Test for a difference in means will be conducted. The\n     assumptions that the groups are independent is upheld because each\n     individual can only be either a smoker or nonsmoker. The difference\n     in sample means $\\bar{x_1} - \\bar{x_2}$ is normally distributed --\n     this is a valid assumption due to the large sample size and that\n     differences typically are normally distributed. The observations\n     are independent, and the variability is roughly equal (IQR 9.9 v.\n     9.0).\n\nIV. We use the `t.test` function, but use model notation of the format\n    `outcome` $\\sim$ `category`. Here, `BMI` is our continuous outcome\n    that we're testing across the (binary) categorical predictor\n    `eversmoke_c`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(BMI ~ eversmoke_c, data=addhealth)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  BMI by eversmoke_c\nt = 3.6937, df = 3395.3, p-value = 0.0002245\nalternative hypothesis: true difference in means between group Non Smoker and group Smoker is not equal to 0\n95 percent confidence interval:\n 0.3906204 1.2744780\nsample estimates:\nmean in group Non Smoker     mean in group Smoker \n                29.67977                 28.84722 \n```\n\n\n:::\n:::\n\n\n\n\nWe have very strong evidence against the null hypothesis, $p = 0.0002$.\n\n**5. Write a conclusion in the context of the problem.**\n\nOn average, nonsmokers have a significantly higher BMI by 0.83 (0.39,\n1.27) compared to nonsmokers ($p = 0.0002$).\n\n::: {.callout-warning appearance=\"simple\"}\nAlways check the output against the direction you are testing. R always\nwill calculate a difference as group 1 - group 2, and it defines the\ngroups alphabetically. For example, for a factor variable that has\ngroups A and B, R will automatically calculate the difference as A-B. In\nthis example it is Nonsmoker - Smoker.\n:::\n\n## (Q\\~C) Multiple means: ANOVA {#bv-anova}\n\nFrequently, a researcher wants to compare the means of an outcome across\nthree or more treatments in a single experiment. We might initially\nthink to do pairwise comparisons (1v2, 1v3, 2v3) for a total of three\ncomparisons. However, this strategy can be treacherous. If we have many\ngroups and do many comparisons, it is likely that we will eventually\nfind a difference just by chance, even if there is no difference in the\npopulations.\n\nWhen we analyze a conventional two-treatment experiment, we are prepared\nto run a 1 in 20 risk of an apparently significant result arising purely\nby accident (the 5% chance of a Type I error). We regard such a risk as\nbeing fairly unlikely and feel justified in accepting with confidence\nany significant results we obtain.\n\nAnalyzing a single experiment as a series of 10 treatment pairs is a\nvery different proposition. The chance of an apparently significant\nresult arising purely by chance somewhere in the 10 analyses increases\ndramatically. Using a 5% error rate, the chance of NOT making at Type I\nerror is .95. To not make a Type I error 10 times is $.95^{10} = .6$.\nThat means there is a 40% chance of making a Type I error!\n\n[![Significant - xkcd\nwebcomic](https://imgs.xkcd.com/comics/significant.png)](https://xkcd.com/882)\n\n### Example: Visual comparison\n\nExamine the figure below. Compare groups I, II, and III. Can you\nvisually determine if the differences in the group centers is due to\nchance or not? What about groups IV, V, and VI?\n\n![Side-by-side dot plot for the outcomes for six\ngroups.](images/toyANOVA.png)\n\nSo we need some method of comparing treatments for more than two groups\nat a time. This is done using an Analysis of Variance (ANOVA) model.\n\n::: {.callout-note icon=\"false\"}\n## Terminology\n\n-   **Response Variable**: The response variable in the ANOVA setting is\n    the quantitative (continuous) variable that we want to compare among\n    the different treatments.\n-   **Factor/Treatment**: A property or characteristic (categorical\n    variable) that allows us to distinguish the different populations\n    from one another. An independent variable to be studied in an\n    investigation such as temperature, type of plant, color of flower,\n    location.\n-   **Factor/Treatment level**: Factors have different levels, such as 3\n    temperatures, 5 locations, 3 colors, etc.\n-   **Within-sample Variation**: Variation within a sample from one\n    population. Individuals who receive the same treatment will\n    experience identical experimental conditions. The variation within\n    each of the treatment groups must therefore be a consequence of\n    solely random variation.\n-   **Between-sample Variation**: Variation between samples. This is the\n    difference between the group means. If some treatments are genuinely\n    more effective than others, then we would expect to see relatively\n    large differences between the treatment means and a relatively large\n    between-treatments variation.\n:::\n\n### Formulation of the One-way ANOVA model\n\nANOVA is a mathematical technique which uses a model based approach to\npartition the variance in an experiment into different sources of\nvariance. This technique enables us to test if most the variation in the\ntreatment means is due to differences between the groups rather than\nrandom chance.\n\nStarting with our generic conceptual understanding of statistical\nmodels:\n\n> DATA = MODEL + RESIDUAL\n\nour MODEL for this situation is the group membership. Does knowing what\ngroup an observation is in tell you about the location of the data? The\none-way (or one-factor) ANOVA model is\n\n$$\ny_{ij} = \\mu_{i} + \\epsilon_{ij} \\qquad \\qquad\n\\epsilon_{ij} \\overset{iid}{\\sim} \\mathcal{N}(0,\\sigma^{2})\n$$\n\nfor $i=1, \\ldots, I$ factor levels and $j = 1, \\ldots, n_{i}$ subjects\nwithin each factor level. The random error terms are independently and\nidentically distributed (iid) normally with common variance.\n\nThe null and alternative hypotheses are always phrased as follows:\n\n-   $H_0$: The mean outcome is the same across all groups.\n    $\\mu_1 = \\mu_2 = \\cdots = \\mu_k$\n-   $H_A$: At least one mean is different.\n\nHow do we compare means using an **AN**alysis **O**f **VA**riance? By\ncomparing the portion of the variance in the outcome that is explained\nby the groups, to the portion that's leftover is due to unexplained\nrandomness. Essentially we're comparing the ratio of `MODEL` to\n`RESIDUAL`.\n\nThe total variation of the observed data is broken down into 2 parts:\n\n> Total Variation = Between Group Variation + Within Group Variation\n\nVariation is measured using the Sum of Squares (SS): The sum of the\nsquares within a group (SSE), the sum of squares between groups (SSG),\nand the total sum of squares (SST).\n\n**SSG (Between groups)**: Measures the variation of the $I$ group means\naround the overall mean. $$\n  SSG = \\sum_{i=1}^{I}n_{i}(\\bar{y}_{i.}-\\bar{y}..)^{2} = n_{1}(\\bar{y}_{1.}-\\bar{y}..)^{2} + n_{2}(\\bar{y}_{2.}-\\bar{y}..)^{2} + n_{3}(\\bar{y}_{3.}-\\bar{y}..)^{2}\n$$\n\n**SSE (Within group)**: Measures the variation of each observation\naround its group mean. $$\nSSE = \\sum_{i=1}^{I}\\sum_{j=1}^{n_{i}}(y_{ij}-\\bar{y}_{i.})^{2} = \\sum_{i=1}^{I}(n_{i}-1)Var(Y_{i})\n$$\n\n**SST (Total)**: Measures the variation of the $N$ data points around\nthe overall mean. $$\nSST =  \\sum_{i=1}^{I}\\sum_{j=1}^{n_{i}}(y_{ij}-\\bar{y}..)^{2} = (N-1)Var(Y)\n$$\n\n### Analysis of Variance table\n\nThe results of an analysis of variance test are always summarized in an\nANOVA table. The format of an ANOVA table is as follows:\n\n| Source    | SS      | $df$  | MS                        | F                 |\n|-----------|---------|-------|---------------------------|-------------------|\n| Groups    | SSG     | $I-1$ | MSG = $\\frac{SSG}{I-1}\\ $ | $\\frac{MSG}{MSE}$ |\n| Error     | SSE     | $N-I$ | MSE = $\\frac{MSG}{N-I}\\ $ |                   |\n| **Total** | **SST** | $N-1$ |                           |                   |\n\n### The F-distribution\n\nThe $p$-value of the test is the **area to the right** of the F\nstatistic density curve. This is always to the right because the\nF-distribution is not symmetric, truncated at 0 and skewed right. This\nis true regardless of the $df$.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bivariate_analysis_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' fig-pos='!h' width=480}\n:::\n:::\n\n\n\n\n### Assumptions\n\nGenerally we must check three conditions on the data before performing\nANOVA:\n\n-   The observations are independent within and across groups\n-   The data within each group are nearly normal\n-   The variability across the groups is about equal.\n\nWhen these three conditions are met, we may perform an ANOVA to\ndetermine whether the data provide strong evidence against the null\nhypothesis that all the $\\mu_i$ are equal.\n\n### Example: A comparison of plant species under low water conditions\n\n::: {.callout-tip appearance=minimal}\n\nThe `PLANTS1` data file gives the percent of nitrogen in four different\nspecies of plants grown in a laboratory. The researchers collected these\ndata in parts of the country where there is very little rainfall. To\nexamine the effect of water, they varied the amount per day from 50mm to\n650mm in 100mm increments. There were 9 plants per species-by-water\ncombination. Because the plants are to be used primarily for animal\nfood, with some parts that can be consumed by people, a high nitrogen\ncontent is very desirable. Let's formally test to see if the nitrogen\ncontent in the plants differ across species.\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n**1. Identify response and explanatory variables.**\n\n-   The quantitative response variable is % nitrogen (`pctnit`)\n-   The categorical explanatory variable is species (`species`)\n\n**2. Visualize and summarize bivariate relationship.**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot.nitrogen.species <- plants1 %>% select(species, pctnit) %>% na.omit()\n\nggplot(plot.nitrogen.species, aes(x=species, y = pctnit, fill=species)) +\n      geom_boxplot(width=.3) + geom_violin(alpha=.4) +\n      labs(x=\"Species\") +\n      scale_fill_viridis_d(guide=FALSE) +\n      stat_summary(fun.y=\"mean\", geom=\"point\", size=3, pch=17,\n      position=position_dodge(width=0.75))\n```\n\n::: {.cell-output-display}\n![](bivariate_analysis_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' fig-pos='!h' width=480}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot.nitrogen.species %>% group_by(species) %>%\n summarise(mean=mean(pctnit, na.rm=TRUE),\n             sd = sd(pctnit, na.rm=TRUE),\n             IQR = IQR(pctnit, na.rm=TRUE)) %>% kable()\n```\n\n::: {.cell-output-display}\n\n\n|species |     mean|        sd|    IQR|\n|:-------|--------:|---------:|------:|\n|1       | 3.039810| 0.2506118| 0.2690|\n|2       | 2.092841| 0.2377523| 0.2725|\n|3       | 3.284365| 0.3218599| 0.5065|\n|4       | 1.195587| 0.2342217| 0.3125|\n\n\n:::\n:::\n\n\n\n\nWhile the standard deviations are relatively similar across all species,\nthe means are different (3.04 v. 2.09 v. 3.28 v. 1.20), with species 3\nhaving the largest mean nitrogen content and species 4 the smallest.\nSpecies 3 has the highest IQR and species 1 has the lowest (0.506\nv. 0.269).\n\n**3. Write the relationship you want to examine in the form of a\nresearch question.**\n\n-   Null Hypothesis: There is no difference in the average nitrogen\n    content among plant species 1 through 4.\n-   Alternative Hypothesis: There is a difference in the average\n    nitrogen content among plant species 1 through 4.\n\n**4. Perform an appropriate statistical analysis.**\n\nI. Let $\\mu_{1}$, $\\ldots$, $\\mu_{4}$ be the mean nitrogen content in\nplant species 1 through 4 respectively.\n\nII. $H_{0}: \\mu_{1} = \\mu_{2} = \\mu_{3} = \\mu_{4}$\\\n    $H_{A}:$ At least one mean is different.\n\nIII. We are comparing means from multiple groups, so an ANOVA is the\n     appropriate procedure. We need to check for independence,\n     approximate normality and approximately equal variances across\n     groups.\n\nIndependence: We are assuming that each plant was sampled\nindependently of each other, and that the species themselves are\nindependent of each other.\n\nNormality: With grouped data it's easier to look at the histograms\nthan qqplots.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(plants1, aes(x=pctnit, fill=species)) + ylab(\"\") + geom_density() + \n  facet_grid(species~.) +\n  theme(legend.position=\"bottom\") +\n  scale_y_continuous(breaks=NULL) + scale_fill_viridis_d()\n```\n\n::: {.cell-output-display}\n![](bivariate_analysis_files/figure-html/unnamed-chunk-9-1.png){width=288}\n:::\n:::\n\n\n\n\nThe distributions per group tend to follow an approximate normal\ndistribution.\n\nEqual variances: One way to assess if the groups have approximately\nequal variances is by comparing the IQR across groups.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplants1 %>% group_by(species) %>% summarise(IQR = IQR(pctnit), SD = sd(pctnit)) %>% kable()\n```\n\n::: {.cell-output-display}\n\n\n|species |    IQR|        SD|\n|:-------|------:|---------:|\n|1       | 0.2690| 0.2506118|\n|2       | 0.2725| 0.2377523|\n|3       | 0.5065| 0.3218599|\n|4       | 0.3125| 0.2342217|\n\n\n:::\n:::\n\n\n\n\nThe IQRs are similar so assumption of equal variances is not grossly\nviolated. We can proceed with the ANOVA procedure.\n\nIV. We use the `aov(response ~ predictor)` function on the\n    relationship between the nitrogen levels and tree species. We then\n    pipe in `summary()` to make the output display nicely.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(pctnit~species, data=plants1) %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Df Sum Sq Mean Sq F value Pr(>F)    \nspecies       3 172.39   57.46   827.5 <2e-16 ***\nResiduals   248  17.22    0.07                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n**5. Write a conclusion in the context of the problem.** The results of\nthe ANOVA test indicate that at least one species has a different\naverage nitrogen content than the other varieties ($p<$.001).\n\n### Coefficient of determination $R^{2}$\n\nThe coefficient of determination is defined as $R^{2} = \\frac{SSG}{SST}$\nand can be interpreted as the % of the variation seen in the outcome\nthat is due to subject level variation within each of the treatment\ngroups. The strength of this measure can be thought of in a similar\nmanner as the correlation coefficient $r$: $< .3$ indicates a poor fit,\n$< .5$ indicates a medium fit, and $> .7$ indicates a good fit.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n172.39/(172.39+17.22)*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 90.9182\n```\n\n\n:::\n:::\n\n\n\n\nA large amount (91%) of the variation seen in nitrogen content in the\nplant can be explained by the species of plant.\n\n### Multiple Comparisons\n\nSuppose that an ANOVA test reveals that there is a difference in at\nleast one of the means. How can we determine which groups are\nsignificantly different without increasing our chance of a Type I error?\n\nSimple! We perform all the pairwise comparisons but using a test\nstatistic that retains a **family-wise error rate** of 0.05 (or our\nchosen $\\alpha$). There are different methods to adjust for multiple\ncomparisons, we will be using the **Tukey HSD (honest significant\ndifference) test**. Continuing on with the analysis of nitrogen across\nplant species.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTukeyHSD(aov(pctnit~species, data=plants1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = pctnit ~ species, data = plants1)\n\n$species\n          diff        lwr        upr   p adj\n2-1 -0.9469683 -1.0684156 -0.8255209 0.0e+00\n3-1  0.2445556  0.1231082  0.3660029 2.4e-06\n4-1 -1.8442222 -1.9656696 -1.7227748 0.0e+00\n3-2  1.1915238  1.0700764  1.3129712 0.0e+00\n4-2 -0.8972540 -1.0187014 -0.7758066 0.0e+00\n4-3 -2.0887778 -2.2102252 -1.9673304 0.0e+00\n```\n\n\n:::\n:::\n\n\n\n \nThe results from Tukey's HSD for all pairwise comparisons indicate that\nthe average nitrogen content in one species is significantly different\nfrom each of the three other species. The nice benefit of this procedure\nis that the difference between the means of the two groups are compared,\nand a 95 percent confidence interval for each difference is included. So\nspecifically, species 2 has on average 0.94 (0.82, 1.09) lower percent\nnitrogen compared to species 1 ($p<.0001$). Also, species 3 has on\naverage 1.19 (1.07, 1.31) higher percent nitrogen compared to species 2\n($p<.0001$).\n\n## (C\\~C) Multiple Proportions: $\\chi^{2}$ {#bv-chisq}\n\nRecall that the point estimates for the proportion of an event occurring\nis $\\frac{x}{n}$, where $x$ is the number of times the event occurs out\nof $n$ records. In this section we we would like to make conclusions\nabout the difference in two population proportions: $p_1 - p_2$. In\nother words we're testing the hypothesis that $p_{1}-p_{2}=0$.\n\nOur estimate for the difference in proportions based on the sample is\n$\\hat{p}_1 - \\hat{p}_2$. No surprise there. What is slightly different\nis that we use a **pooled proportion** to check the condition of\nnormality, and to calculate the standard error of the estimate. This\npooled proportion is calculated by pooling the number of events in both\ngroups, divided by the effective sample size for those groups.\n\n$$ \\hat{p} = \\frac{x_{1} + x_{2}}{n_{1}+n_{2}} $$\n\nThen the standard error of the point estimate is calculated as\n\n$$ \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n_1} + \\frac{\\hat{p}(1-\\hat{p})}{n_2}} $$\n\nSo the equations for the Confidence Interval for the difference in\nproportions is,\n\n$$\n\\left( \\hat{p}_{1} - \\hat{p}_{2} \\right) \\pm t_{\\frac{\\alpha}{2}, df}\n\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n_1} + \\frac{\\hat{p}(1-\\hat{p})}{n_2}}\n$$\n\nwith test statistic, $$ \nt^{*} =  \\frac{\\left( \\hat{p}_{1} - \\hat{p}_{2} \\right) - d_{0}}\n        {\\left( \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n_1} + \\frac{\\hat{p}(1-\\hat{p})}{n_2}}\n        \\right )}\n$$\n\n### Conditions for the sampling distribution to be normal.\n\nThe difference $\\hat{p}_1 - \\hat{p}_2$ tends to follow a normal model\nwhen 1) each proportion separately follows a normal model, and 2) the\ntwo samples are independent of each other. #1 can be verified by\nchecking the **success-failure condition** for each group.\n\nThat means:\n\n-   $\\hat{p}n_{1} \\geq 10$, AND\n-   $\\hat{p}n_{2} \\geq 10$, AND\n-   $\\hat{q}n_{1} \\geq 10$, AND\n-   $\\hat{q}n_{1} \\geq 10$.\n\nWhere, if I've forgotten to mention it yet, $q = 1-p$.\n\nWhen sample sizes are below 10, but still not *super* small, say like 5,\nwe turn to the non-parameteric version of this test called a **Fisher's\nExact Test**.\n\n### Example: Are Mammograms effective?\n\n::: {.callout-tip appearance=minimal}\nThis example comes from the [OpenIntro\nStatistics](https://www.openintro.org/stat/textbook.php?stat_book=os)\ntextbook (3rd ed).\n\nA 30-year study was conducted with nearly 90,000 female participants.\n[Miller AB. 2014](https://doi.org/10.1136/bmj.g366) During a 5-year screening period, each\nwoman was randomized to one of two groups: in the first group, women\nreceived regular mammograms to screen for breast cancer, and in the\nsecond group, women received regular non-mammogram breast cancer exams.\nNo intervention was made during the following 25 years of the study, and\nwe'll consider death resulting from breast cancer over the full 30-year\nperiod. Results from the study are summarized in the following table.\n:::\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:right;\"> Alive </th>\n   <th style=\"text-align:right;\"> Dead </th>\n   <th style=\"text-align:right;\"> Sum </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Control </td>\n   <td style=\"text-align:right;\"> 44405 </td>\n   <td style=\"text-align:right;\"> 505 </td>\n   <td style=\"text-align:right;\"> 44910 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Mammogram </td>\n   <td style=\"text-align:right;\"> 44425 </td>\n   <td style=\"text-align:right;\"> 500 </td>\n   <td style=\"text-align:right;\"> 44925 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Sum </td>\n   <td style=\"text-align:right;\"> 88830 </td>\n   <td style=\"text-align:right;\"> 1005 </td>\n   <td style=\"text-align:right;\"> 89835 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n<br>\n\nThe independent/explanatory variable is treatment (additional\nmammograms), and the dependent/response variable is death from breast\ncancer. Are these measures associated?\n\nIf mammograms are much more effective than non-mammogram breast cancer\nexams, then we would expect to see additional deaths from breast cancer\nin the control group (there is a relationship). On the other hand, if\nmammograms are not as effective as regular breast cancer exams, we would\nexpect to see no difference in breast cancer deaths in the two groups\n(there is no relationship).\n\nWhat we need to do is to figure out how many deaths would be\n**expected**, if there was no relationship between treatment death by\nbreast cancer, and then examine the **residuals** - the difference\nbetween the observed ($O_{ij}$) and expected ($E_{ij}$).\n\nIn our DATA = MODEL + RESIDUAL framework, the DATA is the observed\ncounts $O_{ij}$, and the MODEL is the expected counts $E_{ij}$.\n\nTo see how the expected counts are calculated, we need to define a few\nmore symbols, so we can find our way around the cells of a table. Just\nlike rows and columns in a matrix, rows are indexed first (as $i$ and\ncolumns indexed as $j$). So the cell in the top left is $i=1$ and $j=1$.\n\n| $O_     \n    {     \n    i     \n    j}$   |                  A l ive                  |                                                      D ead                                                       |                                                                                                                  T o tal                                                                                                                   |\n|:-----------------|:----------------:|:----------------:|:----------------:|\n| M a mmo |                    $n_                    \n                                                    { \n                                                    1 \n                                                 1}$  |                                                        $n_                                                       \n                                                                                                                                                                      {  \n                                                                                                                                                                      1  \n                                                                                                                                                              2}$ \\| $n_ \n                                                                                                                                                                      {  \n                                                                                                                                                                      1  \n                                                                                                                                                                   .}$   |                                                                                                                                                                                                                                            |\n|         |                Co n t rol                 |                                                        $n_                                                       \n                                                                                                                                                                      {  \n                                                                                                                                                                      2  \n                                                                                                                                                                   1}$   |                                                                                                                     $n_                                                                                                                    \n                                                                                                                                                                                                                                                                                                                                                                                                                   {  \n                                                                                                                                                                                                                                                                                                                                                                                                                   2  \n                                                                                                                                                                                                                                                                                                                                                                                                           2}$ \\| $n_ \n                                                                                                                                                                                                                                                                                                                                                                                                                   {  \n                                                                                                                                                                                                                                                                                                                                                                                                                   2  \n                                                                                                                                                                                                                                                                                                                                                                                                                .}$   |\n| T o tal |                    $n_                    \n                                                    { \n                                                    . \n                                                 1}$  |                                                        $n_                                                       \n                                                                                                                                                                      {  \n                                                                                                                                                                      .  \n                                                                                                                                                                   2}$   |                                                                                                                    $N$                                                                                                                     |\n\nThe expected count for each cell is calculated as the row total times\nthe column total for that cell, divided by the overall total. Yes this\nwill end up as a fraction.\n\n$$E_{ij} = \\frac{n_{i.}n_{.j}}{N}$$\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:right;\"> Alive </th>\n   <th style=\"text-align:right;\"> Dead </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Control </td>\n   <td style=\"text-align:right;\"> 44407.58 </td>\n   <td style=\"text-align:right;\"> 502.4161 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Mammogram </td>\n   <td style=\"text-align:right;\"> 44422.42 </td>\n   <td style=\"text-align:right;\"> 502.5839 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nThe residuals are calculated as $$ RESIDUALS = (O_{ij} - E_{ij})$$\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:right;\"> Alive </th>\n   <th style=\"text-align:right;\"> Dead </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Control </td>\n   <td style=\"text-align:right;\"> -0.0122616 </td>\n   <td style=\"text-align:right;\"> 0.1152775 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Mammogram </td>\n   <td style=\"text-align:right;\"> 0.0122596 </td>\n   <td style=\"text-align:right;\"> -0.1152583 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nExamining the residuals can tell us which combinations had counts more\nor less observations than expected. If mammograms were not associated\nwith survival, there were 0.01 fewer people still alive than expected,\nand 0.11 more people dead. This is trivially small (2 x 2) example with\nvery large sample sizes. There will be another example provided later.\n\nThe $\\chi^2$ test statistic is defined as the sum of the squared\nresiduals, divided by the expected counts, and follows a $\\chi^2$\ndistribution with degrees of freedom (#rows -1)(#cols -1).\n\n$$ \\sum_{ij}\\frac{(O_{ij}-E_{ij})^{2}}{E_{ij}} $$\n\nLike every other statistical test, large values of test statistics\ncorrespond to low p-values.\n\nBelow is a picture of the distribution for the current example. The\np-value is reported on the left (vertically), the purple shaded area\ndenotes the rejection region if we were using a hard cutoff of 0.05.\n(The rejection region is the area where the test statistic had to be at\nfor a p-value to be smaller than .05.). For this example the test\nstatistic was 0.017, which corresponds to a p-value\nof 0.895. Thus, this study does not provide enough\nevidence to support the claim that mammograms decrease the rate of\ndeaths by breast cancer.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bivariate_analysis_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' fig-pos='!h' width=672}\n:::\n:::\n\n\n\n\n### Example: Smoking and General Health\n\nMore often than not, we will have the full data available. That is, data\nat each individual record not just a summary table like in the previous\nexample. Let's work through an example.\n\n::: {.callout-tip appearance=minimal}\nUsing the Addhealth data set, what can we say about the relationship\nbetween smoking and a person's perceived general level of general\nhealth?\n:::\n\n**1. Identify response and explanatory variables.**\n\n-   The binary explanatory variable is whether the person has ever\n    smoked an entire cigarette (`eversmoke_c`)\n-   The categorical explanatory variable is the person's general health\n    (`genhealth`) and has levels \"Excellent\", \"Very Good\", \"Good\",\n    \"Fair\", and \"Poor\".\n\n**2. Visualize and summarise bivariate relationship.**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::plot_xtab(grp=addhealth$eversmoke_c, x=addhealth$genhealth, \n                  show.total = FALSE, margin=\"row\", legend.title=\"\") \n```\n\n::: {.cell-output-display}\n![](bivariate_analysis_files/figure-html/unnamed-chunk-18-1.png){width=768}\n:::\n:::\n\n\n\n\nThe percentage of smokers seems to increase as the general health status\ndecreases. Almost three-quarters (73%, n=40) of those reporting poor\nhealth have smoked an entire cigarette at least once in their life\ncompared to 59% (n=573) of those reporting excellent health.\n\n**3. Write the relationship you want to examine in the form of a\nresearch question.**\n\nIs the proportion of those who have ever smoked equal across all levels\nof general health?\n\n-   Null Hypothesis: The proportion of smokers in each general health\n    category is the same.\n-   Alternate Hypothesis: At least one proportion is different.\n\n**4. Perform an appropriate statistical analysis.**\n\nI. Define the parameters under consideration.\n\n-   Let $p_{1}$ be the true proportion of smokers within the\n    \\`\\`Excellent\" health category.\n-   Let $p_{2}$ be the true proportion of smokers within the \\`\\`Very\n    good\" health category.\n-   Let $p_{3}$ be the true proportion of smokers within \\`\\`Good\"\n    health category.\n-   Let $p_{4}$ be the true proportion of smokers within \\`\\`Fair\"\n    health category.\n-   Let $p_{5}$ be the true proportion of smokers within \\`\\`Poor\"\n    health category.\n\nII. $H_{0}: p_{1} = p_{2} = p_{3} = p_{4} = p_{5}$\\\n    $H_{A}:$ At least one proportion is different.\n\nIII. I will conduct a $\\chi$-squared test of association. There is at\n     least 10 observations in each combination of smoking status and\n     general health.\n\nIV. Conduct the test.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhealth.smoke.model <- chisq.test(addhealth$genhealth, addhealth$eversmoke_c)\nhealth.smoke.model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  addhealth$genhealth and addhealth$eversmoke_c\nX-squared = 30.795, df = 4, p-value = 3.371e-06\n```\n\n\n:::\n:::\n\n\n\n\nWe have strong evidence against the null; the $p$-value is less than\n.0001.\n\n**5. Write a conclusion in context of the problem.** We can conclude\nthat there is an association between ever smoking a cigarette in their\nlife and perceived general health ($\\chi^2$ = 30.8, df=4, $p<.0001$).\n\n### Multiple Comparisons\n\nJust like with ANOVA, if we find that the chi-squared test indicates\nthat at least one proportion is different from the others, it's our job\nto figure out which ones might be different! We will analyze the\nresiduals to accomplish this. Not by hand! Never again! You're not\nlearning how to code for nothing.\n\nThe residuals are automatically stored in the model output. You can\neither print them out and look at the values directly:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhealth.smoke.model$residuals\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   addhealth$eversmoke_c\naddhealth$genhealth Non Smoker     Smoker\n          Excellent  3.4461139 -2.5168277\n          Very good  0.4810893 -0.3513578\n          Good      -2.4431255  1.7843072\n          Fair      -1.0556365  0.7709714\n          Poor      -0.9446378  0.6899048\n```\n\n\n:::\n:::\n\n\n\n\nOr you can extract them and save them as a data frame. Then use ggplot\nwith `geom_raster` to fill in your squares.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot.residuals <- health.smoke.model$residuals %>% data.frame()\nggplot(plot.residuals, aes(x=addhealth.genhealth, y=addhealth.eversmoke_c)) +\n       geom_raster(aes(fill=Freq)) +  scale_fill_viridis_c()\n```\n\n::: {.cell-output-display}\n![](bivariate_analysis_files/figure-html/unnamed-chunk-21-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n\nThe proportion of those who have never smoked and report being in\nExcellent health is higher than expected if these two measures were\nindependent (high positive residual means observed is greater than\nexpected). A lower percent of people reporting Good health never smoked,\nwhich is lower than expected if smoking and health status were\nindependent. So these two categories are likely to be the groups that\nhave a different proportion of lifetime smoker $p_{i}$ compared to the\nother groups.\n\n## (Q\\~Q) Correlation {#bv-corr}\n\nThe **correlation coefficient** is designated by $r$ for the sample\ncorrelation, and $\\rho$ for the population correlation. The correlation\nis a measure of the strength and direction of a *linear relationship*\nbetween two variables.\n\nThe correlation ranges from +1 to -1. A correlation of +1 means that\nthere is a perfect, positive linear relationship between the two\nvariables. A correlation of -1 means there is a perfect, negative linear\nrelationship between the two variables. In both cases, knowing the value\nof one variable, you can perfectly predict the value of the second.\n\n### Strength of the correlation\n\nHere are rough estimates for interpreting the strengths of correlations\nbased on the magnitude of $r$.\n\n-   $|r| \\geq 0.7$: Very strong relationship\n-   $0.4 \\leq |r| < 0.7$: Strong relationship\n-   $0.3 \\leq |r| < 0.4$: Moderate relationship\n-   $0.2 \\leq |r| < 0.3:$ Weak relationship\n-   $|r| < 0.2:$ Negligible or no relationship\n\n### Example: Federal spending per capita and poverty rate\n\n::: {.callout-tip appearance=minimal}\n> Add intro text to example once resolved/replaced\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ggplot(county, aes(x=poverty, y=fed_spend00)) +\n#  geom_point() + ylab(\"federal spending per capita\") +\n#  xlab(\"poverty rate\")\n#cor(county$poverty, county$fed_spend00, use=\"complete.obs\")\n```\n:::\n\n\n\n\n-   There is a negligible, positive, linear relationship between poverty\n    rate and per capita federal spending ($r = 0.03$).\n-   Let $\\rho$ denote the true correlation between poverty rate and\n    federal spending per capita.\n-   Our null hypothesis is that there is no correlation between poverty\n    rate and federal spending ($\\rho = 0$), and the alternative\n    hypothesis is that they are correlated ($\\rho \\neq 0$).\n-   We can use the `cor.test()` function to analyze the evidence in\n    favor of this alternative hypothesis.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#cor.test(county$poverty, county$fed_spend00)\n```\n:::\n\n\n\n\nWe conclude from this that there was a non-statistically significant,\nnegligible correlation between poverty and federal spending\n($r = 0.03 (-0.0003, .069), p = 0.05$).\n",
    "supporting": [
      "bivariate_analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}