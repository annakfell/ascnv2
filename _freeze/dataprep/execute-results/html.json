{
  "hash": "fadf656908aa8d507676176f0abe5e1e",
  "result": {
    "engine": "knitr",
    "markdown": "# Workflow and Data Cleaning {#sec-data-prep}\n\n:::{.callout-note}\n#### Packages Used\nThis chapter uses the following packages: [forcats](https://forcats.tidyverse.org), [car](https://cran.r-project.org/web/packages/car/index.html), [janitor](https://sfirke.github.io/janitor/index.html)\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nOnce the data are available from a study there are still a number of steps that must be undertaken to get them into shape for analysis.\n\nOne of the most misunderstood parts of the analysis process is the data preparation stage. To say that 70% of any analysis is spent on the data management stage is not an understatement.\n\nThis chapter provides information on topics related to data processing and corresponds to PMA6 Chapter 3.\n\n## Reproducible workflows\n\nReproducibility is the ability for any researcher to take the same data set and run the same set of software program instructions as another researcher and achieve the same results.\n\nThe goal is to create an exact record of what was done to a data set to produce a specific result. To achieve reproducibility, we believe that three things must be present:\n\n1.  The unprocessed data are connected directly to software code file(s) that perform data preparation techniques.\n2.  The processed data are connected directly to other software code file(s) that perform the analyses.\n3.  All data and code files are self-contained such that they could be given to another researcher to execute the code commands on a separate computer and achieve the same results as the original author.\n\n![](images/Afifi_Fig3_1.png)\n\n::: {.callout-caution}\n## Think about it\n \nWhy do we need a codebook?\n\n-   You are your own collaborator 6 months from now. Make sure you will be able to understand what you were doing.\n-   Investing the time to do things clearly and in a reproducible manner will make your future self happy.\n-   Comment your code with explanations and instructions.\n    -   How did you get from point A to B?\n    -   Why did you recode this variable in this manner?\n-   We need to record those steps (not just for posterity).\n-   This means your code must be saved in a script file.\n    -   Include sufficient notes to yourself describing what you are doing and why.\n    -   For R, this can be in a `.R`, `.Rmd` or `.qmd` file. I always prefer the latter.\n    -   For SAS you'll use a `.sas` file\n    -   For STATA this will be a `.do` file\n    \n:::\n\n![Figure Credits: [Roger Peng](https://rdpeng.org)](images/pipeline.png)\n\n\n### Literate programming\n\n-   Programming paradigm introduced by Knuth (1984)\n-   Explain the logic of the program or analysis process in a natural language\n-   Small code snippets included at each step act as a full set of instructions that can be executed to reproduce the result/analysis being discussed.\n\nLiterate programming tools are integrated into most common statistical packages, including:\n\n-   Markdown (R, Stata), Quarto (R, Python, Julia, and JavaScript)\n-   $\\LaTeX$ (R, SAS, Stata)\n\n::: {.callout-tip}\n## Learn more\nThe current gold standard for writing reproducible literate documents in R is to use [Quarto](https://quarto.org/docs/get-started/hello/rstudio.html). Quarto documents can integrate code snippets from several languages and other code editing platforms, like Jupyter Notebook.\n:::\n\nPracticing reproducible research techniques using literate programming tools allows such major updates to be a simple matter of recompiling all coded instructions using the updated data set.\n\nThe effort then is reduced to a careful review and update of any written results.\n\nUsing literate programming tools, you can create formatted documents with\n\n-   section headers\n-   bold and italicized words\n-   tables and graphics with built-in captions\n\nin a streamlined manner that is fully synchronized with the code itself.\n\nThe author writes the text explanations, interpretations, and code in the statistical software program itself, and the program will execute all commands and combine the text, code and output all together into a final dynamic document.\n\n::: {.callout-caution}\n## Think about it\nWhat stages of the pipeline shown above can we conduct using literate programming tools?\n:::\n\n## Import data\n\nThis section uses the raw `depression` data set from the Afifi et. al. textbook. This is a tab-delimited data set, so we opt to use `read.table` here. We include arguments `sep=\"\\t\"` to indicate columns are separated with tabs and `header=TRUE` to indicate the first row of the data is the variable names.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndepress_raw <- read.table(here::here(\"data/Depress.txt\"), \n                      sep=\"\\t\", header=TRUE)  \n```\n:::\n\n\n\n\n::: {.callout-tip}\n### Learn more\n\nSee [R for Data Science (2e)](https://r4ds.hadley.nz/data-import) for more instruction on importing different types of data and ways you can streamline your data import.\n:::\n\nThe absolute first thing you should do is to look at your raw data table. Are the column headers variable names? Did all the rows get read in? Are there any extra columns or rows included?\n\n## Data management\n\nQuestions to ask yourself while reviewing the codebook to choose variables to be used in an analysis.\n\n1.  Are there codes that indicate missing data? E.g. *MISSING* or *-99*?\n2.  Do you need to make response codes more logical?\n    -   Some systems will record 1=YES and 2=NO. This should be changed to 0=NO.\n3.  Do you need to recode numerical variables to categorical?\n    -   Sometimes categorical data will be recorded as 1, 2, 3 etc. when those numbers represent named categories.\n4.  Do you need to create secondary variables such as an average across measures to create a score?\n5.  Are the variable names user-friendly? Mixtures of CAPS and lower case, names with spaces or special characters should all be changed.\n\nSome of these answers will come only after you look at your data. This can be looking at the raw data itself but also looking at tables and charts generated from the data. Often when you try to create a plot or table you will encounter an error or something odd looking that will be the notification that something has to be adjusted.\n\nThe next sections go over a *few* of the common data management processes, but is not comprehensive, and may only show one method for cleaning. There are always different ways to accomplish tasks.\n\n### Renaming variable names for sanity's sake\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(names(depress_raw))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"ID\"      \"SEX\"     \"AGE\"     \"MARITAL\" \"EDUCAT\"  \"EMPLOY\" \n```\n\n\n:::\n:::\n\n\n\n\nPeeking at the names of the variables we note that they are all in upper case. If that is fine with you, awesome. I prefer to have everything lower case so that I don't ever have to remember which are the capital letters. Here are two ways to accomplish this: \n\n::: panel-tabset\n\n## base\n\nA base R solution is to use `tolower()` to turn all variable names to lower case.\nThis code is not run here because it would overwrite the variable names in the same data set (`depress_raw`). Keep the imported (aka. \"raw\") data untouched, and then make a copy of the data once you start making changes. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(depress_raw) <- tolower(names(depress_raw))\n```\n:::\n\n\n\n\n## janitor\n\nA highly recommended method is to use the `clean_names()` function from the [janitor](https://sfirke.github.io/janitor/) package. This will also remove any special characters, spaces and capital letters from your variable names. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndepress <- depress_raw %>% janitor::clean_names()\n```\n:::\n\n\n\n\nI am \"staging\" the data set at this point because i'm making a major change away from the 'raw' data. So i'm saving the changes to the variable names in a new data set called `depress`. \n\n::: {.callout-warning appearance=simple}\nNote the use of `::` between the package name `janitor` and the function within that package `clean_names`. This is a shortcut that allows you to use a function from a specific package without loading the entire package. This can also reduce in function name conflicts that we'll mention below.\n:::\n\n:::\n\n### Identifying variable types\n\n::: panel-tabset\n## base\n\nThe `str` function is short for *structure*. This shows you the variable names, what data types R thinks each variable are, and some of the raw data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(depress)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t294 obs. of  37 variables:\n $ id      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ sex     : int  2 1 2 2 2 1 2 1 2 1 ...\n $ age     : int  68 58 45 50 33 24 58 22 47 30 ...\n $ marital : int  5 3 2 3 4 2 2 1 2 2 ...\n $ educat  : int  2 4 3 3 3 3 2 3 3 2 ...\n $ employ  : int  4 1 1 3 1 1 5 1 4 1 ...\n $ income  : int  4 15 28 9 35 11 11 9 23 35 ...\n $ relig   : int  1 1 1 1 1 1 1 1 2 4 ...\n $ c1      : int  0 0 0 0 0 0 2 0 0 0 ...\n $ c2      : int  0 0 0 0 0 0 1 1 1 0 ...\n $ c3      : int  0 1 0 0 0 0 1 2 1 0 ...\n $ c4      : int  0 0 0 0 0 0 2 0 0 0 ...\n $ c5      : int  0 0 1 1 0 0 1 2 0 0 ...\n $ c6      : int  0 0 0 1 0 0 0 1 3 0 ...\n $ c7      : int  0 0 0 0 0 0 0 0 0 0 ...\n $ c8      : int  0 0 0 3 3 0 2 0 0 0 ...\n $ c9      : int  0 0 0 0 3 1 2 0 0 0 ...\n $ c10     : int  0 0 0 0 0 0 0 0 0 0 ...\n $ c11     : int  0 0 0 0 0 0 0 0 0 0 ...\n $ c12     : int  0 1 0 0 0 1 0 0 3 0 ...\n $ c13     : int  0 0 0 0 0 2 0 0 0 0 ...\n $ c14     : int  0 0 1 0 0 0 0 0 3 0 ...\n $ c15     : int  0 1 1 0 0 0 3 0 2 0 ...\n $ c16     : int  0 0 1 0 0 2 0 1 3 0 ...\n $ c17     : int  0 1 0 0 0 1 0 1 0 0 ...\n $ c18     : int  0 0 0 0 0 0 0 1 0 0 ...\n $ c19     : int  0 0 0 0 0 0 0 1 0 0 ...\n $ c20     : int  0 0 0 0 0 0 1 0 0 0 ...\n $ cesd    : int  0 4 4 5 6 7 15 10 16 0 ...\n $ cases   : int  0 0 0 0 0 0 0 0 1 0 ...\n $ drink   : int  2 1 1 2 1 1 2 2 1 1 ...\n $ health  : int  2 1 2 1 1 1 3 1 4 1 ...\n $ regdoc  : int  1 1 1 1 1 1 1 2 1 1 ...\n $ treat   : int  1 1 1 2 1 1 1 2 1 2 ...\n $ beddays : int  0 0 0 0 1 0 0 0 1 0 ...\n $ acuteill: int  0 0 0 0 1 1 1 1 0 0 ...\n $ chronill: int  1 1 0 1 0 1 1 0 1 0 ...\n```\n\n\n:::\n:::\n\n\n\n\nTo check the data type of just one variable, you have two options:\n\n- The `typeof` function \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntypeof(depress$marital)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"integer\"\n```\n\n\n:::\n:::\n\n\n\n\n- The `class` function \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(depress$age)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"integer\"\n```\n\n\n:::\n:::\n\n\n\n\n## tidyverse\n\nA `tidyverse` alternative is `glimpse()`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(depress)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 294\nColumns: 37\n$ id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ sex      <int> 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2…\n$ age      <int> 68, 58, 45, 50, 33, 24, 58, 22, 47, 30, 20, 57, 39, 61, 23, 2…\n$ marital  <int> 5, 3, 2, 3, 4, 2, 2, 1, 2, 2, 1, 2, 2, 5, 2, 1, 1, 4, 1, 5, 1…\n$ educat   <int> 2, 4, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 2, 3, 3, 2, 4, 2, 6, 2, 3…\n$ employ   <int> 4, 1, 1, 3, 1, 1, 5, 1, 4, 1, 3, 2, 1, 4, 1, 1, 1, 3, 1, 4, 1…\n$ income   <int> 4, 15, 28, 9, 35, 11, 11, 9, 23, 35, 25, 24, 28, 13, 15, 6, 8…\n$ relig    <int> 1, 1, 1, 1, 1, 1, 1, 1, 2, 4, 4, 1, 1, 1, 2, 1, 1, 1, 1, 4, 2…\n$ c1       <int> 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 3, 1, 0, 0, 0…\n$ c2       <int> 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 3, 0, 0, 0, 0…\n$ c3       <int> 0, 1, 0, 0, 0, 0, 1, 2, 1, 0, 1, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0…\n$ c4       <int> 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0…\n$ c5       <int> 0, 0, 1, 1, 0, 0, 1, 2, 0, 0, 1, 0, 0, 1, 0, 1, 3, 1, 0, 0, 0…\n$ c6       <int> 0, 0, 0, 1, 0, 0, 0, 1, 3, 0, 2, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0…\n$ c7       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0…\n$ c8       <int> 0, 0, 0, 3, 3, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 3, 0…\n$ c9       <int> 0, 0, 0, 0, 3, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 3…\n$ c10      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0…\n$ c11      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0…\n$ c12      <int> 0, 1, 0, 0, 0, 1, 0, 0, 3, 0, 1, 0, 1, 1, 0, 1, 2, 0, 0, 0, 0…\n$ c13      <int> 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0…\n$ c14      <int> 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0…\n$ c15      <int> 0, 1, 1, 0, 0, 0, 3, 0, 2, 0, 1, 2, 0, 0, 1, 1, 3, 0, 0, 0, 0…\n$ c16      <int> 0, 0, 1, 0, 0, 2, 0, 1, 3, 0, 1, 2, 1, 0, 3, 1, 2, 0, 0, 0, 0…\n$ c17      <int> 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0…\n$ c18      <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0…\n$ c19      <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0…\n$ c20      <int> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 3, 0, 0, 0, 0…\n$ cesd     <int> 0, 4, 4, 5, 6, 7, 15, 10, 16, 0, 18, 4, 8, 4, 8, 21, 42, 6, 0…\n$ cases    <int> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0…\n$ drink    <int> 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1…\n$ health   <int> 2, 1, 2, 1, 1, 1, 3, 1, 4, 1, 2, 2, 3, 1, 1, 3, 1, 3, 2, 2, 1…\n$ regdoc   <int> 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1…\n$ treat    <int> 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1…\n$ beddays  <int> 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0…\n$ acuteill <int> 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0…\n$ chronill <int> 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1…\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\nRight away this tells me that **R** thinks all variables are numeric integers, not categorical variables. Many of these will have to be changed. We'll get to that in a moment.\n\n### Convert number to factor\n\nWhen variables have numerical levels it is necessary to ensure that the program knows it is a factor variable.\n\nThe following code uses the `factor()` function to take the marital status variable and convert it into a factor variable with specified labels that match the codebook.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndepress$marital <- factor(depress$marital, \n      labels = c(\"Never Married\", \"Married\", \"Divorced\", \"Separated\", \"Widowed\"))\n```\n:::\n\n\n\n\nNote that I am not making a new variable here, but overwriting the same `marital` variable. If If it did not you will have to re-run the lread in the raw data set again since the variable `marital` was replaced.\n\nIt is important to confirm the recode worked. I do this by creating a two-way table between the variable as it exists on the raw data, and how it exists after this line of code. What we are looking for is that all values on the left/rows (original version) line up with what we want them to say on the right (new version), and that no missing data was created or destroyed. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(depress_raw$MARITAL, depress$marital, useNA = \"always\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      \n       Never Married Married Divorced Separated Widowed <NA>\n  1               73       0        0         0       0    0\n  2                0     127        0         0       0    0\n  3                0       0       43         0       0    0\n  4                0       0        0        13       0    0\n  5                0       0        0         0      38    0\n  <NA>             0       0        0         0       0    0\n```\n\n\n:::\n:::\n\n\n\n\n\n::: {.callout-tip}\n\n## Learn more\n\nSee more examples on [Math 130 Lesson 06](https://norcalbiostat.github.io/MATH130/notes/06_factors.html#Convert_a_number_to_Factor_(Video))\n:::\n\n### Identifying outliers\n\nLet's look at the age variable in the `depression` data set.\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\nboxplot(depress$age)\nhist(depress$age)\n```\n\n::: {.cell-output-display}\n![](dataprep_files/figure-html/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n\n\nJust looking at the data graphically raises no red flags. The boxplot shows no outlying values and the histogram does not look wildly skewed. This is where knowledge about the data set is essential. The codebook does not provide a valid range for the data, but the description of the data starting on page 3 in the textbook clarifies that this data set is on adults. In the research world, this specifies 18 years or older.\n\nNow look back at the graphics. See anything odd? It appears as if the data go pretty far below 20, possibly below 18. Let's check the numerical summary to get more details.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(depress$age)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   28.00   42.50   44.38   59.00   89.00 \n```\n\n\n:::\n:::\n\n\n\n\nThe minimum value is a 9, which is outside the range of valid values for this variable. This is where you, as a statistician, data analyst or researcher goes back to the PI and asks for advice. Should this data be set to missing, or edited in a way that changes this data point into a valid piece of data?\n\n**Another example**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(depress$income)\n```\n\n::: {.cell-output-display}\n![](dataprep_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n\nWhile there is at least one potential outliers (denoted by the dots), there are none so far away from the rest of the group (or at values such as 99 or -99 that may indicate missing codes) that we need to be concerned about.\n\n### Changing numeric values\n\nWhat you didn't know until now, is that for demonstration purposes I went in and changed a 19 to a 9. So the correct thing to do here is to change that 9, back to a 19. \n\n::: panel-tabset\n## ifelse()\n\nThis is a very good use of the `ifelse()` function.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndepress$age <- ifelse(depress$age==9, 19, depress$age)\n```\n:::\n\n\n\n\nThe logical statement is `depress$age9`. Wherever this is true, replace the value of `depress$age` with 19, and wherever this is false keep the value of `depress$age` unchanged (by \"replacing\" the new value with the same old value).\n\n## direct assign\n\nAlternatively, you can change that one value using bracket notation. Here you are specifying that you only want the rows where `age==9`, and directly assign a value of 19 to those rows.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndepress$age[depress$age==9] <- 19\n```\n:::\n\n\n\n\nConfirm the recode.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(depress$age)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   28.00   42.50   44.41   59.00   89.00 \n```\n\n\n:::\n:::\n\n\n\n\nLooks like it worked.\n:::\n\n### Creating secondary variables\n\n### Create a binary indicator\n\nFor analysis purposes you may need to have a numeric binary indicator (0/1) of a variable. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(addhealth$eversmoke_c)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nNon Smoker     Smoker \n      1773       3324 \n```\n\n\n:::\n\n```{.r .cell-code}\naddhealth$smoker <- ifelse(addhealth$eversmoke_c==\"Smoker\", 1, 0)\ntable(addhealth$eversmoke_c, addhealth$smoker, useNA=\"always\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            \n                0    1 <NA>\n  Non Smoker 1773    0    0\n  Smoker        0 3324    0\n  <NA>          0    0 1407\n```\n\n\n:::\n:::\n\n\n\n\n\n::: {.callout-tip}\n### Learn more\nSee [Math 130 lesson 05](https://norcalbiostat.github.io/MATH130/notes/05_dm.html#Creating_new_variables_(Video)) for now.\n:::\n\n## Wrangling factors\n\n::: {.callout-tip}\n## Learn more\nFor more help on renaming, releveling, lumping, and removing levels see [Math 130 lesson 06](https://norcalbiostat.github.io/MATH130/notes/06_factors.html) for now. Also the [forcats](https://forcats.tidyverse.org/index.html) vignette.\n:::\n\n### Collapsing categorical variables into fewer categories\n\nFor unbiased and accurate results of a statistical analysis, sufficient data has to be present. Often times once you start slicing and dicing the data to only look at certain groups, or if you are interested in the behavior of certain variables across levels of another variable, sometimes you start to run into small sample size problems.\n\nFor example, consider marital status again. There are only 13 people who report being separated. This could potentially be too small of a group size for valid statistical analysis. One way to deal with insufficient data within a certain category is to collapse categories.\n\n:::{.callout-note}\nNote I am choosing to 'stage' my data here. So if I mess something up in this section, I don't have to re-read in the raw data again or go back and rerun ALL the code, but just up until `depress1` is created. \n\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndepress1 <- depress\n```\n:::\n\n\n\n\n\n::: {.panel-tabset}\n\n## car\n\nThe following example code uses the `recode()` function from the `car` package to create a new variable that I am calling `marital2` that combines the `Divorced` and `Separated` levels. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndepress1$marital <- car::recode(depress1$marital, \"'Divorced' = 'Sep/Div'; 'Separated' = 'Sep/Div'\")\n```\n:::\n\n\n\n\n::: {.callout-warning appearance=simple}\n\nNote the use of the `::` again. Here it is even more important to use this shortcut because the specific `recode` function we want to use comes from the `car` package. There are other packages (probably `dplyr`) that also have a function called `recode`. So here I use `::` as a way to be SUPER EXPLICIT on which function I want to use. \n\n:::\n\nAlways confirm your recodes. Check a table of the old variable (`depress$marital`) against the new one `depress1$marital`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(depress$marital, depress1$marital , useNA=\"always\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               \n                Married Never Married Sep/Div Widowed <NA>\n  Never Married       0            73       0       0    0\n  Married           127             0       0       0    0\n  Divorced            0             0      43       0    0\n  Separated           0             0      13       0    0\n  Widowed             0             0       0      38    0\n  <NA>                0             0       0       0    0\n```\n\n\n:::\n:::\n\n\n\n\nThis confirms that records where `depress$marital` (rows) is `Divorced` or `Separated` have the value of `Sep/Div` for `depress1$marital` (columns). And that no missing data crept up in the process.\n\n\n## forcats\n\nThe `fct_collapse()` function from the `forcats` package can do the same process, without worrying about a package/function conflict. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndepress1$marital <- fct_collapse(depress$marital, SepDiv = c(\"Divorced\", \"Separated\"))\n```\n:::\n\n\n\n\nAgain, you should always confirm your recodes. Check a table of the old variable (`depress$marital`) against the new one `depress1$marital`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(depress$marital, depress1$marital , useNA=\"always\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               \n                Never Married Married SepDiv Widowed <NA>\n  Never Married            73       0      0       0    0\n  Married                   0     127      0       0    0\n  Divorced                  0       0     43       0    0\n  Separated                 0       0     13       0    0\n  Widowed                   0       0      0      38    0\n  <NA>                      0       0      0       0    0\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n### Binning a continuous variable into categorical ranges.\n\nWe can use the `cut` function to create a new variable that categorizes income into the following ranges: \\<30, \\[30, 40), \\[40,50), \\[50, 60), 60+.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndepress1$inc_cut <- cut(depress1$income, breaks=c(0, 30,40,50,60, 100))\ntable(depress1$inc_cut)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  (0,30]  (30,40]  (40,50]  (50,60] (60,100] \n     231       28       16        9       10 \n```\n\n\n:::\n:::\n\n\n\n\n### Dichotomizing a measure into 2 categories\n\nDichotomous variables tend to be binary indicator variables where a code of `1` is the level you're interested in.\n\n::: {.panel-tabset}\n\n## Example 1\n\n::: {.callout-warning appearance=simple}\n> Switch example from binary gender to different characteristic\n\n:::\n\nIn this study gender is coded as 2=Female and 1=Male. (*This data was collected in the '70s, and so only two genders were provided as options*). We want to convert this be a binary indicator of female, where 1=Female and 0=Male.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndepress1$female <- depress1$sex -1 \ntable(depress1$female)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  0   1 \n111 183 \n```\n\n\n:::\n:::\n\n\n\n\n0/1 binary coding is mandatory for many analyses. One simple reason is that now you can calculate the mean and interpret it as a proportion.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(depress1$female)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.622449\n```\n\n\n:::\n:::\n\n\n\n\n62% of individuals in this data set are female.\n\n## Example 2\n\nSometimes the data is recorded as 1/2 (Yes/No), so just subtracting from 1 doesn't create a positive indicator of the variable. For example, `drink=1` if they are a regular drinker, and `drink=2` if they are not. We want not drinking to be coded as `0`, not `2`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(depress_raw$DRINK)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  1   2 \n234  60 \n```\n\n\n:::\n:::\n\n\n\n\nThe `ifelse()` function says that if `depress$drink` has a value equal to 2 `==2`, then change the value to 0. Otherwise leave it alone.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndepress1$drink <- ifelse(depress1$drink==2, 0, depress1$drink)\ntable(depress1$drink, depress$drink)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   \n      1   2\n  0   0  60\n  1 234   0\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n## Combining values across multiple variables\n\nLet's stage the data again for this section. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndepress2 <- depress1\n```\n:::\n\n\n\n\n### Row-wise sum or average\n\nThe Center for Epidemiological Studies Depression Scale (CESD) is series of questions asked to a person to measure their level of depression. `CESD` is calculated as the sum of all 20 component variables, and is already on this data set. Let's create a new variable named `sleep` as subscale for sleep quality by adding up question numbers 5, 11, and 19.\n\nReference: [CESD-R](https://cesd-r.com/cesdr/)\n\n::: {.panel-tabset}\n## Base\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndepress2$sleep <- depress2$c5 + depress2$c11 + depress2$c19\n```\n:::\n\n\n\n\nI'll confirm it works by looking at a few rows and making sure they all add up. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(depress2[c('c5', 'c11', 'c19', 'sleep')])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  c5 c11 c19 sleep\n1  0   0   0     0\n2  0   0   0     0\n3  1   0   0     1\n4  1   0   0     1\n5  0   0   0     0\n6  0   0   0     0\n```\n\n\n:::\n:::\n\n\n\n\n## mutate + manual\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndepress2 <- depress %>% mutate(sleep = c5+c11+c19)\nhead(depress2[c('c5', 'c11', 'c19', 'sleep')])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  c5 c11 c19 sleep\n1  0   0   0     0\n2  0   0   0     0\n3  1   0   0     1\n4  1   0   0     1\n5  0   0   0     0\n6  0   0   0     0\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n\n## Assessing Normality\n\n### Histogram and density plots\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(depress2$income, prob=TRUE, xlab=\"Annual income (in thousands)\", \n     main=\"Histogram and Density curve of Income\", ylab=\"\")\nlines(density(depress2$income), col=\"blue\")\n```\n\n::: {.cell-output-display}\n![](dataprep_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsummary(depress2$income)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.00    9.00   15.00   20.57   28.00   65.00 \n```\n\n\n:::\n:::\n\n\n\n\nThe distribution of annual income is slightly skewed right with a mean of \\$20.5k per year and a median of \\$15k per year income. The range of values goes from \\$2k to \\$65k. Reported income above \\$40k appear to have been rounded to the nearest \\$10k, because there are noticeable peaks at \\$40k, \\$50k, and \\$60k.\n\n### Q-Q plot\n\nAnother common method of assessing normality is to create a normal probability (or normal quantile) plot.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqqnorm(depress2$income);qqline(depress2$income, col=\"red\")\n```\n\n::: {.cell-output-display}\n![](dataprep_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n\n\nThe points on the normal probability plot do not follow the red reference line very well. The dots show a more curved, or `U` shaped form rather than following a linear line. This is another indication that the data is skewed and a transformation for normality should be created.\n\n### Transformations\n\nAs a demonstration of transformations that can be used to shift a distribution more towards a normal shape, here we create three new variables: `log10inc` as the log base 10 of Income, `loginc` as the natural log of Income, and `xincome` which is equal to the negative of one divided by the cubic root of income.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog10inc <- log10(depress2$income)\nloginc   <- log(depress2$income)\nxincome  <- -1/(depress2$income)^(-1/3)\n```\n:::\n\n\n\n\nCreate a single plot that display normal probability plots for the original, and each of the three transformations of income. Use the base graphics grid organizer `par(mfrow=c(r,c))` where `r` is the number of rows and `c` is the number of columns. Which transformation does a better job of normalizing the distribution of Income?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,2)) # Try (4,1) and (1,4) to see how this works. \nqqnorm(depress2$income, main=\"Income\"); qqline(depress2$income,col=\"blue\")\nqqnorm(log10inc, main=\"Log 10\"); qqline(log10inc, col=\"blue\")\nqqnorm(loginc, main = \"Natural Log\"); qqline(loginc, col=\"blue\")\nqqnorm(xincome, main=\"-1/cuberoot(income)\"); qqline(xincome, col=\"blue\")\n```\n\n::: {.cell-output-display}\n![](dataprep_files/figure-html/unnamed-chunk-29-1.png){width=960}\n:::\n:::\n\n\n\n\n:::{.callout-tip}\n#### To transform or not to transform\n\nIn general, transformations are more effective when the the standard deviation is large relative to the mean. One rule of thumb is if the sd/mean ratio is less than 1/4, a transformation may not be necessary.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsd(depress2$income) / mean(depress2$income)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.743147\n```\n\n\n:::\n:::\n\n\n\n\nAlternatively Hoaglin, Mosteller and Tukey (1985) showed that if the largest observation divided by the smallest observation is over 2, then the data may not be sufficiently variable for the transformation to be decisive.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmax(depress2$income) / (min(depress2$income)+.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 30.95238\n```\n\n\n:::\n:::\n\n\n\n\nNote that these rules are not meaningful for data without a natural zero.\n\n\n:::\n\n\n## Missing data\n\n### Identifying missing data\n\nIn Excel, missing data can show up as a blank cell. R displays missing data as `NA` values.\n\nWhy would data be missing? Other than the obvious data entry errors, tech glitches or just non-cooperative plants or people, sometimes values are out of range and you would rather delete them than change their value (data edit).\n\n::: panel-tabset\n## Categorical\n\nLets look at the religion variable in the depression data set.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(depress2$relig, useNA=\"always\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   1    2    3    4    6 <NA> \n 155   51   30   56    2    0 \n```\n\n\n:::\n:::\n\n\n\n\nLooking at the codebook, there is no category `6` for religion. Let's change all values to `NA`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndepress2$relig[depress2$relig==6] <- NA\n```\n:::\n\n\n\n\nThis code says take all rows where `relig` is equal to 6, and change them to `NA`.\n\nConfirm recode.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(depress2$relig, useNA=\"always\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   1    2    3    4 <NA> \n 155   51   30   56    2 \n```\n\n\n:::\n:::\n\n\n\n\nNotice the use of the `useNA=\"always\"` argument. If we just looked at the base table without this argument, we would have never known there was missing data!\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(depress2$relig)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  1   2   3   4 \n155  51  30  56 \n```\n\n\n:::\n:::\n\n\n\n\n## Continuous\n\nWhat about continuous variables? Well there happens to be no other missing data in this data set, so let's make up a set of 7 data points stored in a variable named `y`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- c(1, 2, 3, NA, 4, NA, 6)\ny\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  1  2  3 NA  4 NA  6\n```\n\n\n:::\n:::\n\n\n\n\nThe #1 way to identify missing data in a continuous variable is by looking at the `summary()` values.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] NA\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n    1.0     2.0     3.0     3.2     4.0     6.0       2 \n```\n\n\n:::\n\n```{.r .cell-code}\nmean(y, na.rm=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.2\n```\n\n\n:::\n:::\n\n\n\n\nIn R, any arithmetic function (like addition, multiplication) on missing data results in a missing value. The `na.rm=TRUE` toggle tells R to calculate the *complete case* mean. This is a biased measure of the mean, but missing data is a topic worthy of it's own course and is introduced in @sec-classification.\n:::\n\n\n\n## Export and save\n\nYou've just made a ton of changes!\n\n-   Save or export the new data set to your computer.\n-   Edit the codebook to reflect the changes that you made.\n-   Keep the data, codebook and data management file in the same folder.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndepress_clean <- depress2\n\n# Save as a .Rdata file for later use in R\nsave(depress_clean, file = \"data/depress_clean.Rdata\") \n```\n:::\n\n\n\n\nNow every time you run your data cleaning script file, it will make all the changes and save/overwrite the `depress_clean.Rdata` data file. This ensures that any analysis script that uses this data has the most up to date variables.\n\nWe can use `dplyr::select` to select and save individual variables without storing the entire data frame.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout <- depress %>% select(list of variables)\nsave(out, \"data/var1.Rdata\")\n```\n:::\n\n\n\n\n\n::: {.callout-tip}\n\n## Learn more\n\nNeed to export to a different software program? Look into the [haven](https://haven.tidyverse.org) package.\n\n:::\n\n## Wide vs. long data {#wide-long}\n\nThe data on Lung function originally was recorded in *wide* format, with separate variables for mother's and father's FEV1 score (`MFEV1` and `FFEV`). In this format, the data is one record per family.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfev <- read.delim(here::here(\"data/Lung.txt\"), sep=\"\\t\", header=TRUE)\nhead(fev[,1:15])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  ID AREA FSEX FAGE FHEIGHT FWEIGHT FFVC FFEV1 MSEX MAGE MHEIGHT MWEIGHT MFVC\n1  1    1    1   53      61     161  391  3.23    2   43      62     136  370\n2  2    1    1   40      72     198  441  3.95    2   38      66     160  411\n3  3    1    1   26      69     210  445  3.47    2   27      59     114  309\n4  4    1    1   34      68     187  433  3.74    2   36      58     123  265\n5  5    1    1   46      61     121  354  2.90    2   39      62     128  245\n6  6    1    1   44      72     153  610  4.91    2   36      66     125  349\n  MFEV1 OCSEX\n1  3.31     2\n2  3.47     1\n3  2.65     1\n4  2.06     2\n5  2.33     1\n6  3.06     1\n```\n\n\n:::\n:::\n\n\n\n\nTo analyze the effect of gender on FEV, the data need to be in *long* format, with a single variable for `fev` and a separate variable for gender. The following code chunk demonstrates one method of combining data on height, gender, age and FEV1 for both males and females.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfev2 <- data.frame(gender = c(fev$FSEX, fev$MSEX), \n                   rev = c(fev$FFEV1, fev$MFEV1), \n                   ht = c(fev$FHEIGHT, fev$MHEIGHT), \n                   age = c(fev$FAGE, fev$MAGE))\nfev2$gender <- factor(fev2$gender, labels=c(\"M\", \"F\"))\nhead(fev2)  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  gender  rev ht age\n1      M 3.23 61  53\n2      M 3.95 72  40\n3      M 3.47 69  26\n4      M 3.74 68  34\n5      M 2.90 61  46\n6      M 4.91 72  44\n```\n\n\n:::\n:::\n\n\n\n\nNearly all analysis procedures and most graphing procedures require the data to be in long format. There are several `R` packages that can help with this including [reshape2](https://seananderson.ca/2013/10/19/reshape/) and [tidyr](https://tidyr.tidyverse.org).\n\n::: {.callout-tip}\n### Learn more\nRead more on tidy data in [R for Data Science 2e](https://r4ds.hadley.nz/data-tidy), or look into the [mice](https://www.gerkovink.com/miceVignettes/) package vignettes.\n:::\n\n### Model predictions\n\n**Situation**: You want to add model predictions to the data set, but you have missing data that was automatically dropped prior to analysis.\n\n::: {.callout-warning appearanc=simple}\n\n> Add methods for dealing with this, once added reference @sec-classification\n\n:::\n\nR objects created by methods such as `lm` and `glm` will store the data used in the model in the model object itself in `model$data`. \n\n### Factor analysis and principal components\n\nIf your original data had missing values, here are two methods to get the PC's / factor scores for available data back onto the data set.\n\n::: panel-tabset\n## Method 1\n\n**Create an ID column and merge new variables onto original data. (add columns)**\n\n1.  If no ID column exists, create one on the original dataset `id = 1:NROW(data)`\n2.  Use `select()` to extract the ID and all variables used in the factor analysis, then do a `na.omit()` to drop rows with any missing data. Save this as a new complete case data set.\n3.  Conduct PCA / Factor analysis on this new complete case data set (MINUS THE ID). Extract the PCs or factor scores.\n4.  Use `bind_cols()` to add the ID variable to the data containing factor scores.\n5.  Then `left_join(original_data, factor_score_data)` the factor scores back to the original data, using the ID variable as the joining key.\n\n## Method 2\n\n**Split the data, analyze one part then concatenate back together. (add rows)**\n\n1.  Use the `complete.cases()` function to create a boolean vector for if each row is complete\n2.  Split the data into complete and incomplete.\n3.  Do the analysis on the complete rows, extracting the PC's/Factors\n4.  Add the PC/Factor data onto the complete rows using `bind_cols`\n5.  Then `bind_rows` the two parts back together.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncc.idx <- hiv %>% select(starts_with(\"pb\")) %>% complete.cases() # 1\n\ncomplete.rows <- hiv[cc.idx,] #2\nincomplete.rows <- hiv[!cc.idx,]\n\npc.scores <- princomp(pb)$scores #3 \n\ncomplete.add.pc <- bind_cols(complete.rows, pc.scores) #4\n\nhiv.with.pcs <- bind_rows(complete.add.pc, incomplete.rows) #5\n```\n:::\n\n\n\n:::\n",
    "supporting": [
      "dataprep_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}